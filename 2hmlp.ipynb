{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.13"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### model.py","metadata":{}},{"cell_type":"code","source":"import torch\nfrom typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass Translator(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        dir_hidden_dims: list[int],\n        scale_hidden_dims: list[int],\n        activation=nn.ReLU,\n        dropout_rate: float=0.3\n    ):\n        super().__init__()\n\n        def build_mlp(hidden_dims, out_dim, apply_softplus=False):\n            layers = []\n            last_dim = input_dim\n            for hidden in hidden_dims:\n                layers += [\n                    nn.Linear(last_dim, hidden),\n                    activation(),\n                    nn.LayerNorm(hidden),\n                    nn.Dropout(dropout_rate)\n                ]\n                last_dim = hidden\n            layers.append(nn.Linear(last_dim, out_dim))\n            \n            if apply_softplus:\n                layers.append(nn.Softplus())\n            \n            return nn.Sequential(*layers)\n\n        self.dir_head = build_mlp(dir_hidden_dims, output_dim, apply_softplus=False)\n        self.scale_head = build_mlp(scale_hidden_dims, 1, apply_softplus=True)\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    def forward(self, x):\n        direction = F.normalize(self.dir_head(x), dim=-1)\n        scale = self.scale_head(x)\n        return direction * scale","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.status.busy":"2025-11-01T18:58:51.579140Z","iopub.execute_input":"2025-11-01T18:58:51.579627Z","iopub.status.idle":"2025-11-01T18:58:51.588670Z","shell.execute_reply.started":"2025-11-01T18:58:51.579596Z","shell.execute_reply":"2025-11-01T18:58:51.587960Z"},"trusted":true},"outputs":[],"execution_count":60},{"cell_type":"markdown","source":"### eval.py","metadata":{}},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport torch\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n    \n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n    \n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n    \n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n    \n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n        \n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n    \n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n    \n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n    \n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n    \n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n    \n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: Translator, device) -> dict:\n    gt_indices = torch.arange(len(y_val))\n    \n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n    \n    return results\n\ndef generate_submission(model: Translator, test_path: Path, output_file=\"submission.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"✓ Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"execution":{"iopub.status.busy":"2025-11-01T18:58:51.589934Z","iopub.execute_input":"2025-11-01T18:58:51.590270Z","iopub.status.idle":"2025-11-01T18:58:51.612727Z","shell.execute_reply.started":"2025-11-01T18:58:51.590245Z","shell.execute_reply":"2025-11-01T18:58:51.611860Z"},"trusted":true},"outputs":[],"execution_count":61},{"cell_type":"markdown","source":"### configs","metadata":{}},{"cell_type":"markdown","source":"### main.py","metadata":{}},{"cell_type":"code","source":"\nimport numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom pathlib import Path\nfrom tqdm import tqdm\n\n# def info_nce_loss(preds, img_targets, temp: float = 0.07, lambda_mag: float = 0.1):\n#     preds_norm = F.normalize(preds, dim=1)\n#     targets_norm = F.normalize(img_targets, dim=1)\n\n#     direction_logits = (preds_norm @ targets_norm.T) / temp\n\n#     preds_mag = preds.norm(dim=1).unsqueeze(1)\n#     targets_mag = img_targets.norm(dim=1).unsqueeze(0\n#     magnitude_logits = -(preds_mag - targets_mag)**2\n\n#     combined_logits = direction_logits + lambda_mag * magnitude_logits\n\n#     labels = torch.arange(preds.size(0), device=preds.device)\n\n#     loss_t2i = F.cross_entropy(combined_logits, labels)\n#     loss_i2t = F.cross_entropy(combined_logits.T, labels)\n\n#     return 0.5 * (loss_t2i + loss_i2t)\n\ndef info_nce_loss(preds_norm, targets_norm, temp=0.07):\n    logits = (preds_norm @ targets_norm.T) / temp\n    labels = torch.arange(logits.size(0), device=logits.device)\n\n    loss_t2i = F.cross_entropy(logits, labels)          \n    loss_i2t = F.cross_entropy(logits.T, labels)        \n    \n    return 0.5 * (loss_t2i + loss_i2t)\n\n\ndef train_model(\n    model: Translator,\n    model_path: Path,\n    train_dataset: TensorDataset,\n    val_dataset: TensorDataset,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    patience: int\n) -> Translator:    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    print(f\"Using device: {device}\")\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    best_val_loss = float('inf')\n    no_improvements = 0\n\n    for epoch in range(epochs):\n        model.train()\n\n        train_loss = 0\n        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            \n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_batch = F.normalize(y_batch, dim=-1)\n\n            optimizer.zero_grad()\n\n            outputs = model(X_batch)\n\n            loss = info_nce_loss(outputs, y_batch, temp=model.logit_scale)\n\n            loss.backward()\n\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        model.eval()\n\n        val_loss = 0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                \n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                outputs = model(X_batch)\n                \n                y_batch = F.normalize(y_batch, dim=-1)\n\n                loss = info_nce_loss(outputs, y_batch, temp=model.logit_scale)\n\n                val_loss += loss.item()\n\n        val_loss /= len(val_loader)\n\n        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n        test(val_dataset, model, device)\n\n\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            no_improvements = 0\n\n            Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n\n            torch.save(model.state_dict(), model_path)\n\n            print(f\"✓ Saved best model (val_loss={val_loss:.6f})\")\n        elif no_improvements >= patience:\n            return model\n        else:\n            no_improvements += 1\n\n    return model\n\n\n\ndef load_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n    \n    print('Texts shape', X_abs.shape)\n    print('Images shape', X_abs.shape)\n\n    def print_stats():\n        mean_X = X_abs.mean(dim=0)\n        std_X = X_abs.std(dim=0)\n        \n        mean_Y = y_abs.mean(dim=0)\n        std_Y = y_abs.std(dim=0)\n\n        print(\"X: mean of stds per dim =\", std_X.mean().item(), \", max =\", std_X.max().item(), \", min =\", std_X.min().item())\n        print(\"Y: mean of stds per dim =\", std_Y.mean().item(), \", max =\", std_Y.max().item(), \", min =\", std_Y.min().item())\n\n    print_stats()\n    \n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n    \n    return train_dataset, val_dataset\n\n\ndef test(val_dataset: TensorDataset, model: Translator, device):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device)\n    return results","metadata":{"execution":{"iopub.status.busy":"2025-11-01T18:58:51.715435Z","iopub.execute_input":"2025-11-01T18:58:51.716029Z","iopub.status.idle":"2025-11-01T18:58:51.729503Z","shell.execute_reply.started":"2025-11-01T18:58:51.716010Z","shell.execute_reply":"2025-11-01T18:58:51.728676Z"},"trusted":true},"outputs":[],"execution_count":62},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nbatch_size= 2048\nlr= 0.01\nepochs= 200\npatience = 5\n\ndata_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n\nmodel_save_path= './models/exp1.pth'\n\ntrain_dataset, val_dataset = load_data(data_path)","metadata":{"execution":{"iopub.status.busy":"2025-11-01T18:58:51.730767Z","iopub.execute_input":"2025-11-01T18:58:51.730978Z","iopub.status.idle":"2025-11-01T18:59:07.582357Z","shell.execute_reply.started":"2025-11-01T18:58:51.730963Z","shell.execute_reply":"2025-11-01T18:59:07.581574Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1024])\nX: mean of stds per dim = 0.788078248500824 , max = 3.573546886444092 , min = 0.3716050386428833\nY: mean of stds per dim = 0.4244377911090851 , max = 1.8597956895828247 , min = 0.08161858469247818\n","output_type":"stream"}],"execution_count":63},{"cell_type":"code","source":"model_args = {\n    'input_dim': 1024,\n    'output_dim': 1536,\n    'dir_hidden_dims': [1256, 2048],\n    'scale_hidden_dims': [256, 128],\n    'activation': nn.SiLU,\n    'dropout_rate': 0.25\n}\nmodel = Translator(**model_args).to(device)\n\ntrain_model(model, model_save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience)\n\nprint('Finished training. Now testing using best model...')\n\nstate = torch.load(model_save_path)\nmodel.load_state_dict(state)\nresults = test(val_dataset, model, device)\n\nprint(\"Test Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T18:59:07.588393Z","iopub.execute_input":"2025-11-01T18:59:07.588623Z","iopub.status.idle":"2025-11-01T19:00:23.758431Z","shell.execute_reply.started":"2025-11-01T18:59:07.588608Z","shell.execute_reply":"2025-11-01T19:00:23.757656Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"Epoch 1/200: 100%|██████████| 55/55 [00:03<00:00, 16.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1: Train Loss = 4.365358, Val Loss = 2.574099\n✓ Saved best model (val_loss=2.574099)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 2/200: 100%|██████████| 55/55 [00:03<00:00, 16.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2: Train Loss = 2.557806, Val Loss = 2.049838\n✓ Saved best model (val_loss=2.049838)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 3/200: 100%|██████████| 55/55 [00:03<00:00, 16.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3: Train Loss = 2.044762, Val Loss = 1.859903\n✓ Saved best model (val_loss=1.859903)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 4/200: 100%|██████████| 55/55 [00:03<00:00, 16.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4: Train Loss = 1.725505, Val Loss = 1.736459\n✓ Saved best model (val_loss=1.736459)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 5/200: 100%|██████████| 55/55 [00:03<00:00, 15.82it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5: Train Loss = 1.477817, Val Loss = 1.674191\n✓ Saved best model (val_loss=1.674191)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 6/200: 100%|██████████| 55/55 [00:03<00:00, 15.95it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6: Train Loss = 1.300288, Val Loss = 1.637229\n✓ Saved best model (val_loss=1.637229)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 7/200: 100%|██████████| 55/55 [00:03<00:00, 16.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7: Train Loss = 1.151557, Val Loss = 1.623874\n✓ Saved best model (val_loss=1.623874)\n","output_type":"stream"},{"name":"stderr","text":"Epoch 8/200: 100%|██████████| 55/55 [00:03<00:00, 16.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8: Train Loss = 1.022859, Val Loss = 1.625003\n","output_type":"stream"},{"name":"stderr","text":"Epoch 9/200: 100%|██████████| 55/55 [00:03<00:00, 16.43it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9: Train Loss = 0.923676, Val Loss = 1.624463\n","output_type":"stream"},{"name":"stderr","text":"Epoch 10/200: 100%|██████████| 55/55 [00:03<00:00, 16.13it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10: Train Loss = 0.844318, Val Loss = 1.637386\n","output_type":"stream"},{"name":"stderr","text":"Epoch 11/200: 100%|██████████| 55/55 [00:03<00:00, 15.32it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11: Train Loss = 0.768754, Val Loss = 1.649925\n","output_type":"stream"},{"name":"stderr","text":"Epoch 12/200: 100%|██████████| 55/55 [00:03<00:00, 16.49it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12: Train Loss = 0.712459, Val Loss = 1.638953\n","output_type":"stream"},{"name":"stderr","text":"Epoch 13/200: 100%|██████████| 55/55 [00:03<00:00, 16.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13: Train Loss = 0.661334, Val Loss = 1.674062\nFinished training. Now testing using best model...\nTest Results: {'mrr': 0.9309442176248136, 'ndcg': 0.9478297591884011, 'recall_at_1': 0.8876, 'recall_at_3': 0.97072, 'recall_at_5': 0.98288, 'recall_at_10': 0.99344, 'recall_at_50': 0.99968, 'l2_dist': 241.52784729003906}\n","output_type":"stream"}],"execution_count":65},{"cell_type":"code","source":"generate_submission(model, Path(test_path), device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T19:00:23.760052Z","iopub.execute_input":"2025-11-01T19:00:23.760330Z","iopub.status.idle":"2025-11-01T19:00:26.571133Z","shell.execute_reply.started":"2025-11-01T19:00:23.760311Z","shell.execute_reply":"2025-11-01T19:00:26.570262Z"}},"outputs":[{"name":"stdout","text":"Generating submission file...\n✓ Saved submission to submission.csv\n","output_type":"stream"},{"execution_count":66,"output_type":"execute_result","data":{"text/plain":"        id                                          embedding\n0        1  [0.8063499927520752, -1.2960084676742554, 6.71...\n1        2  [0.33625540137290955, 1.8729370832443237, -1.3...\n2        3  [-0.25203096866607666, 1.7044193744659424, 3.7...\n3        4  [8.706969261169434, -6.267874240875244, -1.426...\n4        5  [8.63264274597168, 9.660758018493652, 0.019903...\n...    ...                                                ...\n1495  1496  [1.6442174911499023, 1.4861117601394653, 15.40...\n1496  1497  [5.36964225769043, 9.188952445983887, 14.46781...\n1497  1498  [1.1236704587936401, -10.325639724731445, 3.51...\n1498  1499  [-3.5596485137939453, -1.9238801002502441, -2....\n1499  1500  [-0.20316056907176971, -11.891079902648926, -1...\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.8063499927520752, -1.2960084676742554, 6.71...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[0.33625540137290955, 1.8729370832443237, -1.3...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[-0.25203096866607666, 1.7044193744659424, 3.7...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[8.706969261169434, -6.267874240875244, -1.426...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[8.63264274597168, 9.660758018493652, 0.019903...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1496</td>\n      <td>[1.6442174911499023, 1.4861117601394653, 15.40...</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1497</td>\n      <td>[5.36964225769043, 9.188952445983887, 14.46781...</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1498</td>\n      <td>[1.1236704587936401, -10.325639724731445, 3.51...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1499</td>\n      <td>[-3.5596485137939453, -1.9238801002502441, -2....</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1500</td>\n      <td>[-0.20316056907176971, -11.891079902648926, -1...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":66},{"cell_type":"code","source":"from torchsummary import summary\nsummary(model, input_size=(1024,))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T19:00:26.572094Z","iopub.execute_input":"2025-11-01T19:00:26.572404Z","iopub.status.idle":"2025-11-01T19:00:26.581504Z","shell.execute_reply.started":"2025-11-01T19:00:26.572378Z","shell.execute_reply":"2025-11-01T19:00:26.580756Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Linear-1                 [-1, 1256]       1,287,400\n              SiLU-2                 [-1, 1256]               0\n         LayerNorm-3                 [-1, 1256]           2,512\n           Dropout-4                 [-1, 1256]               0\n            Linear-5                 [-1, 2048]       2,574,336\n              SiLU-6                 [-1, 2048]               0\n         LayerNorm-7                 [-1, 2048]           4,096\n           Dropout-8                 [-1, 2048]               0\n            Linear-9                 [-1, 1536]       3,147,264\n           Linear-10                  [-1, 256]         262,400\n             SiLU-11                  [-1, 256]               0\n        LayerNorm-12                  [-1, 256]             512\n          Dropout-13                  [-1, 256]               0\n           Linear-14                  [-1, 128]          32,896\n             SiLU-15                  [-1, 128]               0\n        LayerNorm-16                  [-1, 128]             256\n          Dropout-17                  [-1, 128]               0\n           Linear-18                    [-1, 1]             129\n         Softplus-19                    [-1, 1]               0\n================================================================\nTotal params: 7,311,801\nTrainable params: 7,311,801\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward/backward pass size (MB): 0.12\nParams size (MB): 27.89\nEstimated Total Size (MB): 28.02\n----------------------------------------------------------------\n","output_type":"stream"}],"execution_count":67},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\ndef objective(trial, train_dataset, val_dataset,\n              epochs: int = 10, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n    layer_choices = [512, 1024, 2048, 4096]\n    hidden_layers = [trial.suggest_categorical(f\"n_units_l{i}\", layer_choices) for i in range(n_layers)]\n\n    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024, 2048, 4096])\n    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n    temp = trial.suggest_float(\"temp\", 0.05, 0.3, log=True)\n    dropout_rate = trial.suggest_categorical('dropout_rate', [0.1,0.2,0.3,0.4,0.5])\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    model = Translator(input_dim=input_dim, output_dim=output_dim, hidden_layers=hidden_layers, dropout_rate=dropout_rate)\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            y_batch = F.normalize(y_batch, dim=-1)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = info_nce_loss(outputs, y_batch, temp=model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        model.eval()\n        val_loss = 0.0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                y_batch = F.normalize(y_batch, dim=-1)\n                \n                outputs = model(X_batch)\n                loss = info_nce_loss(outputs, y_batch, temp=model.logit_scale)\n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n        results = test(val_dataset, model, device)\n\n        trial.report(results['mrr'], epoch)\n\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n\n    return results['mrr']\n\n\ndef run_optuna_search(data_path: Path,\n                      n_trials: int = 30, epochs: int = 10,\n                      n_jobs: int = 1, sampler=None, pruner=None):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n    train_dataset, val_dataset = load_data(data_path)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset,\n                                   epochs=epochs)\n    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n\n    print(\"Study statistics:\")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Best trial:\")\n    trial = study.best_trial\n    print(\"    Value: \", trial.value)\n    print(\"    Params: \")\n    for k, v in trial.params.items():\n        print(f\"      {k}: {v}\")\n\n    return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-01T19:00:26.582321Z","iopub.execute_input":"2025-11-01T19:00:26.582608Z","iopub.status.idle":"2025-11-01T19:00:26.835723Z","shell.execute_reply.started":"2025-11-01T19:00:26.582588Z","shell.execute_reply":"2025-11-01T19:00:26.834849Z"}},"outputs":[],"execution_count":68},{"cell_type":"code","source":"study = run_optuna_search(data_path=data_path,\n                          n_trials=100, epochs=10, n_jobs=1)\noptuna.study.study.Storage  \nstudy.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n\nbest_trial_number = study.best_trial.number\nprint(\"Best params:\", study.best_params)\nprint(\"Best trial number:\", study.best_trial.number)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}