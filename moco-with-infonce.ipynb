{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom copy import deepcopy\n\nclass SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        hidden_layers,\n        activation,\n        dropout_rate\n    ):\n        super().__init__()\n\n        layers = []\n        last = input_dim\n\n        for hidden in hidden_layers:\n            layers += [\n                nn.Linear(last, hidden),\n                nn.LayerNorm(hidden),\n                activation(),\n                nn.Dropout(dropout_rate)\n            ]\n            last = hidden\n\n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n\n    def forward(self, x):\n      return F.normalize(self.net(x), p=2, dim=1)\n\n# @torch.no_grad()\n# def momentum_update(model_q, model_k, m=0.999):\n#     \"\"\"Momentum update of the key encoder (model_k) using model_q weights.\"\"\"\n#     for param_q, param_k in zip(model_q.parameters(), model_k.parameters()):\n#         param_k.data = param_k.data * m + param_q.data * (1. - m)","metadata":{"id":"pErT3mEBFSji","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class MoCoTranslator(nn.Module):\n    def __init__(self, base_model, momentum=0.999, queue_size=65536):\n        super().__init__()\n        self.query_encoder = base_model\n        # Momentum encoder: starts with same weights\n        self.key_encoder = deepcopy(base_model)\n        for param in self.key_encoder.parameters():\n            param.requires_grad = False  # momentum encoder is not updated by gradients\n\n        self.momentum = momentum\n        self.register_buffer(\"queue\", F.normalize(torch.randn(queue_size, base_model.net[-1].out_features), dim=1))\n        self.register_buffer(\"queue_ptr\", torch.zeros(1, dtype=torch.long))\n\n    @torch.no_grad()\n    def _momentum_update_key_encoder(self):\n        \"\"\"Momentum update of the key encoder\"\"\"\n        for q_param, k_param in zip(self.query_encoder.parameters(), self.key_encoder.parameters()):\n            k_param.data = self.momentum * k_param.data + (1.0 - self.momentum) * q_param.data\n\n    @torch.no_grad()\n    def _dequeue_and_enqueue(self, keys):\n        \"\"\"Update the queue with the latest keys\"\"\"\n        batch_size = keys.shape[0]\n        ptr = int(self.queue_ptr)\n        queue_size = self.queue.shape[0]\n\n        if ptr + batch_size <= queue_size:\n            self.queue[ptr:ptr+batch_size] = keys\n        else:\n            # wrap around if end of queue\n            first_part = queue_size - ptr\n            self.queue[ptr:] = keys[:first_part]\n            self.queue[:batch_size - first_part] = keys[first_part:]\n\n        ptr = (ptr + batch_size) % queue_size\n        self.queue_ptr[0] = ptr","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n\n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n\n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n\n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n\n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n\n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n\n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n\n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n\n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n\n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n\n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: nn.Module, device) -> dict:\n    gt_indices = torch.arange(len(y_val))\n\n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n\n    return results\n\n\ndef generate_submission(model: nn.Module, test_path: Path, output_file=\"submission-dirmodel.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"âœ“ Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"id":"umdq3vIdKFfa","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\n# def info_nce_loss(\n#     dir_preds,\n#     img_targets,\n#     logit_scale: torch.Tensor,\n#     margin: float = 0.3,\n#     alpha: float = 0.7\n# ):\n#     \"\"\"\n#     InfoNCE simmetrico + hard-negative Margin Ranking Loss su entrambe le direzioni.\n#     \"\"\"\n#     dir_preds = F.normalize(dir_preds, dim=-1)\n#     img_targets = F.normalize(img_targets, dim=-1)\n\n#     # Clamp logit scale per stabilitÃ \n#     logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n#     # --- InfoNCE simmetrico ---\n#     logits = dir_preds @ img_targets.T * logit_scale.exp()\n#     labels = torch.arange(logits.size(0), device=logits.device)\n#     loss_t2i = F.cross_entropy(logits, labels)\n#     loss_i2t = F.cross_entropy(logits.T, labels)\n#     loss_nce = 0.5 * (loss_t2i + loss_i2t)\n\n#     # --- Hard negative Margin Ranking Loss per testo â†’ immagine ---\n#     mask = torch.eye(logits.size(0), device=logits.device)\n#     logits_no_pos = logits - mask * 1e9\n#     hardest_neg_t2i = logits_no_pos.max(dim=1).values\n#     positive_sim_t2i = torch.diag(logits)\n#     loss_hard_t2i = F.relu(hardest_neg_t2i - positive_sim_t2i + margin).mean()\n\n#     # --- Hard negative Margin Ranking Loss per immagine â†’ testo ---\n#     logits_no_pos_i2t = logits.T - mask * 1e9\n#     hardest_neg_i2t = logits_no_pos_i2t.max(dim=1).values\n#     positive_sim_i2t = torch.diag(logits.T)\n#     loss_hard_i2t = F.relu(hardest_neg_i2t - positive_sim_i2t + margin).mean()\n\n#     # Loss finale combinata\n#     loss_hard_total = 0.5 * (loss_hard_t2i + loss_hard_i2t)\n#     loss = loss_nce + alpha * loss_hard_total\n\n#     return loss\n    \n# def info_nce_loss_with_queue(\n#     dir_preds,\n#     img_targets,\n#     queue,                # shape: (queue_size, embed_dim)\n#     logit_scale: torch.Tensor,\n#     margin: float = 0.3,\n#     alpha: float = 0.7\n# ):\n#     \"\"\"\n#     InfoNCE + Hard-negative Margin Ranking Loss with Queue Negatives.\n#     dir_preds: textâ†’image predictions (batch_size, dim)\n#     img_targets: image embeddings (batch_size, dim)\n#     queue: queue of image embeddings (queue_size, dim)\n#     \"\"\"\n#     # Normalize everything\n#     dir_preds = F.normalize(dir_preds, dim=-1)\n#     img_targets = F.normalize(img_targets, dim=-1)\n#     queue = F.normalize(queue, dim=-1)\n\n#     # Clamp for stability\n#     logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n#     batch_size = dir_preds.size(0)\n\n#     # -----------------------------------------\n#     # TEXT â†’ IMAGE direction with queue negatives\n#     # -----------------------------------------\n#     # Positive: same-index pairs\n#     l_pos = torch.sum(dir_preds * img_targets, dim=-1, keepdim=True)  # (B, 1)\n#     # Negatives: all queued keys\n#     l_neg = torch.mm(dir_preds, queue.T)  # (B, Q)\n#     logits_t2i = torch.cat([l_pos, l_neg], dim=1) * logit_scale.exp()\n#     labels = torch.zeros(batch_size, dtype=torch.long, device=dir_preds.device)\n#     loss_t2i = F.cross_entropy(logits_t2i, labels)\n\n#     # -----------------------------------------\n#     # IMAGE â†’ TEXT direction with queue negatives\n#     # -----------------------------------------\n#     # Positive: same-index pairs\n#     l_pos_i2t = torch.sum(img_targets * dir_preds, dim=-1, keepdim=True)\n#     l_neg_i2t = torch.mm(img_targets, queue.T)\n#     logits_i2t = torch.cat([l_pos_i2t, l_neg_i2t], dim=1) * logit_scale.exp()\n#     loss_i2t = F.cross_entropy(logits_i2t, labels)\n#     loss_nce = 0.5 * (loss_t2i + loss_i2t)\n\n#     # -----------------------------------------\n#     # Hard Negative Margin Ranking Loss\n#     # -----------------------------------------\n#     # Combine both in-batch + queue negatives for hard mining\n#     all_img = torch.cat([img_targets, queue], dim=0)\n#     all_txt = torch.cat([dir_preds, queue], dim=0)\n\n#     sim_matrix = dir_preds @ all_img.T * logit_scale.exp()\n#     mask = torch.eye(batch_size, device=dir_preds.device)\n#     sim_no_pos = sim_matrix - mask * 1e9\n#     hardest_neg_t2i = sim_no_pos.max(dim=1).values\n#     positive_sim_t2i = torch.sum(dir_preds * img_targets, dim=1)\n#     loss_hard_t2i = F.relu(hardest_neg_t2i - positive_sim_t2i + margin).mean()\n\n#     sim_matrix_i2t = img_targets @ all_txt.T * logit_scale.exp()\n#     sim_no_pos_i2t = sim_matrix_i2t - mask * 1e9\n#     hardest_neg_i2t = sim_no_pos_i2t.max(dim=1).values\n#     positive_sim_i2t = torch.sum(img_targets * dir_preds, dim=1)\n#     loss_hard_i2t = F.relu(hardest_neg_i2t - positive_sim_i2t + margin).mean()\n\n#     loss_hard_total = 0.5 * (loss_hard_t2i + loss_hard_i2t)\n\n#     # -----------------------------------------\n#     # Final combined loss\n#     # -----------------------------------------\n#     loss = loss_nce + alpha * loss_hard_total\n\n#     return loss\n\n\ndef train_model_direction(model,save_path, train_dataset, val_dataset,\n                          batch_size=1024, epochs=250, lr=0.01, patience=5,\n                          reg_lambda=0.01):\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=3, threshold=0.001, min_lr=1e-9\n    )\n\n    best_mrr = float('-inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n\n        for X_batch, y_batch in progress_bar:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            #y_batch = F.normalize(y_batch, p=2, dim=-1)\n            #X_batch = F.normalize(X_batch, p=2, dim=-1)\n                # ====================\n            # Compute query embeddings\n            # ====================\n            q = model.query_encoder(X_batch)  # normalized by your model's forward\n        \n            # ====================\n            # Compute key embeddings (momentum encoder)\n            # ====================\n            with torch.no_grad():\n                model._momentum_update_key_encoder()\n                k = F.normalize(y_batch, dim=1)\n        \n            # -------------------\n            # InfoNCE with queue\n            # -------------------\n            l_pos = torch.sum(q * k, dim=1, keepdim=True)\n            l_neg = torch.mm(q, model.queue.clone().detach().T)\n            logits = torch.cat([l_pos, l_neg], dim=1)\n            logits *= model.query_encoder.logit_scale.exp()\n\n            labels = torch.zeros(logits.size(0), dtype=torch.long, device=device)\n            loss = F.cross_entropy(logits, labels)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # -------------------\n            # Update queue\n            # -------------------\n            with torch.no_grad():\n                model._dequeue_and_enqueue(k)\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        # Validation\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                #y_batch = F.normalize(y_batch, p=2, dim=-1)\n                #X_batch = F.normalize(X_batch, p=2, dim=-1)\n\n                q = model.query_encoder(X_batch)\n                k = F.normalize(y_batch, dim=1)\n                # -------------------\n                # InfoNCE with queue\n                # -------------------\n                l_pos = torch.sum(q * k, dim=1, keepdim=True)\n                l_neg = torch.mm(q, model.queue.clone().detach().T)\n                logits = torch.cat([l_pos, l_neg], dim=1)\n                logits *= model.query_encoder.logit_scale.exp()\n                labels = torch.zeros(logits.size(0), dtype=torch.long, device=device)\n                loss = F.cross_entropy(logits, labels)\n\n                running_val_loss += loss.item()\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        results = test(val_dataset, model.query_encoder, device)\n        mrr = results['mrr']\n\n        scheduler.step(mrr)\n\n        print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | MRR: {mrr:.6f} | Recall-1: {results['recall_at_1']:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        if mrr > best_mrr:\n            best_mrr = mrr\n            no_improvements = 0\n            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), save_path)\n            print(f\"ðŸ’¾ Saved new best model (MRR={mrr:.6f})\")\n        else:\n            no_improvements += 1\n            if no_improvements >= patience:\n                print(\"â¹ Early stopping triggered.\")\n                break\n\n    print(f\"âœ… Training complete. Best MRR: {best_mrr:.6f}\")\n    return model\n\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n\ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n\n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n\n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model: nn.Module, device):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device)\n    return results","metadata":{"id":"_Ng-afH6FqPt","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n\nsave_path = '/kaggle/working/models/dir-model.pth'","metadata":{"id":"X_QhTeUoFrLm","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x, y = get_data(data_path)\ntrain_dataset, val_dataset = get_datasets(x, y)","metadata":{"id":"zI0blWNyFtDI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3b312e46-853d-49ad-b154-074f9d0cb098","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"x.shape[1]\ny.shape[1]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"input_dim = x.shape[1]\noutput_dim = y.shape[1]\nhidden_layers=[1256, 1536]\ndropout_rate = 0.5\n\nbatch_size= 5000\nlr=0.01\nepochs= 100\npatience = 10\n\nmodel_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    'hidden_layers': hidden_layers,\n    'dropout_rate': dropout_rate,\n    'activation': nn.GELU\n}\n\nmodel = SpaceTranslator(**model_args).to(device)\n\nmoco_model = MoCoTranslator(base_model=model)\n\ntrain_model_direction(moco_model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience)\n\nprint('Finished training. Now testing using best model...')\n\n# state = torch.load(save_path)\n# moco_model.load_state_dict(state)\n# results = test(val_dataset, moco_model.query_encoder, device)\n# print(\"Test Results:\", results)","metadata":{"id":"P5k77RkVFuwg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"fdd8c69d-9521-4cac-bf8a-88893e6b3ce6","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"state = torch.load(save_path)\nmoco_model.load_state_dict(state)\nresults = test(val_dataset, moco_model.query_encoder, device)\nprint(\"Test Results:\", results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"generate_submission(moco_model.query_encoder, Path(test_path), output_file=\"MoCo.csv\", device=device)","metadata":{"id":"_Wk_esyYFw8X","colab":{"base_uri":"https://localhost:8080/","height":459},"outputId":"cfd93622-2278-4903-a679-50eacea40f70","trusted":true},"outputs":[],"execution_count":null}]}