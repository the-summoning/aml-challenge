{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13611855,"sourceType":"datasetVersion","datasetId":8650259}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\n\nclass _SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        hidden_layers,\n        activation,\n        dropout_rate\n    ):\n        super().__init__()\n\n        layers = []\n        last = input_dim\n\n        for hidden in hidden_layers:\n            layers += [\n                nn.Linear(last, hidden),\n                # nn.LayerNorm(hidden),\n                nn.BatchNorm1d(hidden),\n                activation(),\n                nn.Dropout(dropout_rate)\n            ]\n            last = hidden\n        self.body = nn.Sequential(*layers)\n\n        self.final = nn.Linear(last, output_dim)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.kaiming_uniform_(module.weight, a=0, mode='fan_in', nonlinearity='relu')\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm) or isinstance(module, nn.BatchNorm1d):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    def forward(self, x):\n        x = self.body(x)\n        return self.final(x)\n\n\n\nclass SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        hidden_layers,\n        activation,\n        dropout_rate\n    ):\n        super().__init__()\n\n        layers = []\n        last = input_dim\n\n        for hidden in hidden_layers:\n            layers += [\n                nn.Linear(last, hidden),\n                nn.LayerNorm(hidden),\n                activation(),\n                nn.Dropout(dropout_rate)\n            ]\n            last = hidden\n\n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n\n    def forward(self, x):\n      return F.normalize(self.net(x), p=2, dim=1)\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:53:25.476694Z","iopub.execute_input":"2025-11-07T22:53:25.476951Z","iopub.status.idle":"2025-11-07T22:53:29.362390Z","shell.execute_reply.started":"2025-11-07T22:53:25.476932Z","shell.execute_reply":"2025-11-07T22:53:29.361790Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport torch\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n    \n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n    \n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n    \n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n    \n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n        \n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n    \n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n    \n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n    \n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n    \n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n    \n    return results\n\n\n\ndef generate_submission(models: nn.Module, test_path: Path, ensemble, output_file=\"submission-dirmodel.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    '''with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()'''\n    with torch.no_grad():\n        if ensemble:\n            preds = []\n            for model in models:\n                model.eval()\n                preds.append(model(test_embds.to(device)).to('cpu'))\n                \n            pred_embds = torch.stack(preds).mean(dim=0)\n        else:\n            pred_embds = model(test_embds.to(device)).cpu()\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"✓ Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T23:04:17.083207Z","iopub.execute_input":"2025-11-07T23:04:17.083479Z","iopub.status.idle":"2025-11-07T23:04:17.098529Z","shell.execute_reply.started":"2025-11-07T23:04:17.083458Z","shell.execute_reply":"2025-11-07T23:04:17.097817Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingLR\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef compute_targets_metrics(train_dataset, device):\n    sum_vec = None\n    sum_sq = None\n    n = len(train_dataset)\n    dummy_loader = DataLoader(train_dataset, len(train_dataset))\n    for x,y in dummy_loader:\n        b = y.shape[0]\n\n    sum_vec = y.sum(dim=0).double().to(device)\n    sum_sq = (y * y).sum(dim=0).double().to(device)\n    \n    target_mean = (sum_vec / n).float().to(device)\n    var = (sum_sq / n - target_mean.double()**2).float().clamp(min=0.0)\n    target_std = torch.sqrt(var + 1e-12).to(device)\n    return target_mean, target_std\n\n    \ndef moment_loss(preds, target_mean, target_std, mode='per_dim', coef=1e-2):\n    if mode == 'per_dim':\n        pm = preds.mean(dim=0)\n        ps = preds.std(dim=0)\n        loss_mean = F.mse_loss(pm, target_mean)\n        loss_std  = F.mse_loss(ps,  target_std)\n        return coef * (loss_mean + loss_std)\n    elif mode == 'scalar':\n        pm = preds.mean()\n        ps = preds.std()\n        tm = target_mean.mean()\n        ts = target_std.mean()\n        return coef * (F.mse_loss(pm, tm) + F.mse_loss(ps, ts))\n    \n'''\ndef info_nce_loss(dir_preds, img_targets, logit_scale):\n    dir_preds = F.normalize(dir_preds, dim=-1)\n    img_targets = F.normalize(img_targets, dim=-1)\n    \n    logit_scale = torch.clamp(logit_scale, max=np.log(100))\n    logits = (dir_preds @ img_targets.T) * logit_scale.exp()\n\n    labels = torch.arange(logits.size(0), device=logits.device)\n\n    loss_t2i = F.cross_entropy(logits, labels)\n    loss_i2t = F.cross_entropy(logits.T, labels)\n\n    return 0.5 * (loss_t2i + loss_i2t)\n'''\n\ndef info_nce_loss(\n    dir_preds,\n    img_targets,\n    logit_scale: torch.Tensor,\n    margin: float = 0.3,\n    alpha: float = 0.7\n):\n    \"\"\"\n    InfoNCE simmetrico + hard-negative Margin Ranking Loss su entrambe le direzioni.\n    \"\"\"\n    dir_preds = F.normalize(dir_preds, dim=-1)\n    img_targets = F.normalize(img_targets, dim=-1)\n\n    # Clamp logit scale per stabilità\n    logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n    # --- InfoNCE simmetrico ---\n    logits = dir_preds @ img_targets.T * logit_scale.exp()\n    labels = torch.arange(logits.size(0), device=logits.device)\n    loss_t2i = F.cross_entropy(logits, labels)\n    loss_i2t = F.cross_entropy(logits.T, labels)\n    loss_nce = 0.5 * (loss_t2i + loss_i2t)\n\n    # --- Hard negative Margin Ranking Loss per testo → immagine ---\n    mask = torch.eye(logits.size(0), device=logits.device)\n    logits_no_pos = logits - mask * 1e9\n    hardest_neg_t2i = logits_no_pos.max(dim=1).values\n    positive_sim_t2i = torch.diag(logits)\n    loss_hard_t2i = F.relu(hardest_neg_t2i - positive_sim_t2i + margin).mean()\n\n    # --- Hard negative Margin Ranking Loss per immagine → testo ---\n    logits_no_pos_i2t = logits.T - mask * 1e9\n    hardest_neg_i2t = logits_no_pos_i2t.max(dim=1).values\n    positive_sim_i2t = torch.diag(logits.T)\n    loss_hard_i2t = F.relu(hardest_neg_i2t - positive_sim_i2t + margin).mean()\n\n    # Loss finale combinata\n    loss_hard_total = 0.5 * (loss_hard_t2i + loss_hard_i2t)\n    loss = loss_nce + alpha * loss_hard_total\n\n    return loss\n\n\n\n\ndef train_model_direction(\n    model: SpaceTranslator,\n    model_path: Path,\n    train_dataset: TensorDataset,\n    val_dataset: TensorDataset,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    patience: int,\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    print(f\"Using device: {device}\")\n\n    # Dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-3)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=3, threshold=0.001, min_lr=1e-9\n    )\n    \n    ema_state = {k: v.clone().detach().cpu() for k, v in model.state_dict().items()}\n    ema_decay = 0.9995\n    \n    best_mrr = float('-inf')\n    best_val_loss = float('inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_train_loss = 0.0\n\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n        for X_batch, y_batch in progress_bar:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            # X_batch = F.normalize(X_batch, dim=-1)\n            # y_batch = F.normalize(y_batch, dim=-1)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            \n            loss = info_nce_loss(outputs, y_batch, model.logit_scale) #+ moment_loss(outputs, target_mean, target_std)\n            loss.backward()\n            optimizer.step()\n            ema_update(model, ema_state, ema_decay)\n\n            running_train_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        train_loss = running_train_loss / len(train_loader)\n\n        # Validation phase\n        model.eval()\n        running_val_loss = 0.0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                # X_batch = F.normalize(X_batch, dim=-1)\n                # y_batch = F.normalize(y_batch, dim=-1)\n\n                outputs = model(X_batch)\n                loss = info_nce_loss(outputs, y_batch, model.logit_scale) #+ moment_loss(outputs, target_mean, target_std)\n\n                running_val_loss += loss.item()\n\n        val_loss = running_val_loss / len(val_loader)\n\n        \n\n        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        # Optional: external validation/test\n        results = test(val_dataset, model, device)\n        print(results)\n        mrr = results['mrr']\n\n        # Step the scheduler\n        scheduler.step(mrr)\n        \n        # Save best model and early stopping\n        if mrr > best_mrr:\n            best_mrr = mrr\n            no_improvements = 0\n\n            Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), model_path)\n            print(f\"Saved new best model (mrr={mrr:.6f})\")\n        else:\n            no_improvements += 1\n            if no_improvements >= patience:\n                print(\"Early stopping triggered.\")\n                break\n\n    print(f\"Training complete. Best mrr: {mrr:.6f}\")\n\n    return model, ema_state\n\ndef ema_update(model, ema_state, decay):\n    with torch.no_grad():\n        for k, v in model.state_dict().items():\n            if v.dtype.is_floating_point:\n                ema_state[k].mul_(decay).add_(v.detach().cpu(), alpha=1 - decay)\n\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n\n    \ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:    \n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n    \n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n    \n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model, device, scale=None):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device, scale=scale)\n    return results\n    \ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model, device, scale=None) -> dict:\n    gt_indices = torch.arange(len(y_val))\n    \n    model.eval()\n\n    with torch.no_grad():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n    \n    return results\n\n\ndef test_ensemble(val_dataset: TensorDataset, models, device, scale=None):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_ensemble(x_val, y_val, models=models, device=device, scale=scale)\n    return results\n    \n\ndef eval_ensemble(x_val: np.ndarray, y_val: np.ndarray, models, device, scale=None) -> dict:\n    gt_indices = torch.arange(len(y_val))\n    preds = []\n    with torch.no_grad():\n        for model in models:\n            model.eval()\n            preds.append(model(x_val.to(device)).to('cpu'))\n            \n    avg_pred = torch.stack(preds).mean(dim=0)\n\n    results = evaluate_retrieval(avg_pred, y_val, gt_indices)\n    \n    return results\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:53:29.631241Z","iopub.execute_input":"2025-11-07T22:53:29.631609Z","iopub.status.idle":"2025-11-07T22:53:29.653844Z","shell.execute_reply.started":"2025-11-07T22:53:29.631573Z","shell.execute_reply":"2025-11-07T22:53:29.653294Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_dim = 1024\noutput_dim = 1536\nhidden_layers=[1256, 1536]\ndropout_rate = 0.5\n\nbatch_size= 4096\nlr=0.01\nepochs= 250\npatience = 10\n\n\ndata_path= '/kaggle/input/amlcomp/data/train/train.npz'\ntest_path= '/kaggle/input/amlcomp/data/test/test.clean.npz'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:53:29.655433Z","iopub.execute_input":"2025-11-07T22:53:29.655616Z","iopub.status.idle":"2025-11-07T22:53:29.756485Z","shell.execute_reply.started":"2025-11-07T22:53:29.655602Z","shell.execute_reply":"2025-11-07T22:53:29.755946Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dir_x, dir_y = get_data(data_path)\n\ntarget_norm_mean = dir_y.norm(dim=1).mean().item()\n\ndir_train_dataset, dir_val_dataset = get_datasets(dir_x, dir_y)\n\nprint('Target norm mean', target_norm_mean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:53:29.757209Z","iopub.execute_input":"2025-11-07T22:53:29.757452Z","iopub.status.idle":"2025-11-07T22:53:47.248676Z","shell.execute_reply.started":"2025-11-07T22:53:29.757435Z","shell.execute_reply":"2025-11-07T22:53:47.247851Z"}},"outputs":[{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\nTarget norm mean 25.93919563293457\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"\nmodel_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    'hidden_layers': hidden_layers,\n    'dropout_rate': dropout_rate,\n    'activation': nn.GELU,\n}\n\nlast_model = SpaceTranslator(**model_args).to(device)\nlast_model, ema_state = train_model_direction(last_model, './models/best_model.pth', dir_train_dataset, dir_val_dataset, batch_size, epochs, lr, patience)\n\nprint('Finished training. Now testing using best model...')\n\nema_model = SpaceTranslator(**model_args).to(device)\nsd = {k: v.to(device) for k, v in ema_state.items()}\nema_model.load_state_dict(sd)\n\nbest_model = SpaceTranslator(**model_args).to(device)\nstate = torch.load('./models/best_model.pth')\nbest_model.load_state_dict(state)\n\nmodels = [best_model, last_model, ema_model]\nresults = test_ensemble(dir_val_dataset, models, device)\nprint(\"Test Results:\", results)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T22:53:50.063281Z","iopub.execute_input":"2025-11-07T22:53:50.063488Z","iopub.status.idle":"2025-11-07T23:03:50.092065Z","shell.execute_reply.started":"2025-11-07T22:53:50.063473Z","shell.execute_reply":"2025-11-07T23:03:50.091231Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | Train Loss: 8.380910 | Val Loss: 7.383249 | LR: 1.00e-02\n{'mrr': 0.35580608111744055, 'ndcg': 0.49583899478748644, 'recall_at_1': 0.18932, 'recall_at_3': 0.41204, 'recall_at_5': 0.5488, 'recall_at_10': 0.7406, 'recall_at_50': 0.98944, 'l2_dist': 25.613479614257812}\nSaved new best model (mrr=0.355806)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 002 | Train Loss: 7.389417 | Val Loss: 6.538291 | LR: 1.00e-02\n{'mrr': 0.5540739395520453, 'ndcg': 0.657135108267455, 'recall_at_1': 0.37944, 'recall_at_3': 0.66484, 'recall_at_5': 0.78676, 'recall_at_10': 0.9048, 'recall_at_50': 0.9974, 'l2_dist': 25.63945770263672}\nSaved new best model (mrr=0.554074)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 003 | Train Loss: 6.656401 | Val Loss: 5.798542 | LR: 1.00e-02\n{'mrr': 0.6746046666404142, 'ndcg': 0.7517534664516107, 'recall_at_1': 0.5212, 'recall_at_3': 0.79124, 'recall_at_5': 0.87868, 'recall_at_10': 0.95372, 'recall_at_50': 0.9986, 'l2_dist': 25.6640567779541}\nSaved new best model (mrr=0.674605)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 004 | Train Loss: 6.003433 | Val Loss: 5.101874 | LR: 1.00e-02\n{'mrr': 0.7597227165049936, 'ndcg': 0.8175587733763083, 'recall_at_1': 0.63316, 'recall_at_3': 0.86472, 'recall_at_5': 0.92772, 'recall_at_10': 0.97404, 'recall_at_50': 0.99896, 'l2_dist': 25.67354965209961}\nSaved new best model (mrr=0.759723)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 005 | Train Loss: 5.427717 | Val Loss: 4.548949 | LR: 1.00e-02\n{'mrr': 0.8108942331647567, 'ndcg': 0.8568531328198261, 'recall_at_1': 0.70336, 'recall_at_3': 0.90488, 'recall_at_5': 0.95016, 'recall_at_10': 0.98324, 'recall_at_50': 0.99932, 'l2_dist': 25.693864822387695}\nSaved new best model (mrr=0.810894)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 006 | Train Loss: 4.946915 | Val Loss: 4.133269 | LR: 1.00e-02\n{'mrr': 0.8464540934941884, 'ndcg': 0.8839154049114717, 'recall_at_1': 0.75652, 'recall_at_3': 0.92568, 'recall_at_5': 0.96276, 'recall_at_10': 0.98748, 'recall_at_50': 0.99932, 'l2_dist': 25.71617317199707}\nSaved new best model (mrr=0.846454)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 007 | Train Loss: 4.577795 | Val Loss: 3.851835 | LR: 1.00e-02\n{'mrr': 0.8648983891531877, 'ndcg': 0.8979857975738033, 'recall_at_1': 0.78276, 'recall_at_3': 0.93856, 'recall_at_5': 0.96868, 'recall_at_10': 0.9896, 'recall_at_50': 0.99952, 'l2_dist': 25.73394203186035}\nSaved new best model (mrr=0.864898)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 008 | Train Loss: 4.273684 | Val Loss: 3.642885 | LR: 1.00e-02\n{'mrr': 0.8789125186376634, 'ndcg': 0.9086085171807898, 'recall_at_1': 0.80408, 'recall_at_3': 0.9474, 'recall_at_5': 0.97256, 'recall_at_10': 0.99048, 'recall_at_50': 0.99952, 'l2_dist': 25.753223419189453}\nSaved new best model (mrr=0.878913)\n","output_type":"stream"},{"name":"stderr","text":"                                                                       \r","output_type":"stream"},{"name":"stdout","text":"Epoch 009 | Train Loss: 4.017218 | Val Loss: 3.463404 | LR: 1.00e-02\n{'mrr': 0.8903844087722886, 'ndcg': 0.9172789375365563, 'recall_at_1': 0.82248, 'recall_at_3': 0.95156, 'recall_at_5': 0.97564, 'recall_at_10': 0.99148, 'recall_at_50': 0.99948, 'l2_dist': 25.76553726196289}\nSaved new best model (mrr=0.890384)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 010 | Train Loss: 3.805119 | Val Loss: 3.343525 | LR: 1.00e-02\n{'mrr': 0.8966019973236101, 'ndcg': 0.9220252132956973, 'recall_at_1': 0.83096, 'recall_at_3': 0.957, 'recall_at_5': 0.97808, 'recall_at_10': 0.99228, 'recall_at_50': 0.9994, 'l2_dist': 25.778079986572266}\nSaved new best model (mrr=0.896602)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 011 | Train Loss: 3.615846 | Val Loss: 3.237405 | LR: 1.00e-02\n{'mrr': 0.9005923167356508, 'ndcg': 0.9250760432655348, 'recall_at_1': 0.837, 'recall_at_3': 0.95924, 'recall_at_5': 0.97948, 'recall_at_10': 0.99312, 'recall_at_50': 0.9994, 'l2_dist': 25.787567138671875}\nSaved new best model (mrr=0.900592)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 012 | Train Loss: 3.443288 | Val Loss: 3.142991 | LR: 1.00e-02\n{'mrr': 0.9076585661014327, 'ndcg': 0.9304228724765103, 'recall_at_1': 0.84804, 'recall_at_3': 0.96232, 'recall_at_5': 0.982, 'recall_at_10': 0.99356, 'recall_at_50': 0.99952, 'l2_dist': 25.794769287109375}\nSaved new best model (mrr=0.907659)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 013 | Train Loss: 3.301806 | Val Loss: 3.067406 | LR: 1.00e-02\n{'mrr': 0.9123512656359155, 'ndcg': 0.933922459051361, 'recall_at_1': 0.85608, 'recall_at_3': 0.96456, 'recall_at_5': 0.9822, 'recall_at_10': 0.99328, 'recall_at_50': 0.9994, 'l2_dist': 25.801929473876953}\nSaved new best model (mrr=0.912351)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 014 | Train Loss: 3.168260 | Val Loss: 3.019124 | LR: 1.00e-02\n{'mrr': 0.9132785006389482, 'ndcg': 0.9346240239411833, 'recall_at_1': 0.85764, 'recall_at_3': 0.96468, 'recall_at_5': 0.98268, 'recall_at_10': 0.9936, 'recall_at_50': 0.9994, 'l2_dist': 25.805822372436523}\nSaved new best model (mrr=0.913279)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 015 | Train Loss: 3.049540 | Val Loss: 2.962376 | LR: 1.00e-02\n{'mrr': 0.9180012910369498, 'ndcg': 0.9381988702052382, 'recall_at_1': 0.8648, 'recall_at_3': 0.9672, 'recall_at_5': 0.98332, 'recall_at_10': 0.9938, 'recall_at_50': 0.99944, 'l2_dist': 25.80666160583496}\nSaved new best model (mrr=0.918001)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 016 | Train Loss: 2.929943 | Val Loss: 2.927990 | LR: 1.00e-02\n{'mrr': 0.91831103037989, 'ndcg': 0.938412932853889, 'recall_at_1': 0.86556, 'recall_at_3': 0.9678, 'recall_at_5': 0.98364, 'recall_at_10': 0.99348, 'recall_at_50': 0.99936, 'l2_dist': 25.80826759338379}\nSaved new best model (mrr=0.918311)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 017 | Train Loss: 2.833404 | Val Loss: 2.903053 | LR: 1.00e-02\n{'mrr': 0.9202249908921978, 'ndcg': 0.9398535508632168, 'recall_at_1': 0.86892, 'recall_at_3': 0.96836, 'recall_at_5': 0.98348, 'recall_at_10': 0.99432, 'recall_at_50': 0.99936, 'l2_dist': 25.807594299316406}\nSaved new best model (mrr=0.920225)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 018 | Train Loss: 2.746961 | Val Loss: 2.860410 | LR: 1.00e-02\n{'mrr': 0.9212542752833387, 'ndcg': 0.9406448614973074, 'recall_at_1': 0.87016, 'recall_at_3': 0.969, 'recall_at_5': 0.98388, 'recall_at_10': 0.99428, 'recall_at_50': 0.99936, 'l2_dist': 25.807334899902344}\nSaved new best model (mrr=0.921254)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 019 | Train Loss: 2.661004 | Val Loss: 2.830985 | LR: 1.00e-02\n{'mrr': 0.9228837394119696, 'ndcg': 0.9418572410116838, 'recall_at_1': 0.87296, 'recall_at_3': 0.96904, 'recall_at_5': 0.98364, 'recall_at_10': 0.994, 'recall_at_50': 0.99928, 'l2_dist': 25.806922912597656}\nSaved new best model (mrr=0.922884)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 020 | Train Loss: 2.587376 | Val Loss: 2.829229 | LR: 1.00e-02\n{'mrr': 0.9244149224415631, 'ndcg': 0.9429819913265683, 'recall_at_1': 0.87608, 'recall_at_3': 0.96884, 'recall_at_5': 0.98424, 'recall_at_10': 0.99396, 'recall_at_50': 0.99932, 'l2_dist': 25.80609703063965}\nSaved new best model (mrr=0.924415)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 021 | Train Loss: 2.519988 | Val Loss: 2.798049 | LR: 1.00e-02\n{'mrr': 0.9252509688307761, 'ndcg': 0.9436459330507627, 'recall_at_1': 0.8768, 'recall_at_3': 0.97072, 'recall_at_5': 0.98488, 'recall_at_10': 0.99456, 'recall_at_50': 0.9994, 'l2_dist': 25.806129455566406}\nSaved new best model (mrr=0.925251)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 022 | Train Loss: 2.458097 | Val Loss: 2.790547 | LR: 1.00e-02\n{'mrr': 0.9258039490496521, 'ndcg': 0.9440449247795694, 'recall_at_1': 0.87796, 'recall_at_3': 0.97072, 'recall_at_5': 0.9846, 'recall_at_10': 0.994, 'recall_at_50': 0.99948, 'l2_dist': 25.805112838745117}\nSaved new best model (mrr=0.925804)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 023 | Train Loss: 2.404535 | Val Loss: 2.761713 | LR: 1.00e-02\n{'mrr': 0.9274354969005641, 'ndcg': 0.9452582276845849, 'recall_at_1': 0.8808, 'recall_at_3': 0.97088, 'recall_at_5': 0.98496, 'recall_at_10': 0.99368, 'recall_at_50': 0.99932, 'l2_dist': 25.803247451782227}\nSaved new best model (mrr=0.927435)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 024 | Train Loss: 2.347515 | Val Loss: 2.755812 | LR: 1.00e-02\n{'mrr': 0.928383452663581, 'ndcg': 0.9459725736936131, 'recall_at_1': 0.88232, 'recall_at_3': 0.97028, 'recall_at_5': 0.98504, 'recall_at_10': 0.9936, 'recall_at_50': 0.99948, 'l2_dist': 25.80314064025879}\nSaved new best model (mrr=0.928383)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 025 | Train Loss: 2.303176 | Val Loss: 2.745455 | LR: 1.00e-02\n{'mrr': 0.9280781939428003, 'ndcg': 0.9457310524575538, 'recall_at_1': 0.88184, 'recall_at_3': 0.97132, 'recall_at_5': 0.98468, 'recall_at_10': 0.99404, 'recall_at_50': 0.99932, 'l2_dist': 25.802305221557617}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 026 | Train Loss: 2.248813 | Val Loss: 2.735030 | LR: 1.00e-02\n{'mrr': 0.9290460212519926, 'ndcg': 0.9464614525986059, 'recall_at_1': 0.88352, 'recall_at_3': 0.97196, 'recall_at_5': 0.98476, 'recall_at_10': 0.99392, 'recall_at_50': 0.99932, 'l2_dist': 25.80044937133789}\nSaved new best model (mrr=0.929046)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 027 | Train Loss: 2.199817 | Val Loss: 2.730633 | LR: 1.00e-02\n{'mrr': 0.9288574095491803, 'ndcg': 0.9463168970332081, 'recall_at_1': 0.88308, 'recall_at_3': 0.97132, 'recall_at_5': 0.98492, 'recall_at_10': 0.99384, 'recall_at_50': 0.99928, 'l2_dist': 25.80037498474121}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 028 | Train Loss: 2.156844 | Val Loss: 2.713791 | LR: 1.00e-02\n{'mrr': 0.9293321298723217, 'ndcg': 0.9466679879865599, 'recall_at_1': 0.88448, 'recall_at_3': 0.97088, 'recall_at_5': 0.98464, 'recall_at_10': 0.99408, 'recall_at_50': 0.99932, 'l2_dist': 25.80062484741211}\nSaved new best model (mrr=0.929332)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 029 | Train Loss: 2.115837 | Val Loss: 2.711139 | LR: 1.00e-02\n{'mrr': 0.9300936436543854, 'ndcg': 0.947258838026343, 'recall_at_1': 0.88488, 'recall_at_3': 0.97192, 'recall_at_5': 0.98556, 'recall_at_10': 0.99396, 'recall_at_50': 0.99924, 'l2_dist': 25.799917221069336}\nSaved new best model (mrr=0.930094)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 030 | Train Loss: 2.079249 | Val Loss: 2.708037 | LR: 1.00e-02\n{'mrr': 0.9311285617090628, 'ndcg': 0.9480110060528593, 'recall_at_1': 0.88708, 'recall_at_3': 0.9716, 'recall_at_5': 0.98528, 'recall_at_10': 0.99376, 'recall_at_50': 0.9994, 'l2_dist': 25.798791885375977}\nSaved new best model (mrr=0.931129)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 031 | Train Loss: 2.039774 | Val Loss: 2.703317 | LR: 1.00e-02\n{'mrr': 0.9311532877025941, 'ndcg': 0.9480271596993, 'recall_at_1': 0.88724, 'recall_at_3': 0.9722, 'recall_at_5': 0.98492, 'recall_at_10': 0.99368, 'recall_at_50': 0.99928, 'l2_dist': 25.797462463378906}\nSaved new best model (mrr=0.931153)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 032 | Train Loss: 2.011385 | Val Loss: 2.686060 | LR: 1.00e-02\n{'mrr': 0.9307549318717766, 'ndcg': 0.9477416665384872, 'recall_at_1': 0.88648, 'recall_at_3': 0.97144, 'recall_at_5': 0.98512, 'recall_at_10': 0.99404, 'recall_at_50': 0.99932, 'l2_dist': 25.79694175720215}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 033 | Train Loss: 1.978727 | Val Loss: 2.686340 | LR: 1.00e-02\n{'mrr': 0.9309662506376771, 'ndcg': 0.9479085206158799, 'recall_at_1': 0.88644, 'recall_at_3': 0.97232, 'recall_at_5': 0.98532, 'recall_at_10': 0.99376, 'recall_at_50': 0.99936, 'l2_dist': 25.797170639038086}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 034 | Train Loss: 1.946780 | Val Loss: 2.685623 | LR: 1.00e-02\n{'mrr': 0.9311725753712627, 'ndcg': 0.9480304991446379, 'recall_at_1': 0.8874, 'recall_at_3': 0.9722, 'recall_at_5': 0.98472, 'recall_at_10': 0.99388, 'recall_at_50': 0.99944, 'l2_dist': 25.796310424804688}\nSaved new best model (mrr=0.931173)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 035 | Train Loss: 1.838080 | Val Loss: 2.649760 | LR: 5.00e-03\n{'mrr': 0.9326254943421306, 'ndcg': 0.9491000343471561, 'recall_at_1': 0.89024, 'recall_at_3': 0.97192, 'recall_at_5': 0.98456, 'recall_at_10': 0.99368, 'recall_at_50': 0.99932, 'l2_dist': 25.794919967651367}\nSaved new best model (mrr=0.932625)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 036 | Train Loss: 1.752867 | Val Loss: 2.636541 | LR: 5.00e-03\n{'mrr': 0.9333037091886133, 'ndcg': 0.949643514910053, 'recall_at_1': 0.8908, 'recall_at_3': 0.97324, 'recall_at_5': 0.98532, 'recall_at_10': 0.99416, 'recall_at_50': 0.99932, 'l2_dist': 25.794681549072266}\nSaved new best model (mrr=0.933304)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 037 | Train Loss: 1.709147 | Val Loss: 2.633349 | LR: 5.00e-03\n{'mrr': 0.9323623194403796, 'ndcg': 0.948930878278772, 'recall_at_1': 0.8892, 'recall_at_3': 0.97272, 'recall_at_5': 0.9854, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.794340133666992}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 038 | Train Loss: 1.686249 | Val Loss: 2.627029 | LR: 5.00e-03\n{'mrr': 0.9333713807460171, 'ndcg': 0.9496789123347132, 'recall_at_1': 0.89108, 'recall_at_3': 0.97244, 'recall_at_5': 0.98556, 'recall_at_10': 0.99384, 'recall_at_50': 0.99928, 'l2_dist': 25.793954849243164}\nSaved new best model (mrr=0.933371)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 039 | Train Loss: 1.662844 | Val Loss: 2.627471 | LR: 5.00e-03\n{'mrr': 0.9328818330534576, 'ndcg': 0.9493081420743241, 'recall_at_1': 0.89036, 'recall_at_3': 0.97192, 'recall_at_5': 0.98548, 'recall_at_10': 0.99356, 'recall_at_50': 0.99924, 'l2_dist': 25.79345703125}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 040 | Train Loss: 1.610342 | Val Loss: 2.613528 | LR: 2.50e-03\n{'mrr': 0.9337026438760349, 'ndcg': 0.9499266877208143, 'recall_at_1': 0.89156, 'recall_at_3': 0.97272, 'recall_at_5': 0.98528, 'recall_at_10': 0.99372, 'recall_at_50': 0.99924, 'l2_dist': 25.79306411743164}\nSaved new best model (mrr=0.933703)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 041 | Train Loss: 1.584051 | Val Loss: 2.608236 | LR: 2.50e-03\n{'mrr': 0.9340878944514277, 'ndcg': 0.9502237904025213, 'recall_at_1': 0.89232, 'recall_at_3': 0.97292, 'recall_at_5': 0.98508, 'recall_at_10': 0.99416, 'recall_at_50': 0.99924, 'l2_dist': 25.792781829833984}\nSaved new best model (mrr=0.934088)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 042 | Train Loss: 1.554579 | Val Loss: 2.608389 | LR: 2.50e-03\n{'mrr': 0.9338970597341559, 'ndcg': 0.9500709501897607, 'recall_at_1': 0.89212, 'recall_at_3': 0.97256, 'recall_at_5': 0.985, 'recall_at_10': 0.99376, 'recall_at_50': 0.99924, 'l2_dist': 25.79255485534668}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 043 | Train Loss: 1.542355 | Val Loss: 2.600090 | LR: 2.50e-03\n{'mrr': 0.9336916145116207, 'ndcg': 0.9499340549498992, 'recall_at_1': 0.89152, 'recall_at_3': 0.97308, 'recall_at_5': 0.98512, 'recall_at_10': 0.99404, 'recall_at_50': 0.99932, 'l2_dist': 25.7926082611084}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 044 | Train Loss: 1.530987 | Val Loss: 2.601380 | LR: 2.50e-03\n{'mrr': 0.933487868403298, 'ndcg': 0.9497687165715075, 'recall_at_1': 0.8914, 'recall_at_3': 0.97284, 'recall_at_5': 0.98476, 'recall_at_10': 0.99388, 'recall_at_50': 0.99932, 'l2_dist': 25.792112350463867}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 045 | Train Loss: 1.496105 | Val Loss: 2.599255 | LR: 1.25e-03\n{'mrr': 0.9332075602392791, 'ndcg': 0.9495637211543897, 'recall_at_1': 0.89072, 'recall_at_3': 0.9726, 'recall_at_5': 0.985, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.79210090637207}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 046 | Train Loss: 1.482428 | Val Loss: 2.594173 | LR: 1.25e-03\n{'mrr': 0.9339258830502695, 'ndcg': 0.9501031700342107, 'recall_at_1': 0.89196, 'recall_at_3': 0.97268, 'recall_at_5': 0.98544, 'recall_at_10': 0.9938, 'recall_at_50': 0.9992, 'l2_dist': 25.79204559326172}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 047 | Train Loss: 1.473732 | Val Loss: 2.594404 | LR: 1.25e-03\n{'mrr': 0.9344063211820803, 'ndcg': 0.950462712048782, 'recall_at_1': 0.89272, 'recall_at_3': 0.973, 'recall_at_5': 0.98528, 'recall_at_10': 0.99376, 'recall_at_50': 0.99928, 'l2_dist': 25.79180908203125}\nSaved new best model (mrr=0.934406)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 048 | Train Loss: 1.458167 | Val Loss: 2.596526 | LR: 1.25e-03\n{'mrr': 0.9343364166791048, 'ndcg': 0.9504002462953953, 'recall_at_1': 0.89276, 'recall_at_3': 0.97288, 'recall_at_5': 0.98524, 'recall_at_10': 0.99396, 'recall_at_50': 0.9992, 'l2_dist': 25.791662216186523}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 049 | Train Loss: 1.449331 | Val Loss: 2.594361 | LR: 6.25e-04\n{'mrr': 0.9342122152278471, 'ndcg': 0.9503100994620056, 'recall_at_1': 0.89252, 'recall_at_3': 0.97244, 'recall_at_5': 0.98524, 'recall_at_10': 0.99392, 'recall_at_50': 0.99924, 'l2_dist': 25.79153823852539}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 050 | Train Loss: 1.445197 | Val Loss: 2.593144 | LR: 6.25e-04\n{'mrr': 0.9343138969267267, 'ndcg': 0.9503861738864958, 'recall_at_1': 0.89272, 'recall_at_3': 0.9726, 'recall_at_5': 0.98516, 'recall_at_10': 0.99392, 'recall_at_50': 0.9992, 'l2_dist': 25.79148292541504}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 051 | Train Loss: 1.440666 | Val Loss: 2.591266 | LR: 6.25e-04\n{'mrr': 0.9347131891900016, 'ndcg': 0.9506799312825273, 'recall_at_1': 0.8936, 'recall_at_3': 0.97268, 'recall_at_5': 0.98544, 'recall_at_10': 0.99404, 'recall_at_50': 0.99924, 'l2_dist': 25.791385650634766}\nSaved new best model (mrr=0.934713)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 052 | Train Loss: 1.432362 | Val Loss: 2.590660 | LR: 6.25e-04\n{'mrr': 0.9346984444919628, 'ndcg': 0.9506746384038447, 'recall_at_1': 0.8934, 'recall_at_3': 0.97316, 'recall_at_5': 0.9852, 'recall_at_10': 0.994, 'recall_at_50': 0.99924, 'l2_dist': 25.7912654876709}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 053 | Train Loss: 1.430760 | Val Loss: 2.589856 | LR: 6.25e-04\n{'mrr': 0.9347953094810894, 'ndcg': 0.9507543758988949, 'recall_at_1': 0.89336, 'recall_at_3': 0.97308, 'recall_at_5': 0.98556, 'recall_at_10': 0.99396, 'recall_at_50': 0.99924, 'l2_dist': 25.791284561157227}\nSaved new best model (mrr=0.934795)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 054 | Train Loss: 1.424198 | Val Loss: 2.588831 | LR: 6.25e-04\n{'mrr': 0.9346199669911136, 'ndcg': 0.9506177373231574, 'recall_at_1': 0.89324, 'recall_at_3': 0.97304, 'recall_at_5': 0.9854, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.79121208190918}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 055 | Train Loss: 1.423280 | Val Loss: 2.587293 | LR: 6.25e-04\n{'mrr': 0.9343762840633089, 'ndcg': 0.9504412761956023, 'recall_at_1': 0.89268, 'recall_at_3': 0.97308, 'recall_at_5': 0.9852, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.791088104248047}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 056 | Train Loss: 1.412294 | Val Loss: 2.587547 | LR: 3.13e-04\n{'mrr': 0.9343690761909182, 'ndcg': 0.9504355636762454, 'recall_at_1': 0.89264, 'recall_at_3': 0.97296, 'recall_at_5': 0.98508, 'recall_at_10': 0.99384, 'recall_at_50': 0.99932, 'l2_dist': 25.791000366210938}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 057 | Train Loss: 1.410353 | Val Loss: 2.586589 | LR: 3.13e-04\n{'mrr': 0.9345267091185141, 'ndcg': 0.9505551556495302, 'recall_at_1': 0.89292, 'recall_at_3': 0.97296, 'recall_at_5': 0.98508, 'recall_at_10': 0.994, 'recall_at_50': 0.99928, 'l2_dist': 25.790964126586914}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 058 | Train Loss: 1.408190 | Val Loss: 2.586844 | LR: 3.13e-04\n{'mrr': 0.934510752606386, 'ndcg': 0.950544772791889, 'recall_at_1': 0.89288, 'recall_at_3': 0.97336, 'recall_at_5': 0.9852, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.79092025756836}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 059 | Train Loss: 1.401390 | Val Loss: 2.586945 | LR: 3.13e-04\n{'mrr': 0.9345887974873985, 'ndcg': 0.9505959970593953, 'recall_at_1': 0.8932, 'recall_at_3': 0.97296, 'recall_at_5': 0.98508, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.79092025756836}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 060 | Train Loss: 1.398437 | Val Loss: 2.586512 | LR: 1.56e-04\n{'mrr': 0.9346348233335966, 'ndcg': 0.950627299291487, 'recall_at_1': 0.89332, 'recall_at_3': 0.97284, 'recall_at_5': 0.98516, 'recall_at_10': 0.99392, 'recall_at_50': 0.99928, 'l2_dist': 25.79091453552246}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 061 | Train Loss: 1.398751 | Val Loss: 2.586562 | LR: 1.56e-04\n{'mrr': 0.9346321484254573, 'ndcg': 0.9506236985005841, 'recall_at_1': 0.8934, 'recall_at_3': 0.97268, 'recall_at_5': 0.98528, 'recall_at_10': 0.99392, 'recall_at_50': 0.99924, 'l2_dist': 25.79088592529297}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 062 | Train Loss: 1.398613 | Val Loss: 2.586545 | LR: 1.56e-04\n{'mrr': 0.9347112034793268, 'ndcg': 0.9506829478831946, 'recall_at_1': 0.89352, 'recall_at_3': 0.9726, 'recall_at_5': 0.98516, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790864944458008}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 063 | Train Loss: 1.400603 | Val Loss: 2.586180 | LR: 1.56e-04\n{'mrr': 0.9348013972912316, 'ndcg': 0.950747921056819, 'recall_at_1': 0.89372, 'recall_at_3': 0.9726, 'recall_at_5': 0.98508, 'recall_at_10': 0.99384, 'recall_at_50': 0.99924, 'l2_dist': 25.79084014892578}\nSaved new best model (mrr=0.934801)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 064 | Train Loss: 1.397282 | Val Loss: 2.586412 | LR: 7.81e-05\n{'mrr': 0.9348855541679426, 'ndcg': 0.9508114014478065, 'recall_at_1': 0.89384, 'recall_at_3': 0.97272, 'recall_at_5': 0.98504, 'recall_at_10': 0.99384, 'recall_at_50': 0.99924, 'l2_dist': 25.79082489013672}\nSaved new best model (mrr=0.934886)\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 065 | Train Loss: 1.401224 | Val Loss: 2.586147 | LR: 7.81e-05\n{'mrr': 0.9348400612951752, 'ndcg': 0.9507770592371252, 'recall_at_1': 0.89376, 'recall_at_3': 0.97272, 'recall_at_5': 0.98508, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790813446044922}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 066 | Train Loss: 1.392488 | Val Loss: 2.586265 | LR: 7.81e-05\n{'mrr': 0.9348046586330465, 'ndcg': 0.950748652325498, 'recall_at_1': 0.89376, 'recall_at_3': 0.97276, 'recall_at_5': 0.985, 'recall_at_10': 0.99384, 'recall_at_50': 0.99924, 'l2_dist': 25.79079818725586}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 067 | Train Loss: 1.391595 | Val Loss: 2.586392 | LR: 7.81e-05\n{'mrr': 0.934781808090187, 'ndcg': 0.9507341701978439, 'recall_at_1': 0.89364, 'recall_at_3': 0.97288, 'recall_at_5': 0.98504, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.79077911376953}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 068 | Train Loss: 1.398097 | Val Loss: 2.586240 | LR: 3.91e-05\n{'mrr': 0.9348148499636745, 'ndcg': 0.9507587198158036, 'recall_at_1': 0.89372, 'recall_at_3': 0.97292, 'recall_at_5': 0.98504, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790769577026367}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 069 | Train Loss: 1.395823 | Val Loss: 2.586165 | LR: 3.91e-05\n{'mrr': 0.9347534383783844, 'ndcg': 0.9507133765447778, 'recall_at_1': 0.8936, 'recall_at_3': 0.97288, 'recall_at_5': 0.98504, 'recall_at_10': 0.99384, 'recall_at_50': 0.99924, 'l2_dist': 25.790767669677734}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 070 | Train Loss: 1.394935 | Val Loss: 2.586106 | LR: 3.91e-05\n{'mrr': 0.9347316142814743, 'ndcg': 0.9506960763373038, 'recall_at_1': 0.8936, 'recall_at_3': 0.97292, 'recall_at_5': 0.98504, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.7907657623291}\n","output_type":"stream"},{"name":"stderr","text":"                                                                        \r","output_type":"stream"},{"name":"stdout","text":"Epoch 071 | Train Loss: 1.392418 | Val Loss: 2.586043 | LR: 3.91e-05\n{'mrr': 0.9346701840416668, 'ndcg': 0.9506513588162451, 'recall_at_1': 0.89344, 'recall_at_3': 0.97304, 'recall_at_5': 0.985, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790761947631836}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 072 | Train Loss: 1.397355 | Val Loss: 2.585938 | LR: 1.95e-05\n{'mrr': 0.9346766073732048, 'ndcg': 0.9506566267715785, 'recall_at_1': 0.89344, 'recall_at_3': 0.973, 'recall_at_5': 0.985, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790761947631836}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 073 | Train Loss: 1.394305 | Val Loss: 2.585942 | LR: 1.95e-05\n{'mrr': 0.9347187862117953, 'ndcg': 0.9506882097997466, 'recall_at_1': 0.89352, 'recall_at_3': 0.973, 'recall_at_5': 0.98504, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790761947631836}\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 074 | Train Loss: 1.390131 | Val Loss: 2.585994 | LR: 1.95e-05\n{'mrr': 0.9347284655248365, 'ndcg': 0.9506938806527518, 'recall_at_1': 0.89356, 'recall_at_3': 0.97292, 'recall_at_5': 0.98504, 'recall_at_10': 0.99388, 'recall_at_50': 0.99924, 'l2_dist': 25.790760040283203}\nEarly stopping triggered.\nTraining complete. Best mrr: 0.934728\nFinished training. Now testing using best model...\nTest Results: {'mrr': 0.9354795373180256, 'ndcg': 0.9513033506208108, 'recall_at_1': 0.89412, 'recall_at_3': 0.97404, 'recall_at_5': 0.9862, 'recall_at_10': 0.9944, 'recall_at_50': 0.99932, 'l2_dist': 25.790616989135742}\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"generate_submission(models, Path(test_path), output_file=\"sub.csv\", device=device, ensemble=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-07T23:04:26.095372Z","iopub.execute_input":"2025-11-07T23:04:26.095942Z","iopub.status.idle":"2025-11-07T23:04:29.220408Z","shell.execute_reply.started":"2025-11-07T23:04:26.095919Z","shell.execute_reply":"2025-11-07T23:04:29.219637Z"}},"outputs":[{"name":"stdout","text":"Generating submission file...\n✓ Saved submission to sub.csv\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"        id                                          embedding\n0        1  [0.011552590876817703, 0.00943775288760662, -0...\n1        2  [-0.017103342339396477, -0.0030133062973618507...\n2        3  [0.0025980216450989246, -0.008530800230801105,...\n3        4  [0.03638746961951256, -0.0017537014791741967, ...\n4        5  [0.054213255643844604, 0.03685036301612854, 0....\n...    ...                                                ...\n1495  1496  [-0.008818816393613815, -0.008429779671132565,...\n1496  1497  [0.017080791294574738, 0.03277495130896568, 0....\n1497  1498  [0.051879268139600754, -0.026452703401446342, ...\n1498  1499  [-0.01728557050228119, -0.0016986479749903083,...\n1499  1500  [0.01597260870039463, -0.06013273075222969, -0...\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.011552590876817703, 0.00943775288760662, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[-0.017103342339396477, -0.0030133062973618507...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[0.0025980216450989246, -0.008530800230801105,...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.03638746961951256, -0.0017537014791741967, ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[0.054213255643844604, 0.03685036301612854, 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1496</td>\n      <td>[-0.008818816393613815, -0.008429779671132565,...</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1497</td>\n      <td>[0.017080791294574738, 0.03277495130896568, 0....</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1498</td>\n      <td>[0.051879268139600754, -0.026452703401446342, ...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1499</td>\n      <td>[-0.01728557050228119, -0.0016986479749903083,...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1500</td>\n      <td>[0.01597260870039463, -0.06013273075222969, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows × 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\n\n\ndef objective(trial, train_dataset, val_dataset, epochs: int = 10, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n    layer_choices = [1256, 1565, 2048]\n    hidden_layers = [trial.suggest_categorical(f\"n_units_l{i}\", layer_choices) for i in range(n_layers)]\n\n    activation_fn = nn.ReLU\n    \n    batch_size = trial.suggest_categorical(\"batch_size\", [2048, 4096])\n    lr = trial.suggest_categorical(\"lr\", [1e-4, 1e-3, 1e-2])\n    dropout_rate = trial.suggest_categorical('dropout_rate', [0.3, 0.4, 0.5])\n\n    weight_decay = trial.suggest_categorical(\"weight_decay\", [1e-4, 1e-3, 1e-2])\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    model = SpaceTranslator(\n        input_dim=1024, \n        output_dim=1536,\n        hidden_layers=hidden_layers,\n        activation=activation_fn,\n        dropout_rate=dropout_rate\n    )\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode=\"min\",\n        factor=0.5,           # reduce LR by half when plateau\n        patience=2,           # wait 2 epochs before reducing LR\n        min_lr=1e-6,          # don't go below this LR\n    )\n\n    best_val_loss = float('inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_train_loss = 0.0\n\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n\n        train_loss = running_train_loss / len(train_loader)\n\n        # Validation phase\n        model.eval()\n        running_val_loss = 0.0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                outputs = model(X_batch)\n                loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n                running_val_loss += loss.item()\n\n        val_loss = running_val_loss / len(val_loader)\n\n        scheduler.step(val_loss)\n\n        results = test(val_dataset, model, device)\n\n        trial.report(results['mrr'], epoch)\n\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n\n    return results['mrr']\n\n\ndef run_optuna_search(data_path: Path, n_trials: int = 30, epochs: int = 10, n_jobs: int = 1, pruner=None):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n\n    X, Y = get_data(data_path)    \n    train_dataset, val_dataset = get_datasets(X, Y)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset,\n                                   epochs=epochs)\n    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n\n    print(\"Study statistics:\")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Best trial:\")\n    trial = study.best_trial\n    print(\"    Value: \", trial.value)\n    print(\"    Params: \")\n    for k, v in trial.params.items():\n        print(f\"      {k}: {v}\")\n\n    return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-06T15:59:28.836345Z","iopub.execute_input":"2025-11-06T15:59:28.836570Z","iopub.status.idle":"2025-11-06T15:59:29.189783Z","shell.execute_reply.started":"2025-11-06T15:59:28.836549Z","shell.execute_reply":"2025-11-06T15:59:29.189174Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"study = run_optuna_search(data_path=data_path, n_trials=200, epochs=30, n_jobs=1)\nstudy.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n\nbest_trial_number = study.best_trial.number\nprint(\"Best params:\", study.best_params)\nprint(\"Best trial number:\", study.best_trial.number)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}