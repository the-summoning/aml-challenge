{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-27T19:13:20.177553Z",
     "iopub.status.busy": "2025-10-27T19:13:20.176859Z",
     "iopub.status.idle": "2025-10-27T19:13:20.183299Z",
     "shell.execute_reply": "2025-10-27T19:13:20.182728Z",
     "shell.execute_reply.started": "2025-10-27T19:13:20.177526Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "### OLD MODEL ###\n",
    "\n",
    "# class Translator(nn.Module):\n",
    "#     def __init__(self, pad: bool, dim_imgs: int = 1536, dim_text: int = 1024,  mode: str ='linear'):\n",
    "#         super().__init__()\n",
    "#         assert mode in ['linear', 'affine', 'isometry'], f'Mode \"{mode}\" not supported'\n",
    "\n",
    "#         self.mode = mode\n",
    "#         use_bias = mode == 'affine'\n",
    "#         if pad:\n",
    "#             dim = max(dim_imgs, dim_text)\n",
    "#             self.linear = nn.Linear(dim, dim, bias=use_bias)\n",
    "\n",
    "#         else:\n",
    "#             self.linear = nn.Linear(dim_text, dim_imgs, bias=use_bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.linear(x)\n",
    "\n",
    "#     @torch.no_grad()\n",
    "#     def orthogonalize(self):\n",
    "#         assert self.mode == 'isometry', 'Cannot be called for modes != isometry'\n",
    "\n",
    "#         W = self.linear.weight.data\n",
    "#         U, _, Vh = torch.linalg.svd(W, full_matrices=False)\n",
    "#         self.linear.weight.data.copy_(U @ Vh)\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, input_dim=1024, output_dim=1536, mode='affine', use_relative=False, anchors: Optional[torch.Tensor] = None):\n",
    "        super().__init__()\n",
    "        assert mode in ['linear', 'affine', 'isometry'], f'Mode \"{mode}\" not supported'\n",
    "        assert input_dim > 0 and output_dim > 0, \"Expecting positive dimensions\"\n",
    "        assert not use_relative or isinstance(anchors, torch.Tensor) , 'Anchors must be set if using relative representations'\n",
    "        assert anchors is None or (anchors.ndim == 2 and anchors.shape[0] > 0), '2D Anchors must be provided if using relative representations'\n",
    "        \n",
    "        self.mode = mode\n",
    "        self.use_relative = use_relative\n",
    "        self.anchors = anchors\n",
    "        \n",
    "        self.linear = nn.Linear(\n",
    "            anchors.shape[0] if self.use_relative else input_dim,\n",
    "            output_dim,\n",
    "            bias=self.mode == 'affine'\n",
    "        )\n",
    "    \n",
    "    def compute_relative(self, x):\n",
    "        assert self.anchors is not None, 'Anchors must be set by calling \"set_anchors\"'\n",
    "        \n",
    "        return F.normalize(x, p=2, dim=1) @ F.normalize(self.anchors.T)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.use_relative:\n",
    "            x = self.compute_relative(x)\n",
    "        \n",
    "        return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:13:23.773277Z",
     "iopub.status.busy": "2025-10-27T19:13:23.772997Z",
     "iopub.status.idle": "2025-10-27T19:13:23.787357Z",
     "shell.execute_reply": "2025-10-27T19:13:23.786541Z",
     "shell.execute_reply.started": "2025-10-27T19:13:23.773253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from model import Translator\n",
    "'''Code from https://github.com/Mamiglia/challenge'''\n",
    "\n",
    "def mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Reciprocal Rank (MRR)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "    Returns:\n",
    "        mrr: Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n",
    "    \"\"\"Compute Recall@k\n",
    "    Args:\n",
    "        pred_indices: (N, N) array of top indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        recall: Recall@k\n",
    "    \"\"\"\n",
    "    recall = 0\n",
    "    for i in range(len(gt_indices)):\n",
    "        if gt_indices[i] in pred_indices[i, :k]:\n",
    "            recall += 1\n",
    "    recall /= len(gt_indices)\n",
    "    return recall\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        ndcg: NDCG@k\n",
    "    \"\"\"\n",
    "    ndcg_total = 0.0\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            rank = matches[0] + 1\n",
    "            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n",
    "    return ndcg_total / len(gt_indices)\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n",
    "    \"\"\"Evaluate retrieval performance using cosine similarity\n",
    "    Args:\n",
    "        translated_embd: (N_captions, D) translated caption embeddings\n",
    "        image_embd: (N_images, D) image embeddings\n",
    "        gt_indices: (N_captions,) ground truth image indices for each caption\n",
    "        max_indices: number of top predictions to consider\n",
    "    Returns:\n",
    "        results: dict of evaluation metrics\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute similarity matrix\n",
    "    if isinstance(translated_embd, np.ndarray):\n",
    "        translated_embd = torch.from_numpy(translated_embd).float()\n",
    "    if isinstance(image_embd, np.ndarray):\n",
    "        image_embd = torch.from_numpy(image_embd).float()\n",
    "    \n",
    "    n_queries = translated_embd.shape[0]\n",
    "    device = translated_embd.device\n",
    "    \n",
    "    # Prepare containers for the fragments to be reassembled\n",
    "    all_sorted_indices = []\n",
    "    l2_distances = []\n",
    "    \n",
    "    # Process in batches - the narrow gate approach\n",
    "    for start_idx in range(0, n_queries, batch_size):\n",
    "        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n",
    "        batch_translated = translated_embd[batch_slice]\n",
    "        batch_img_embd = image_embd[batch_slice]\n",
    "        \n",
    "        # Compute similarity only for this batch\n",
    "        batch_similarity = batch_translated @ batch_img_embd.T\n",
    "\n",
    "        # Get top-k predictions for this batch\n",
    "        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n",
    "        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n",
    "\n",
    "        # Compute L2 distance for this batch\n",
    "        batch_gt = gt_indices[batch_slice]\n",
    "        batch_gt_embeddings = image_embd[batch_gt]\n",
    "        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n",
    "        l2_distances.append(batch_l2)\n",
    "    \n",
    "    # Reassemble the fragments\n",
    "    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n",
    "    \n",
    "    # Apply the sacred metrics to the whole\n",
    "    metrics = {\n",
    "        'mrr': mrr,\n",
    "        'ndcg': ndcg,\n",
    "        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n",
    "        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n",
    "        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n",
    "        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n",
    "        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        name: func(sorted_indices, gt_indices)\n",
    "        for name, func in metrics.items()\n",
    "    }\n",
    "    \n",
    "    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n",
    "    results['l2_dist'] = l2_dist\n",
    "    \n",
    "    return results\n",
    "\n",
    "def eval_on_val(X_val: np.ndarray, y_val: np.ndarray, model: Translator, device) -> dict:\n",
    "    gt_indices = torch.arange(len(y_val))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        translated = model(X_val.to(device)).to('cpu')\n",
    "\n",
    "    results = evaluate_retrieval(translated, y_val, gt_indices)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_submission(model: Translator, test_path: Path, output_file=\"submission.csv\", device=None):\n",
    "    test_data = np.load(test_path)\n",
    "    sample_ids = test_data['captions/ids']\n",
    "    test_embds = test_data['captions/embeddings']\n",
    "    test_embds = torch.from_numpy(test_embds).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_embds = model(test_embds.to(device)).cpu()\n",
    "\n",
    "    print(\"Generating submission file...\")\n",
    "\n",
    "    if isinstance(pred_embds, torch.Tensor):\n",
    "        pred_embds = pred_embds.cpu().numpy()\n",
    "\n",
    "    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n",
    "\n",
    "    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n",
    "    print(f\"✓ Saved submission to {output_file}\")\n",
    "\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 1024\n",
    "output_dim = 1536\n",
    "\n",
    "batch_size= 64\n",
    "lr= 0.001\n",
    "epochs= 50\n",
    "anchors_number= 350\n",
    "data_path= '/kaggle/input/aml-competition/train/train/train.npz'\n",
    "test_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n",
    "\n",
    "\n",
    "use_pad= False\n",
    "use_standardize= False\n",
    "use_normalize= False\n",
    "\n",
    "anchors_method='pca'\n",
    "use_relative= True\n",
    "mode= 'affine'\n",
    "model_save_path= './models/exp1.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:16:16.139577Z",
     "iopub.status.busy": "2025-10-27T19:16:16.139068Z",
     "iopub.status.idle": "2025-10-27T19:16:16.155882Z",
     "shell.execute_reply": "2025-10-27T19:16:16.155067Z",
     "shell.execute_reply.started": "2025-10-27T19:16:16.139552Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from typing import Literal\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def pad(data: torch.Tensor, pad_val: int) -> torch.Tensor:\n",
    "    return F.pad(data, (0, pad_val), mode=\"constant\", value=0)\n",
    "\n",
    "def standardize(data: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "    mean = data.mean(dim=0, keepdim=True)\n",
    "    std = data.std(dim=0, keepdim=True) + 1e-8\n",
    "    data_standardized = (data - mean) / std\n",
    "\n",
    "    return data_standardized\n",
    "\n",
    "def preprocess(X_abs: np.array, Y_abs: np.array, pad: bool, standardize: bool, normalize: bool) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    assert X_abs.ndim == 2 and Y_abs.ndim == 2, \"Both data must be 2D\"\n",
    "    X_abs, Y_abs = torch.from_numpy(X_abs).float(), torch.from_numpy(Y_abs).float()\n",
    "\n",
    "    # if pad:\n",
    "    #     x_pad = max(Y_abs.shape[1] - X_abs.shape[1], 0)\n",
    "    #     y_pad = max(X_abs.shape[1] - Y_abs.shape[1], 0)\n",
    "\n",
    "    #     X_abs = pad(X_abs, x_pad)\n",
    "    #     Y_abs = pad(Y_abs, y_pad)\n",
    "\n",
    "    if standardize:\n",
    "        X_abs = standardize(X_abs)\n",
    "        Y_abs = standardize(Y_abs)\n",
    "\n",
    "    if normalize:\n",
    "        X_abs = F.normalize(X_abs, dim=1)\n",
    "        Y_abs = F.normalize(Y_abs, dim=1)\n",
    "\n",
    "    return X_abs, Y_abs\n",
    "\n",
    "\n",
    "def train_model(model: Translator, model_path: Path, mode: str, \n",
    "                train_loader: DataLoader, val_loader: DataLoader,\n",
    "                epochs: int, lr: float) -> Translator:\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            #loss = 1 - F.cosine_similarity(outputs, y_batch, dim=1).mean()\n",
    "            loss = F.mse_loss(outputs, y_batch)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            if mode == 'isometry':\n",
    "                model.orthogonalize()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                #loss = 1 - F.cosine_similarity(outputs, y_batch, dim=1).mean()\n",
    "                loss = F.mse_loss(outputs, y_batch)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "\n",
    "            Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            print(f\"✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "\n",
    "    return model\n",
    "\n",
    "def extract_anchors(data: torch.Tensor, method: Literal['pca', 'k-means', 'random'], anchors_number: int):\n",
    "    assert isinstance(data, torch.Tensor) and data.ndim == 2 and data.shape[0] > 0, \"Expected a valid tensor\"\n",
    "    assert method in ['pca', 'k-means', 'random'], f'Method {method} not supported'\n",
    "    assert isinstance(anchors_number, int) and anchors_number > 0, \"Expected a natural positive number\"\n",
    "\n",
    "    data_np = data.cpu().numpy()\n",
    "\n",
    "    if method == 'pca':\n",
    "        # PCA already returns normalized anchors\n",
    "        pca = PCA(n_components=anchors_number)\n",
    "        pca.fit(data_np)\n",
    "        \n",
    "        anchors = torch.from_numpy(pca.components_).float()\n",
    "    elif method == 'k-means':\n",
    "        kmeans = KMeans(n_clusters=anchors_number, init='k-means++', n_init=10, random_state=42)\n",
    "        kmeans.fit(data_np)\n",
    "        \n",
    "        anchors = torch.from_numpy(kmeans.cluster_centers_).float()\n",
    "    else:\n",
    "        anchors = data[torch.randperm(data.size(0))[:anchors_number]]\n",
    "\n",
    "    return anchors\n",
    "\n",
    "def load_data(data_path: Path, config: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    caption_embeddings = data['captions/embeddings']\n",
    "    image_embeddings = data['images/embeddings']\n",
    "    caption_labels = data['captions/label']\n",
    "\n",
    "    X_abs, y_abs = preprocess(caption_embeddings, image_embeddings[np.argmax(caption_labels, axis=1)], \n",
    "                              pad=use_pad, standardize=use_standardize, normalize=use_normalize)\n",
    "    \n",
    "    print('Texts shape', X_abs.shape)\n",
    "    print('Images shape', X_abs.shape)\n",
    "\n",
    "    n_train = int(0.9 * X_abs.shape[0])\n",
    "    train_split = torch.zeros(X_abs.shape[0], dtype=torch.bool)\n",
    "    train_split[:n_train] = 1\n",
    "    \n",
    "    X_train, X_val = X_abs[train_split], X_abs[~train_split]\n",
    "    y_train, y_val = y_abs[train_split], y_abs[~train_split]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val\n",
    "\n",
    "    \n",
    "def test(model: Translator, X_val: torch.Tensor, y_val: torch.tensor, device):\n",
    "    results = eval_on_val(X_val, y_val, model=model, device=device)\n",
    "    print(\"Test Results:\", results)\n",
    "\n",
    "\n",
    "def train(config: dict, model: Translator, X_train: torch.Tensor, y_train: torch.Tensor, X_val: torch.Tensor, y_val: torch.Tensor):\n",
    "    \n",
    "    model_save_path = config['model_save_path']\n",
    "    \n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    train_model(model, model_save_path, 'affine', train_loader, val_loader, epochs, lr)\n",
    "\n",
    "    print('Finished training. Now testing using best model...')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-27T19:13:33.071711Z",
     "iopub.status.busy": "2025-10-27T19:13:33.071421Z",
     "iopub.status.idle": "2025-10-27T19:14:10.543772Z",
     "shell.execute_reply": "2025-10-27T19:14:10.543199Z",
     "shell.execute_reply.started": "2025-10-27T19:13:33.071691Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "X_train, Y_train, X_val, y_val = load_data(data_path, dict())\n",
    "\n",
    "X_anchors = extract_anchors(X_train, anchors_method, anchors_number).to(device) if use_relative else None\n",
    "model_args = {\n",
    "    'input_dim': input_dim,\n",
    "    'output_dim': output_dim,\n",
    "    'mode': mode,\n",
    "    'use_relative': use_relative,\n",
    "    'anchors': X_anchors\n",
    "}\n",
    "model = Translator(**model_args).to(device)\n",
    "\n",
    "train(config=dict(), model=model, X_train=X_train, y_train=Y_train, X_val=X_val, y_val=y_val)\n",
    "\n",
    "state = torch.load(model_save_path)\n",
    "model.load_state_dict(state)\n",
    "\n",
    "test(model, X_val, y_val, test_path, device)\n",
    "generate_submission(model, Path(test_path), device=device)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14220991,
     "sourceId": 117959,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
