{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"},{"sourceId":13718660,"sourceType":"datasetVersion","datasetId":8727971}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nimport torch\nimport numpy as np\n\n# class SpaceTranslator(nn.Module):\n#     def __init__(\n#         self,\n#         input_dim,\n#         output_dim,\n#         hidden_layers,\n#         activation,\n#         dropout_rate,\n#         init_method: str = 'xavier'\n#     ):\n#         super().__init__()\n\n#         self.init_method = init_method.lower()\n#         if self.init_method not in ['xavier', 'kaiming']:\n#             raise ValueError(\"Unsupported init_method\")\n\n#         layers = []\n#         last = input_dim\n\n#         for hidden in hidden_layers:\n#             layers += [\n#                 nn.Linear(last, hidden),\n#                 nn.LayerNorm(hidden),\n#                 activation(),\n#                 nn.Dropout(dropout_rate)\n#             ]\n#             last = hidden\n\n#         layers.append(nn.Linear(last, output_dim))\n#         self.net = nn.Sequential(*layers)\n\n#         self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n#         # Applica l'inizializzazione scelta\n#         self.apply(self.init_weights)\n\n#     def init_weights(self, module):\n#         if isinstance(module, nn.Linear):\n#             if self.init_method == 'kaiming':\n#                 nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n#             else:\n#                 nn.init.xavier_uniform_(module.weight)\n            \n#             if module.bias is not None:\n#                 nn.init.constant_(module.bias, 0.0)\n                \n#         elif isinstance(module, nn.LayerNorm):\n#             nn.init.ones_(module.weight)\n#             nn.init.zeros_(module.bias)\n\n\n#     def forward(self, x):\n#         return self.net(x)\n#         #return F.normalize(self.net(x), p=2, dim=1)\n\n\nimport torch\nfrom typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\n\nclass SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim: int,\n        output_dim: int,\n        dir_hidden_dims: list[int],\n        scale_hidden_dims: list[int],\n        activation=nn.ReLU,\n        dropout_rate: float=0.3,\n        init_method: str = 'xavier'\n    ):\n        super().__init__()\n\n        self.init_method = init_method.lower()\n        if self.init_method not in ['xavier', 'kaiming']:\n            raise ValueError(\"Unsupported init_method\")\n\n        def build_mlp(hidden_dims, out_dim, apply_softplus=False):\n            layers = []\n            last_dim = input_dim\n            for hidden in hidden_dims:\n                layers += [\n                    nn.Linear(last_dim, hidden),\n                    activation(),\n                    nn.LayerNorm(hidden),\n                    nn.Dropout(dropout_rate)\n                ]\n                last_dim = hidden\n            layers.append(nn.Linear(last_dim, out_dim))\n            \n            if apply_softplus:\n                layers.append(nn.Softplus())\n            \n            return nn.Sequential(*layers)\n\n        self.dir_head = build_mlp(dir_hidden_dims, output_dim, apply_softplus=False)\n        self.scale_head = build_mlp(scale_hidden_dims, 1, apply_softplus=True)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.init_method == 'kaiming':\n                nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n            else:\n                nn.init.xavier_uniform_(module.weight)\n            \n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n                \n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    def forward(self, x):\n        direction = self.dir_head(x)\n        scale = self.scale_head(x)\n        \n        return F.normalize(direction, p=2, dim=-1) * scale","metadata":{"execution":{"iopub.status.busy":"2025-11-15T13:51:34.831973Z","iopub.execute_input":"2025-11-15T13:51:34.832558Z","iopub.status.idle":"2025-11-15T13:51:34.842105Z","shell.execute_reply.started":"2025-11-15T13:51:34.832534Z","shell.execute_reply":"2025-11-15T13:51:34.841493Z"},"id":"pErT3mEBFSji","trusted":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n\n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n\n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n\n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n\n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n\n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n\n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n\n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n\n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n\n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n\n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: nn.Module, device) -> dict:\n    gt_indices = torch.arange(len(y_val))\n\n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n\n    return results\n\n\ndef generate_submission(model: nn.Module, test_path: Path, output_file=\"submission.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"‚úì Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"execution":{"iopub.status.busy":"2025-11-15T12:43:55.555567Z","iopub.execute_input":"2025-11-15T12:43:55.555863Z","iopub.status.idle":"2025-11-15T12:43:55.571102Z","shell.execute_reply.started":"2025-11-15T12:43:55.555841Z","shell.execute_reply":"2025-11-15T12:43:55.570558Z"},"id":"umdq3vIdKFfa","trusted":true},"outputs":[],"execution_count":32},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\ndef moco_info_nce_loss(q, k, queue, logit_scale):\n    # Positivo\n    l_pos = torch.sum(q * k, dim=1, keepdim=True)  # [batch,1]\n\n    # Negativi\n    l_neg = q @ queue.T  # [batch, queue_size]\n\n    logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n    logits = torch.cat([l_pos, l_neg], dim=1)\n    logits = logits * logit_scale.exp()\n\n    # Labels: positivo sempre in posizione 0\n    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n\n    return F.cross_entropy(logits, labels)\n\n\n@torch.no_grad()\ndef enqueue(queue, keys, queue_ptr):\n    batch_size = keys.shape[0]\n    queue_size = queue.shape[0]\n    ptr = int(queue_ptr[0])\n\n    if ptr + batch_size <= queue_size:\n        # Slice diretta\n        queue[ptr:ptr+batch_size, :] = keys\n    else:\n        #  (wrap-around)\n        first_part = queue_size - ptr\n        queue[ptr:, :] = keys[:first_part, :]\n        queue[:batch_size - first_part, :] = keys[first_part:, :]\n\n    # Aggiorna il puntatore\n    queue_ptr[0] = (ptr + batch_size) % queue_size\n\ndef train_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size, weight_decay):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode='max',\n        factor=0.7,\n        patience=3,\n        threshold=0.001,\n        min_lr=1e-6\n    )\n\n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    best_mrr = -float('inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n\n        # ---- Training ----\n        for text_batch, image_emb_batch in progress_bar:\n            text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n\n            optimizer.zero_grad()\n\n            q = model(text_batch)\n            k = image_emb_batch\n\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for text_batch, image_emb_batch in val_loader:\n                text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n\n                q = model(text_batch)\n                k = image_emb_batch\n\n                loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n                running_val_loss += loss.item()\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        results = test(val_dataset, model, device)\n        mrr = results[\"mrr\"]\n\n        scheduler.step(mrr)\n\n        print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | \"\n              f\"MRR: {mrr:.6f} | Recall-1: {results['recall_at_1']:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        # ---- EARLY STOPPING + MODEL SAVING basati su MRR ----\n        if mrr > best_mrr:\n            best_mrr = mrr\n            no_improvements = 0\n\n            # Save best model\n            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), save_path)\n            print(f\"üíæ Saved new best model (MRR={mrr:.6f})\")\n\n        else:\n            no_improvements += 1\n            if no_improvements >= patience:\n                print(\"‚èπ Early stopping triggered based on MRR.\")\n                break\n\n    print(\"‚úÖ Training complete\")\n    return model\n\n\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n\ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n\n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n\n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model: nn.Module, device):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device)\n    return results\n\ndef center(X: torch.Tensor):\n    mean = X.mean(dim=0, keepdim=True)\n    return X - mean, mean","metadata":{"execution":{"iopub.status.busy":"2025-11-15T14:02:06.926353Z","iopub.execute_input":"2025-11-15T14:02:06.926947Z","iopub.status.idle":"2025-11-15T14:02:06.942910Z","shell.execute_reply.started":"2025-11-15T14:02:06.926925Z","shell.execute_reply":"2025-11-15T14:02:06.942073Z"},"id":"_Ng-afH6FqPt","trusted":true},"outputs":[],"execution_count":55},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n\nsave_path = './models/dir-model.pth'","metadata":{"execution":{"iopub.status.busy":"2025-11-15T12:02:20.017079Z","iopub.execute_input":"2025-11-15T12:02:20.017811Z","iopub.status.idle":"2025-11-15T12:02:20.021705Z","shell.execute_reply.started":"2025-11-15T12:02:20.017783Z","shell.execute_reply":"2025-11-15T12:02:20.020952Z"},"id":"X_QhTeUoFrLm","trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"x, y = get_data(data_path)\n\n#x_centered, x_center = center(x)\n\ntrain_dataset, val_dataset = get_datasets(x, y)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-15T12:27:12.419046Z","iopub.execute_input":"2025-11-15T12:27:12.419250Z","iopub.status.idle":"2025-11-15T12:27:25.739354Z","shell.execute_reply.started":"2025-11-15T12:27:12.419234Z","shell.execute_reply":"2025-11-15T12:27:25.738584Z"},"id":"zI0blWNyFtDI","outputId":"e2e2f865-ee19-4a00-b275-f0678aa7f501","trusted":true},"outputs":[{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"input_dim = x.shape[1]\noutput_dim = y.shape[1]\n#hidden_layers = [1472, 2048]\ndropout_rate = 0.3881417338814056\nbatch_size = 256\nlr = 0.00017095147919728726\nepochs = 250\npatience = 5\n\nqueue_size = 32768\nweight_decay = 5.9917829998790125e-06\n\n# {'lr': 0.00017095147919728726, 'weight_decay': 5.9917829998790125e-06, 'batch_size': 256, 'queue_size': 32768, 'dropout_rate': 0.3881417338814056, 'activation': 'silu', 'init_method': 'xavier', 'n_layers_dir': 1, 'dir_dim_0': 2048, 'n_layers_scale': 1, 'scale_dim_0': 512}\nmodel_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    #'hidden_layers': hidden_layers,\n    'dir_hidden_dims': [2048],\n    'scale_hidden_dims': [512],\n    'dropout_rate': dropout_rate,\n    'activation': nn.SiLU,\n    'init_method': 'xavier'\n}\n\nmodel = SpaceTranslator(**model_args)\n\ntrain_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size, weight_decay)\n\nprint('Finished training. Now testing using best model...')\n\nstate = torch.load(save_path)\nmodel.load_state_dict(state)\nresults = test(val_dataset, model, device)\nprint(\"Test Results:\", results)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.status.busy":"2025-11-15T14:22:26.461923Z","iopub.execute_input":"2025-11-15T14:22:26.462197Z","iopub.status.idle":"2025-11-15T14:26:24.946899Z","shell.execute_reply.started":"2025-11-15T14:22:26.462178Z","shell.execute_reply":"2025-11-15T14:26:24.946179Z"},"id":"P5k77RkVFuwg","outputId":"c5d2268c-1c45-4399-e821-b1d5572f5f60","trusted":true},"outputs":[{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | Train Loss: 7.385201 | Val Loss: 5.580341 | MRR: 0.845623 | Recall-1: 0.758880 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.845623)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 002 | Train Loss: 5.526271 | Val Loss: 4.973348 | MRR: 0.886905 | Recall-1: 0.820360 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.886905)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 003 | Train Loss: 4.876020 | Val Loss: 4.694013 | MRR: 0.902863 | Recall-1: 0.844800 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.902863)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 004 | Train Loss: 4.456214 | Val Loss: 4.501168 | MRR: 0.911738 | Recall-1: 0.858000 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.911738)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 005 | Train Loss: 4.128471 | Val Loss: 4.389642 | MRR: 0.918572 | Recall-1: 0.868760 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.918572)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 006 | Train Loss: 3.849420 | Val Loss: 4.266974 | MRR: 0.923166 | Recall-1: 0.876680 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.923166)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 007 | Train Loss: 3.590361 | Val Loss: 4.244143 | MRR: 0.924325 | Recall-1: 0.878240 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.924325)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 008 | Train Loss: 3.355412 | Val Loss: 4.154324 | MRR: 0.926931 | Recall-1: 0.882320 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.926931)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 009 | Train Loss: 3.137401 | Val Loss: 4.190953 | MRR: 0.928013 | Recall-1: 0.884240 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.928013)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 010 | Train Loss: 2.932488 | Val Loss: 4.229918 | MRR: 0.928188 | Recall-1: 0.884360 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.928188)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 011 | Train Loss: 2.744548 | Val Loss: 4.222723 | MRR: 0.928733 | Recall-1: 0.885600 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.928733)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 012 | Train Loss: 2.583290 | Val Loss: 4.269889 | MRR: 0.929174 | Recall-1: 0.886400 | LR: 1.71e-04\nüíæ Saved new best model (MRR=0.929174)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 013 | Train Loss: 2.435150 | Val Loss: 4.294111 | MRR: 0.927875 | Recall-1: 0.884560 | LR: 1.71e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 014 | Train Loss: 2.302876 | Val Loss: 4.305423 | MRR: 0.928858 | Recall-1: 0.886760 | LR: 1.71e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 015 | Train Loss: 2.182079 | Val Loss: 4.440578 | MRR: 0.927141 | Recall-1: 0.884080 | LR: 1.71e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 016 | Train Loss: 2.070082 | Val Loss: 4.449347 | MRR: 0.928678 | Recall-1: 0.886240 | LR: 1.20e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 017 | Train Loss: 1.852690 | Val Loss: 4.631016 | MRR: 0.928252 | Recall-1: 0.886000 | LR: 1.20e-04\n‚èπ Early stopping triggered based on MRR.\n‚úÖ Training complete\nFinished training. Now testing using best model...\nTest Results: {'mrr': 0.9291739106035963, 'ndcg': 0.946409809518304, 'recall_at_1': 0.8864, 'recall_at_3': 0.96796, 'recall_at_5': 0.98244, 'recall_at_10': 0.99228, 'recall_at_50': 0.99928, 'l2_dist': 25.875186920166016}\n","output_type":"stream"}],"execution_count":64},{"cell_type":"code","source":"generate_submission(model, Path(test_path), output_file=\"2hmlp_memory-bank.csv\", device=device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"execution":{"iopub.status.busy":"2025-11-15T14:12:10.200148Z","iopub.execute_input":"2025-11-15T14:12:10.200822Z","iopub.status.idle":"2025-11-15T14:12:13.121953Z","shell.execute_reply.started":"2025-11-15T14:12:10.200798Z","shell.execute_reply":"2025-11-15T14:12:13.121115Z"},"id":"_Wk_esyYFw8X","outputId":"4c5c742d-e576-4c86-ecd7-48e853ef0ea9","trusted":true},"outputs":[{"name":"stdout","text":"Generating submission file...\n‚úì Saved submission to 2hmlp_memory-bank.csv\n","output_type":"stream"},{"execution_count":57,"output_type":"execute_result","data":{"text/plain":"        id                                          embedding\n0        1  [-0.009818177670240402, 0.011352929286658764, ...\n1        2  [0.002839854918420315, -0.007432268001139164, ...\n2        3  [0.003888091305270791, 0.01134349126368761, 0....\n3        4  [0.03038654662668705, -0.023458782583475113, -...\n4        5  [0.035906244069337845, 0.02111988514661789, 0....\n...    ...                                                ...\n1495  1496  [-0.006532762665301561, -0.02181270718574524, ...\n1496  1497  [0.011069669388234615, 0.022174226120114326, 0...\n1497  1498  [0.013020381331443787, -0.021422527730464935, ...\n1498  1499  [-0.04034360498189926, -0.006283098831772804, ...\n1499  1500  [0.02213854342699051, -0.03314811736345291, -0...\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[-0.009818177670240402, 0.011352929286658764, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[0.002839854918420315, -0.007432268001139164, ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[0.003888091305270791, 0.01134349126368761, 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.03038654662668705, -0.023458782583475113, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[0.035906244069337845, 0.02111988514661789, 0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1496</td>\n      <td>[-0.006532762665301561, -0.02181270718574524, ...</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1497</td>\n      <td>[0.011069669388234615, 0.022174226120114326, 0...</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1498</td>\n      <td>[0.013020381331443787, -0.021422527730464935, ...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1499</td>\n      <td>[-0.04034360498189926, -0.006283098831772804, ...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1500</td>\n      <td>[0.02213854342699051, -0.03314811736345291, -0...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows √ó 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":57},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_weights.pth\")","metadata":{"execution":{"iopub.status.busy":"2025-11-15T11:59:59.233673Z","iopub.status.idle":"2025-11-15T11:59:59.233940Z","shell.execute_reply.started":"2025-11-15T11:59:59.233770Z","shell.execute_reply":"2025-11-15T11:59:59.233792Z"},"id":"y46fQFKd3t4M","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\ndef objective(\n    trial,\n    train_dataset,\n    val_dataset,\n    epochs: int = 15,\n    device=None\n):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # ------------------------------\n    # HYPERPARAMETERS\n    # ------------------------------\n\n    # Optimizer params\n    lr = trial.suggest_float(\"lr\", 5e-5, 5e-3, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n\n    # Training params\n    batch_size = trial.suggest_categorical(\"batch_size\", [32, 256, 512])\n\n    # MoCo queue\n    queue_size = trial.suggest_categorical(\"queue_size\", [8192, 12288, 16384, 24576, 32768])\n\n    # Model params\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    activation = {\n        #\"relu\": nn.ReLU,\n        \"gelu\": nn.GELU,\n        \"silu\": nn.SiLU\n    }[trial.suggest_categorical(\"activation\", [\"relu\", \"gelu\", \"silu\"])]\n    init_method = trial.suggest_categorical(\"init_method\", [\"xavier\", \"kaiming\"])\n\n    # DIR head\n    n_layers_dir = trial.suggest_int(\"n_layers_dir\", 1, 3)\n    dir_hidden_dims = [\n        trial.suggest_categorical(f\"dir_dim_{i}\", [1024, 1472, 1856, 2048])\n        for i in range(n_layers_dir)\n    ]\n\n    # SCALE head\n    n_layers_scale = trial.suggest_int(\"n_layers_scale\", 1, 3)\n    scale_hidden_dims = [\n    trial.suggest_categorical(f\"scale_dim_{i}\", [256, 512, 768, 1024, 1280, 1472, 1856, 2048])\n        for i in range(n_layers_scale)\n    ]\n\n    # ------------------------------\n    # DATA LOADERS\n    # ------------------------------\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    # ------------------------------\n    # MODEL\n    # ------------------------------\n    model_args = {\n        \"input_dim\": 1024,\n        \"output_dim\": 1536,\n        \"dir_hidden_dims\": dir_hidden_dims,\n        \"scale_hidden_dims\": scale_hidden_dims,\n        \"dropout_rate\": dropout_rate,\n        \"activation\": nn.GELU,\n        \"init_method\": init_method,\n    }\n    model = SpaceTranslator(**model_args).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode=\"max\",\n        factor=0.75,\n        patience=4,\n        threshold=1e-4,\n        min_lr=1e-6,\n    )\n\n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    best_mrr = -float(\"inf\")\n\n    # ------------------------------\n    # TRAINING LOOP\n    # ------------------------------\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n\n        for text_batch, image_emb_batch in train_loader:\n            text_batch = text_batch.to(device)\n            image_emb_batch = image_emb_batch.to(device)\n\n            optimizer.zero_grad()\n\n            q = model(text_batch)\n            k = image_emb_batch\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n\n        # ------------------------------\n        # VALIDATION + MRR\n        # ------------------------------\n        model.eval()\n        with torch.no_grad():\n            results = test(val_dataset, model, device)\n        mrr = results[\"mrr\"]\n\n        # Step scheduler based on MRR\n        scheduler.step(mrr)\n\n        # Report to Optuna\n        trial.report(mrr, epoch)\n\n        if mrr > best_mrr:\n            best_mrr = mrr\n\n        # Pruning based on MRR\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n    print(f\"Trial {trial.number} finished ‚Äî Best MRR: {best_mrr:.6f}\")\n    return best_mrr   # <<<<<<< Maximize MRR\n\n\ndef run_optuna_search(\n    data_path: Path,\n    n_trials: int = 150,\n    epochs: int = 30,\n    n_jobs: int = 1,\n    sampler=None,\n    pruner=None\n):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=3)\n\n    X, y = get_data(data_path)\n    train_dataset, val_dataset = get_datasets(X, y)\n\n    # MAXIMIZE MRR\n    study = optuna.create_study(\n        direction=\"maximize\",\n        pruner=pruner,\n        sampler=sampler\n    )\n\n    func = lambda trial: objective(\n        trial,\n        train_dataset=train_dataset,\n        val_dataset=val_dataset,\n        epochs=epochs\n    )\n\n    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n\n    print(\"\\n==== OPTUNA FINISHED ====\")\n    print(\"Trials:\", len(study.trials))\n    print(\"Best trial:\")\n    print(f\"  Best MRR: {study.best_trial.value:.6f}\")\n    print(\"  Params:\")\n    for k, v in study.best_trial.params.items():\n        print(f\"    {k}: {v}\")\n\n    return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T14:19:47.478064Z","iopub.execute_input":"2025-11-15T14:19:47.478347Z","iopub.status.idle":"2025-11-15T14:19:47.492405Z","shell.execute_reply.started":"2025-11-15T14:19:47.478324Z","shell.execute_reply":"2025-11-15T14:19:47.491800Z"}},"outputs":[],"execution_count":62},{"cell_type":"code","source":"study = run_optuna_search(data_path=data_path, n_trials=100, epochs=35, n_jobs=1)\nstudy.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n\nprint(\"Best params:\", study.best_params)\nprint(\"Best trial number:\", study.best_trial.number)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-15T14:29:58.975457Z","iopub.execute_input":"2025-11-15T14:29:58.975947Z"}},"outputs":[{"name":"stderr","text":"[I 2025-11-15 14:30:12,232] A new study created in memory with name: no-name-48ff3ae4-c006-47ae-8cea-f27bf647f40c\n","output_type":"stream"},{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\n","output_type":"stream"}],"execution_count":null}]}