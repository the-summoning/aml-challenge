{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31154,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\n\n\nclass SpaceTranslator(nn.Module):\n    def __init__(self, input_dim, output_dim, hidden_layers, activation, dropout_rate):\n        super().__init__()\n        layers = []\n        last = input_dim\n        \n        for hidden in hidden_layers:\n            layers += [nn.Linear(last, hidden), activation(), nn.LayerNorm(hidden), nn.Dropout(dropout_rate)]\n            last = hidden\n            \n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n        \n        self.apply(self.init_weights)\n        \n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            nn.init.xavier_uniform_(module.weight)\n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n    def forward(self, x):\n        return self.net(x)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:11:11.122376Z","iopub.execute_input":"2025-11-04T12:11:11.122989Z","iopub.status.idle":"2025-11-04T12:11:11.129699Z","shell.execute_reply.started":"2025-11-04T12:11:11.122960Z","shell.execute_reply":"2025-11-04T12:11:11.128894Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport torch\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n    \n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n    \n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n    \n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n    \n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n        \n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n    \n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n    \n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n    \n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n    \n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n    \n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: nn.Module, device, scale=None) -> dict:\n    gt_indices = torch.arange(len(y_val))\n    \n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n        if scale is not None:\n            translated *= scale\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n    \n    return results\n\n\ndef generate_submission(model: nn.Module, test_path: Path, output_file=\"submission-dirmodel.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"âœ“ Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:12:27.070453Z","iopub.execute_input":"2025-11-04T12:12:27.070734Z","iopub.status.idle":"2025-11-04T12:12:27.347356Z","shell.execute_reply.started":"2025-11-04T12:12:27.070709Z","shell.execute_reply":"2025-11-04T12:12:27.346768Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom pathlib import Path\nfrom tqdm import tqdm\n\ndef info_nce_loss(dir_preds, img_targets, logit_scale):\n    # dir_preds = F.normalize(dir_preds, dim=-1)\n    # img_targets = F.normalize(img_targets, dim=-1)\n\n    # mult, not div\n    logits = (dir_preds @ img_targets.T) * logit_scale.exp()\n\n    labels = torch.arange(logits.size(0), device=logits.device)\n\n    loss_t2i = F.cross_entropy(logits, labels)\n    loss_i2t = F.cross_entropy(logits.T, labels)\n\n    return 0.5 * (loss_t2i + loss_i2t)\n\ndef train_model_direction(\n    model: SpaceTranslator,\n    model_path: Path,\n    train_dataset: TensorDataset,\n    val_dataset: TensorDataset,\n    batch_size: int,\n    epochs: int,\n    lr: float,\n    patience: int\n):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    print(f\"ğŸš€ Using device: {device}\")\n\n    # Dataloaders\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    # Optimizer and scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode=\"min\",\n        factor=0.5,           # reduce LR by half when plateau\n        patience=2,           # wait 2 epochs before reducing LR\n        min_lr=1e-6,          # don't go below this LR\n        verbose=True\n    )\n\n    best_val_loss = float('inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_train_loss = 0.0\n\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n        for X_batch, y_batch in progress_bar:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n            # X_batch = F.normalize(X_batch, dim=-1)\n            # y_batch = F.normalize(y_batch, dim=-1)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        train_loss = running_train_loss / len(train_loader)\n\n        # Validation phase\n        model.eval()\n        running_val_loss = 0.0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n                # X_batch = F.normalize(X_batch, dim=-1)\n                # y_batch = F.normalize(y_batch, dim=-1)\n\n                outputs = model(X_batch)\n                loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n                running_val_loss += loss.item()\n\n        val_loss = running_val_loss / len(val_loader)\n\n        # Step the scheduler\n        scheduler.step(val_loss)\n\n        print(f\"ğŸ“˜ Epoch {epoch:03d} | Train Loss: {train_loss:.6f} | Val Loss: {val_loss:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        # Optional: external validation/test\n        test(val_dataset, model, device)\n\n        # Early stopping\n        if val_loss < best_val_loss:\n            best_val_loss = val_loss\n            no_improvements = 0\n\n            Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), model_path)\n            print(f\"ğŸ’¾ Saved new best model (val_loss={val_loss:.6f})\")\n        else:\n            no_improvements += 1\n            if no_improvements >= patience:\n                print(\"â¹ Early stopping triggered.\")\n                break\n\n    print(f\"âœ… Training complete. Best Val Loss: {best_val_loss:.6f}\")\n\n    return model\n\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n    \ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:    \n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n    \n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n    \n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model: nn.Module, device, scale=None):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device, scale=scale)\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:12:31.319173Z","iopub.execute_input":"2025-11-04T12:12:31.319568Z","iopub.status.idle":"2025-11-04T12:12:31.336630Z","shell.execute_reply.started":"2025-11-04T12:12:31.319544Z","shell.execute_reply":"2025-11-04T12:12:31.335892Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ninput_dim = 1024\noutput_dim = 1536\nhidden_layers=[1256, 2048]\ndropout_rate = 0.3\n\nbatch_size= 512\nlr= 0.0005 # 0.0006\nepochs= 200\npatience = 5\n\ndata_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:13:17.957899Z","iopub.execute_input":"2025-11-04T12:13:17.958464Z","iopub.status.idle":"2025-11-04T12:13:18.045806Z","shell.execute_reply.started":"2025-11-04T12:13:17.958441Z","shell.execute_reply":"2025-11-04T12:13:18.045211Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dir_x, dir_y = get_data(data_path)\n\ntarget_norm_mean = dir_y.norm(dim=1).mean().item()\n\ndir_train_dataset, dir_val_dataset = get_datasets(dir_x, dir_y)\n\nprint('Target norm mean', target_norm_mean)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:45:06.018715Z","iopub.execute_input":"2025-11-04T11:45:06.019427Z","iopub.status.idle":"2025-11-04T11:45:23.508461Z","shell.execute_reply.started":"2025-11-04T11:45:06.019399Z","shell.execute_reply":"2025-11-04T11:45:23.507788Z"}},"outputs":[{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\nTarget norm mean 25.93919563293457\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"model_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    'hidden_layers': hidden_layers,\n    'dropout_rate': dropout_rate,\n    'activation': nn.GELU()\n}\n\ndir_model = SpaceTranslator(**model_args).to(device)\n\ntrain_model_direction(dir_model, './models/dir-model.pth', dir_train_dataset, dir_val_dataset, batch_size, epochs, lr, patience)\n\nprint('Finished training. Now testing using best model...')\n\nstate = torch.load('./models/dir-model.pth')\ndir_model.load_state_dict(state)\nresults = test(dir_val_dataset, dir_model, device)\nprint(\"Test Results:\", results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:48:54.462102Z","iopub.execute_input":"2025-11-04T11:48:54.462740Z","iopub.status.idle":"2025-11-04T11:54:27.608498Z","shell.execute_reply.started":"2025-11-04T11:48:54.462714Z","shell.execute_reply":"2025-11-04T11:54:27.607814Z"}},"outputs":[{"name":"stdout","text":"ğŸš€ Using device: cuda\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 001 | Train Loss: 318.539068 | Val Loss: 81.438514 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=81.438514)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 002 | Train Loss: 99.156497 | Val Loss: 29.774160 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=29.774160)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 003 | Train Loss: 41.014556 | Val Loss: 13.851297 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=13.851297)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 004 | Train Loss: 22.011934 | Val Loss: 8.558918 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=8.558918)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 005 | Train Loss: 14.419357 | Val Loss: 6.304316 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=6.304316)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 006 | Train Loss: 10.454085 | Val Loss: 4.686280 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=4.686280)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 007 | Train Loss: 7.868387 | Val Loss: 3.845680 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=3.845680)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 008 | Train Loss: 6.248211 | Val Loss: 3.191059 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=3.191059)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 009 | Train Loss: 5.114729 | Val Loss: 2.804919 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=2.804919)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 010 | Train Loss: 4.266265 | Val Loss: 2.400999 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=2.400999)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 011 | Train Loss: 3.670849 | Val Loss: 2.160378 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=2.160378)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 012 | Train Loss: 3.208276 | Val Loss: 1.965864 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.965864)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 013 | Train Loss: 2.857385 | Val Loss: 1.854259 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.854259)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 014 | Train Loss: 2.545371 | Val Loss: 1.755534 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.755534)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 015 | Train Loss: 2.323258 | Val Loss: 1.635172 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.635172)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 016 | Train Loss: 2.135793 | Val Loss: 1.556911 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.556911)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 017 | Train Loss: 1.951210 | Val Loss: 1.506136 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.506136)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 018 | Train Loss: 1.831726 | Val Loss: 1.457209 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.457209)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 019 | Train Loss: 1.688245 | Val Loss: 1.398977 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.398977)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 020 | Train Loss: 1.579630 | Val Loss: 1.359474 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.359474)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 021 | Train Loss: 1.478844 | Val Loss: 1.320802 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.320802)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 022 | Train Loss: 1.402812 | Val Loss: 1.298414 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.298414)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 023 | Train Loss: 1.309194 | Val Loss: 1.262660 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.262660)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 024 | Train Loss: 1.229015 | Val Loss: 1.249122 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.249122)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 025 | Train Loss: 1.163878 | Val Loss: 1.236126 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.236126)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 026 | Train Loss: 1.101801 | Val Loss: 1.200440 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.200440)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 027 | Train Loss: 1.039828 | Val Loss: 1.182981 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.182981)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 028 | Train Loss: 0.980529 | Val Loss: 1.186791 | LR: 5.00e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 029 | Train Loss: 0.930632 | Val Loss: 1.154743 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.154743)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 030 | Train Loss: 0.887174 | Val Loss: 1.166998 | LR: 5.00e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 031 | Train Loss: 0.843978 | Val Loss: 1.146198 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.146198)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 032 | Train Loss: 0.815367 | Val Loss: 1.130331 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.130331)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 033 | Train Loss: 0.784752 | Val Loss: 1.134780 | LR: 5.00e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 034 | Train Loss: 0.755035 | Val Loss: 1.128379 | LR: 5.00e-04\nğŸ’¾ Saved new best model (val_loss=1.128379)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 035 | Train Loss: 0.721758 | Val Loss: 1.134540 | LR: 5.00e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 036 | Train Loss: 0.701733 | Val Loss: 1.132248 | LR: 5.00e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 037 | Train Loss: 0.691491 | Val Loss: 1.129015 | LR: 2.50e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 038 | Train Loss: 0.513652 | Val Loss: 1.016528 | LR: 2.50e-04\nğŸ’¾ Saved new best model (val_loss=1.016528)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 039 | Train Loss: 0.445459 | Val Loss: 1.005388 | LR: 2.50e-04\nğŸ’¾ Saved new best model (val_loss=1.005388)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 040 | Train Loss: 0.417942 | Val Loss: 1.010879 | LR: 2.50e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 041 | Train Loss: 0.411164 | Val Loss: 1.016820 | LR: 2.50e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 042 | Train Loss: 0.405491 | Val Loss: 1.019798 | LR: 1.25e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 043 | Train Loss: 0.341997 | Val Loss: 0.978885 | LR: 1.25e-04\nğŸ’¾ Saved new best model (val_loss=0.978885)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 044 | Train Loss: 0.314756 | Val Loss: 0.976614 | LR: 1.25e-04\nğŸ’¾ Saved new best model (val_loss=0.976614)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 045 | Train Loss: 0.305703 | Val Loss: 0.976394 | LR: 1.25e-04\nğŸ’¾ Saved new best model (val_loss=0.976394)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 046 | Train Loss: 0.296407 | Val Loss: 0.992611 | LR: 1.25e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 047 | Train Loss: 0.293725 | Val Loss: 0.994276 | LR: 1.25e-04\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 048 | Train Loss: 0.283520 | Val Loss: 0.996238 | LR: 6.25e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 049 | Train Loss: 0.256713 | Val Loss: 0.975419 | LR: 6.25e-05\nğŸ’¾ Saved new best model (val_loss=0.975419)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 050 | Train Loss: 0.246330 | Val Loss: 0.975781 | LR: 6.25e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 051 | Train Loss: 0.241144 | Val Loss: 0.968746 | LR: 6.25e-05\nğŸ’¾ Saved new best model (val_loss=0.968746)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 052 | Train Loss: 0.229607 | Val Loss: 0.969642 | LR: 6.25e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 053 | Train Loss: 0.230313 | Val Loss: 0.971962 | LR: 6.25e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 054 | Train Loss: 0.224061 | Val Loss: 0.980784 | LR: 3.13e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 055 | Train Loss: 0.209293 | Val Loss: 0.963822 | LR: 3.13e-05\nğŸ’¾ Saved new best model (val_loss=0.963822)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 056 | Train Loss: 0.199199 | Val Loss: 0.960824 | LR: 3.13e-05\nğŸ’¾ Saved new best model (val_loss=0.960824)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 057 | Train Loss: 0.197905 | Val Loss: 0.968573 | LR: 3.13e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 058 | Train Loss: 0.192395 | Val Loss: 0.966727 | LR: 3.13e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 059 | Train Loss: 0.193420 | Val Loss: 0.960105 | LR: 3.13e-05\nğŸ’¾ Saved new best model (val_loss=0.960105)\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 060 | Train Loss: 0.187490 | Val Loss: 0.970065 | LR: 3.13e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 061 | Train Loss: 0.183968 | Val Loss: 0.970354 | LR: 3.13e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 062 | Train Loss: 0.184603 | Val Loss: 0.970935 | LR: 1.56e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 063 | Train Loss: 0.178825 | Val Loss: 0.961557 | LR: 1.56e-05\n","output_type":"stream"},{"name":"stderr","text":"                                                                           \r","output_type":"stream"},{"name":"stdout","text":"ğŸ“˜ Epoch 064 | Train Loss: 0.174347 | Val Loss: 0.963780 | LR: 1.56e-05\nâ¹ Early stopping triggered.\nâœ… Training complete. Best Val Loss: 0.960105\nFinished training. Now testing using best model...\nTest Results: {'mrr': 0.9385330845008674, 'ndcg': 0.9534359349718273, 'recall_at_1': 0.90184, 'recall_at_3': 0.97168, 'recall_at_5': 0.98296, 'recall_at_10': 0.99272, 'recall_at_50': 0.99912, 'l2_dist': 25.877605438232422}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"generate_submission(dir_model, Path(test_path), output_file=\"dav.csv\", device=device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T11:55:24.646746Z","iopub.execute_input":"2025-11-04T11:55:24.647032Z","iopub.status.idle":"2025-11-04T11:55:27.678369Z","shell.execute_reply.started":"2025-11-04T11:55:24.647009Z","shell.execute_reply":"2025-11-04T11:55:27.677728Z"}},"outputs":[{"name":"stdout","text":"Generating submission file...\nâœ“ Saved submission to dav.csv\n","output_type":"stream"},{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"        id                                          embedding\n0        1  [0.027106191962957382, 0.006234061438590288, 0...\n1        2  [-0.0018218876793980598, 0.010847451165318489,...\n2        3  [0.006840117275714874, 0.01300758309662342, 0....\n3        4  [0.06066161394119263, -0.03855207562446594, -0...\n4        5  [0.05592562258243561, 0.02765999175608158, -0....\n...    ...                                                ...\n1495  1496  [-0.01505435723811388, 0.007482768967747688, 0...\n1496  1497  [0.03140179440379143, 0.03376109153032303, 0.0...\n1497  1498  [0.06372750550508499, -0.00012322794646024704,...\n1498  1499  [-0.008761415258049965, 0.02096746675670147, 0...\n1499  1500  [0.002903832122683525, -0.009362362325191498, ...\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.027106191962957382, 0.006234061438590288, 0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[-0.0018218876793980598, 0.010847451165318489,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[0.006840117275714874, 0.01300758309662342, 0....</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.06066161394119263, -0.03855207562446594, -0...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[0.05592562258243561, 0.02765999175608158, -0....</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1496</td>\n      <td>[-0.01505435723811388, 0.007482768967747688, 0...</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1497</td>\n      <td>[0.03140179440379143, 0.03376109153032303, 0.0...</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1498</td>\n      <td>[0.06372750550508499, -0.00012322794646024704,...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1499</td>\n      <td>[-0.008761415258049965, 0.02096746675670147, 0...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1500</td>\n      <td>[0.002903832122683525, -0.009362362325191498, ...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows Ã— 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\nACTIVATIONS = {\n    #\"relu\": nn.ReLU,\n    \"gelu\": nn.GELU,\n    \"silu\": nn.SiLU,\n    'selu': nn.SELU,\n    'celu': nn.CELU\n    #\"leakyrelu\": nn.LeakyReLU\n}\n\ndef objective(trial, train_dataset, val_dataset, epochs: int = 10, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n    layer_choices = [1024, 1536, 2048, 4096]\n    hidden_layers = [trial.suggest_categorical(f\"n_units_l{i}\", layer_choices) for i in range(n_layers)]\n\n    activation_name = trial.suggest_categorical(\"activation\", list(ACTIVATIONS.keys()))\n    activation_fn = ACTIVATIONS[activation_name]\n    \n    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024, 2048, 4096])\n    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n    dropout_rate = trial.suggest_categorical('dropout_rate', [0.1, 0.2, 0.3])\n\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True)\n    \n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    model = SpaceTranslator(\n        input_dim=1024, \n        output_dim=1536,\n        hidden_layers=hidden_layers,\n        activation=activation_fn,\n        dropout_rate=dropout_rate\n    )\n    model = model.to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = ReduceLROnPlateau(\n        optimizer,\n        mode=\"min\",\n        factor=0.5,           # reduce LR by half when plateau\n        patience=2,           # wait 2 epochs before reducing LR\n        min_lr=1e-6,          # don't go below this LR\n        verbose=True\n    )\n\n    best_val_loss = float('inf')\n    no_improvements = 0\n\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_train_loss = 0.0\n\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            outputs = model(X_batch)\n            loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n\n            loss.backward()\n            optimizer.step()\n\n            running_train_loss += loss.item()\n\n        train_loss = running_train_loss / len(train_loader)\n\n        # Validation phase\n        model.eval()\n        running_val_loss = 0.0\n\n        with torch.no_grad():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                outputs = model(X_batch)\n                loss = info_nce_loss(outputs, y_batch, model.logit_scale)\n                running_val_loss += loss.item()\n\n        val_loss = running_val_loss / len(val_loader)\n\n        scheduler.step(val_loss)\n\n        results = test(val_dataset, model, device)\n\n        trial.report(results['mrr'], epoch)\n\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n\n    return results['mrr']\n\n\ndef run_optuna_search(data_path: Path, n_trials: int = 30, epochs: int = 10, n_jobs: int = 1, pruner=None):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n\n    X, Y = get_data(data_path)    \n    train_dataset, val_dataset = get_datasets(X, Y)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset,\n                                   epochs=epochs)\n    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n\n    print(\"Study statistics:\")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Best trial:\")\n    trial = study.best_trial\n    print(\"    Value: \", trial.value)\n    print(\"    Params: \")\n    for k, v in trial.params.items():\n        print(f\"      {k}: {v}\")\n\n    return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:19:56.003443Z","iopub.execute_input":"2025-11-04T12:19:56.004035Z","iopub.status.idle":"2025-11-04T12:19:56.016503Z","shell.execute_reply.started":"2025-11-04T12:19:56.004002Z","shell.execute_reply":"2025-11-04T12:19:56.015625Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"study = run_optuna_search(data_path=data_path, n_trials=150, epochs=40, n_jobs=1)\nstudy.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n\nbest_trial_number = study.best_trial.number\nprint(\"Best params:\", study.best_params)\nprint(\"Best trial number:\", study.best_trial.number)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-04T12:19:59.918380Z","iopub.execute_input":"2025-11-04T12:19:59.918886Z"}},"outputs":[{"name":"stderr","text":"[I 2025-11-04 12:20:12,966] A new study created in memory with name: no-name-60b28e84-1ada-4a77-ad28-8a3fc859e01a\n","output_type":"stream"},{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-04 12:24:37,712] Trial 0 finished with value: 0.9028130716112096 and parameters: {'n_layers': 2, 'n_units_l0': 4096, 'n_units_l1': 1536, 'activation': 'selu', 'batch_size': 512, 'lr': 7.50332230817106e-05, 'dropout_rate': 0.2, 'weight_decay': 0.00013430021043607675}. Best is trial 0 with value: 0.9028130716112096.\n[I 2025-11-04 12:27:44,752] Trial 1 finished with value: 0.9047564361940321 and parameters: {'n_layers': 1, 'n_units_l0': 1536, 'activation': 'selu', 'batch_size': 256, 'lr': 0.00013520855942195204, 'dropout_rate': 0.1, 'weight_decay': 0.004668758209667024}. Best is trial 1 with value: 0.9047564361940321.\n[I 2025-11-04 12:31:21,241] Trial 2 finished with value: 0.9182350042165024 and parameters: {'n_layers': 2, 'n_units_l0': 1024, 'n_units_l1': 2048, 'activation': 'silu', 'batch_size': 256, 'lr': 2.8966031405401815e-05, 'dropout_rate': 0.3, 'weight_decay': 0.00015524028860226148}. Best is trial 2 with value: 0.9182350042165024.\n[I 2025-11-04 12:35:00,329] Trial 3 finished with value: 0.9086947991645231 and parameters: {'n_layers': 1, 'n_units_l0': 4096, 'activation': 'celu', 'batch_size': 1024, 'lr': 0.0005480694087859737, 'dropout_rate': 0.2, 'weight_decay': 0.00047884770449086264}. Best is trial 2 with value: 0.9182350042165024.\n[I 2025-11-04 12:38:29,841] Trial 4 finished with value: 0.8286214624941318 and parameters: {'n_layers': 2, 'n_units_l0': 1536, 'n_units_l1': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 2.002898001898498e-06, 'dropout_rate': 0.2, 'weight_decay': 0.00020604510801225018}. Best is trial 2 with value: 0.9182350042165024.\n[I 2025-11-04 12:38:34,514] Trial 5 pruned. \n[I 2025-11-04 12:38:39,668] Trial 6 pruned. \n[I 2025-11-04 12:38:45,991] Trial 7 pruned. \n[I 2025-11-04 12:38:51,590] Trial 8 pruned. \n[I 2025-11-04 12:39:01,220] Trial 9 pruned. \n[I 2025-11-04 12:39:09,427] Trial 10 pruned. \n[I 2025-11-04 12:39:15,185] Trial 11 pruned. \n[I 2025-11-04 12:39:20,853] Trial 12 pruned. \n[I 2025-11-04 12:39:26,590] Trial 13 pruned. \n[I 2025-11-04 12:39:31,713] Trial 14 pruned. \n[I 2025-11-04 12:39:43,467] Trial 15 pruned. \n[I 2025-11-04 12:39:49,033] Trial 16 pruned. \n[I 2025-11-04 12:40:28,264] Trial 17 pruned. \n[I 2025-11-04 12:40:33,302] Trial 18 pruned. \n[I 2025-11-04 12:40:39,174] Trial 19 pruned. \n[I 2025-11-04 12:40:46,065] Trial 20 pruned. \n[I 2025-11-04 12:41:00,169] Trial 21 pruned. \n[I 2025-11-04 12:41:23,411] Trial 22 pruned. \n[I 2025-11-04 12:41:31,678] Trial 23 pruned. \n[I 2025-11-04 12:44:37,823] Trial 24 finished with value: 0.9206607029536742 and parameters: {'n_layers': 1, 'n_units_l0': 1536, 'activation': 'selu', 'batch_size': 256, 'lr': 0.0005089415388587989, 'dropout_rate': 0.2, 'weight_decay': 0.0036934523785652185}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 12:44:42,614] Trial 25 pruned. \n[I 2025-11-04 12:44:48,278] Trial 26 pruned. \n[I 2025-11-04 12:44:52,582] Trial 27 pruned. \n[I 2025-11-04 12:44:58,579] Trial 28 pruned. \n[I 2025-11-04 12:45:05,661] Trial 29 pruned. \n[I 2025-11-04 12:45:10,539] Trial 30 pruned. \n[I 2025-11-04 12:46:25,047] Trial 31 pruned. \n[I 2025-11-04 12:49:30,805] Trial 32 finished with value: 0.9154125729258281 and parameters: {'n_layers': 1, 'n_units_l0': 1536, 'activation': 'selu', 'batch_size': 256, 'lr': 0.0002188684908564621, 'dropout_rate': 0.2, 'weight_decay': 0.0050690451595502865}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 12:49:35,389] Trial 33 pruned. \n[I 2025-11-04 12:50:03,337] Trial 34 pruned. \n[I 2025-11-04 12:51:52,301] Trial 35 pruned. \n[I 2025-11-04 12:52:19,385] Trial 36 pruned. \n[I 2025-11-04 12:52:24,061] Trial 37 pruned. \n[I 2025-11-04 12:55:22,084] Trial 38 finished with value: 0.9094182247676024 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.0006759211715711061, 'dropout_rate': 0.2, 'weight_decay': 0.0008546614746620734}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 12:58:21,236] Trial 39 finished with value: 0.9088542387688072 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.0007135592231591052, 'dropout_rate': 0.2, 'weight_decay': 0.006311782300829477}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 12:58:26,941] Trial 40 pruned. \n[I 2025-11-04 12:58:31,495] Trial 41 pruned. \n[I 2025-11-04 13:01:30,085] Trial 42 finished with value: 0.9091630964564212 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.0006298734364962454, 'dropout_rate': 0.2, 'weight_decay': 0.004425808971669928}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 13:04:28,801] Trial 43 finished with value: 0.9130361083623281 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.0004059807364126846, 'dropout_rate': 0.2, 'weight_decay': 0.004150142779650215}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 13:07:28,579] Trial 44 finished with value: 0.91117807601631 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.00034228505042769897, 'dropout_rate': 0.2, 'weight_decay': 0.002497446641501529}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 13:07:43,197] Trial 45 pruned. \n[I 2025-11-04 13:10:42,912] Trial 46 finished with value: 0.9147048089045484 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.00026490588999125356, 'dropout_rate': 0.3, 'weight_decay': 0.0032990857567667654}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 13:10:48,433] Trial 47 pruned. \n[I 2025-11-04 13:10:52,279] Trial 48 pruned. \n[I 2025-11-04 13:10:57,345] Trial 49 pruned. \n[I 2025-11-04 13:11:01,837] Trial 50 pruned. \n[I 2025-11-04 13:14:01,190] Trial 51 finished with value: 0.9159338551744283 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.00028923430517652893, 'dropout_rate': 0.3, 'weight_decay': 0.00300378927467836}. Best is trial 24 with value: 0.9206607029536742.\n[I 2025-11-04 13:17:01,267] Trial 52 finished with value: 0.9156988501936244 and parameters: {'n_layers': 1, 'n_units_l0': 2048, 'activation': 'celu', 'batch_size': 512, 'lr': 0.0002667144727698129, 'dropout_rate': 0.3, 'weight_decay': 0.0031194231508722254}. Best is trial 24 with value: 0.9206607029536742.\n","output_type":"stream"}],"execution_count":null}]}