{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-30T15:42:35.751674Z",
     "iopub.status.busy": "2025-10-30T15:42:35.751118Z",
     "iopub.status.idle": "2025-10-30T15:42:35.758128Z",
     "shell.execute_reply": "2025-10-30T15:42:35.757416Z",
     "shell.execute_reply.started": "2025-10-30T15:42:35.751653Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_layers, dropout_rate):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        last = input_dim\n",
    "        for hidden in hidden_layers:\n",
    "            layers += [nn.Linear(last, hidden), nn.GELU(), nn.LayerNorm(hidden), nn.Dropout(dropout_rate)]\n",
    "            last = hidden\n",
    "        layers.append(nn.Linear(last, output_dim))\n",
    "        self.net = nn.Sequential(*layers)\n",
    "        \n",
    "        self.apply(self.init_weights)\n",
    "        \n",
    "    def init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            nn.init.xavier_uniform_(module.weight)\n",
    "            if module.bias is not None:\n",
    "                nn.init.constant_(module.bias, 0.0)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            nn.init.ones_(module.weight)\n",
    "            nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.normalize(x, dim=-1)\n",
    "        out = self.net(x)\n",
    "        \n",
    "        dir_vec = F.normalize(out, dim=-1)\n",
    "        return dir_vec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eval.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T15:42:35.759522Z",
     "iopub.status.busy": "2025-10-30T15:42:35.759255Z",
     "iopub.status.idle": "2025-10-30T15:42:35.780751Z",
     "shell.execute_reply": "2025-10-30T15:42:35.780153Z",
     "shell.execute_reply.started": "2025-10-30T15:42:35.759482Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "'''Code from https://github.com/Mamiglia/challenge'''\n",
    "\n",
    "def mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Compute Mean Reciprocal Rank (MRR)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "    Returns:\n",
    "        mrr: Mean Reciprocal Rank\n",
    "    \"\"\"\n",
    "    reciprocal_ranks = []\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n",
    "        else:\n",
    "            reciprocal_ranks.append(0.0)\n",
    "    return np.mean(reciprocal_ranks)\n",
    "\n",
    "\n",
    "def recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n",
    "    \"\"\"Compute Recall@k\n",
    "    Args:\n",
    "        pred_indices: (N, N) array of top indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        recall: Recall@k\n",
    "    \"\"\"\n",
    "    recall = 0\n",
    "    for i in range(len(gt_indices)):\n",
    "        if gt_indices[i] in pred_indices[i, :k]:\n",
    "            recall += 1\n",
    "    recall /= len(gt_indices)\n",
    "    return recall\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n",
    "    \"\"\"\n",
    "    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n",
    "    Args:\n",
    "        pred_indices: (N, K) array of predicted indices for N queries\n",
    "        gt_indices: (N,) array of ground truth indices\n",
    "        k: number of top predictions to consider\n",
    "    Returns:\n",
    "        ndcg: NDCG@k\n",
    "    \"\"\"\n",
    "    ndcg_total = 0.0\n",
    "    for i in range(len(gt_indices)):\n",
    "        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n",
    "        if matches.size > 0:\n",
    "            rank = matches[0] + 1\n",
    "            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n",
    "    return ndcg_total / len(gt_indices)\n",
    "\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n",
    "    \"\"\"Evaluate retrieval performance using cosine similarity\n",
    "    Args:\n",
    "        translated_embd: (N_captions, D) translated caption embeddings\n",
    "        image_embd: (N_images, D) image embeddings\n",
    "        gt_indices: (N_captions,) ground truth image indices for each caption\n",
    "        max_indices: number of top predictions to consider\n",
    "    Returns:\n",
    "        results: dict of evaluation metrics\n",
    "    \n",
    "    \"\"\"\n",
    "    # Compute similarity matrix\n",
    "    if isinstance(translated_embd, np.ndarray):\n",
    "        translated_embd = torch.from_numpy(translated_embd).float()\n",
    "    if isinstance(image_embd, np.ndarray):\n",
    "        image_embd = torch.from_numpy(image_embd).float()\n",
    "    \n",
    "    n_queries = translated_embd.shape[0]\n",
    "    device = translated_embd.device\n",
    "    \n",
    "    # Prepare containers for the fragments to be reassembled\n",
    "    all_sorted_indices = []\n",
    "    l2_distances = []\n",
    "    \n",
    "    # Process in batches - the narrow gate approach\n",
    "    for start_idx in range(0, n_queries, batch_size):\n",
    "        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n",
    "        batch_translated = translated_embd[batch_slice]\n",
    "        batch_img_embd = image_embd[batch_slice]\n",
    "        \n",
    "        # Compute similarity only for this batch\n",
    "        batch_similarity = batch_translated @ batch_img_embd.T\n",
    "\n",
    "        # Get top-k predictions for this batch\n",
    "        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n",
    "        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n",
    "\n",
    "        # Compute L2 distance for this batch\n",
    "        batch_gt = gt_indices[batch_slice]\n",
    "        batch_gt_embeddings = image_embd[batch_gt]\n",
    "        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n",
    "        l2_distances.append(batch_l2)\n",
    "    \n",
    "    # Reassemble the fragments\n",
    "    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n",
    "    \n",
    "    # Apply the sacred metrics to the whole\n",
    "    metrics = {\n",
    "        'mrr': mrr,\n",
    "        'ndcg': ndcg,\n",
    "        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n",
    "        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n",
    "        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n",
    "        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n",
    "        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n",
    "    }\n",
    "    \n",
    "    results = {\n",
    "        name: func(sorted_indices, gt_indices)\n",
    "        for name, func in metrics.items()\n",
    "    }\n",
    "    \n",
    "    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n",
    "    results['l2_dist'] = l2_dist\n",
    "    \n",
    "    return results\n",
    "\n",
    "def eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: Translator, device) -> dict:\n",
    "    gt_indices = torch.arange(len(y_val))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        translated = model(x_val.to(device)).to('cpu')\n",
    "\n",
    "    results = evaluate_retrieval(translated, y_val, gt_indices)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def generate_submission(model: Translator, test_path: Path, output_file=\"submission.csv\", device=None):\n",
    "    test_data = np.load(test_path)\n",
    "    sample_ids = test_data['captions/ids']\n",
    "    test_embds = test_data['captions/embeddings']\n",
    "    test_embds = torch.from_numpy(test_embds).float()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred_embds = model(test_embds.to(device)).cpu()\n",
    "\n",
    "    print(\"Generating submission file...\")\n",
    "\n",
    "    if isinstance(pred_embds, torch.Tensor):\n",
    "        pred_embds = pred_embds.cpu().numpy()\n",
    "\n",
    "    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n",
    "\n",
    "    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n",
    "    print(f\"✓ Saved submission to {output_file}\")\n",
    "\n",
    "    return df_submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T15:42:35.913103Z",
     "iopub.status.busy": "2025-10-30T15:42:35.912931Z",
     "iopub.status.idle": "2025-10-30T15:42:35.917204Z",
     "shell.execute_reply": "2025-10-30T15:42:35.916649Z",
     "shell.execute_reply.started": "2025-10-30T15:42:35.913089Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_dim = 1024\n",
    "output_dim = 1536\n",
    "hidden_layers=[1024, 4096]\n",
    "dropout_rate = 0.3\n",
    "\n",
    "batch_size= 1024\n",
    "lr= 0.0001 # 0.0006\n",
    "epochs= 200\n",
    "patience = 10\n",
    "temp = 0.05\n",
    "data_path= '/kaggle/input/aml-competition/train/train/train.npz'\n",
    "test_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n",
    "\n",
    "model_save_path= './models/exp1.pth'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T15:42:35.995435Z",
     "iopub.status.busy": "2025-10-30T15:42:35.995241Z",
     "iopub.status.idle": "2025-10-30T15:42:36.028766Z",
     "shell.execute_reply": "2025-10-30T15:42:36.027951Z",
     "shell.execute_reply.started": "2025-10-30T15:42:35.995420Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import random_split\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def info_nce_loss(dir_preds, img_targets, temp=0.07):\n",
    "    logits = (dir_preds @ img_targets.T) / temp\n",
    "    labels = torch.arange(logits.size(0), device=logits.device)\n",
    "    #loss = F.cross_entropy(logits, labels)    \n",
    "    #return loss\n",
    "    loss_t2i = F.cross_entropy(logits, labels)          \n",
    "    loss_i2t = F.cross_entropy(logits.T, labels)        \n",
    "    return 0.5 * (loss_t2i + loss_i2t)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model: Translator, model_path: Path, \n",
    "                train_dataset: TensorDataset, val_dataset: TensorDataset, batch_size: int,\n",
    "                epochs: int, lr: float, temp: float, patience: int) -> Translator:\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvements = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = F.normalize(y_batch, dim=-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(X_batch)\n",
    "\n",
    "            loss = info_nce_loss(outputs, y_batch, temp=temp)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        val_loss = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "\n",
    "                loss = info_nce_loss(outputs, y_batch, temp=temp)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}: Train Loss = {train_loss:.6f}, Val Loss = {val_loss:.6f}\")\n",
    "        test(val_dataset, model, device)\n",
    "\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            no_improvements = 0\n",
    "\n",
    "            Path(model_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "\n",
    "            print(f\"✓ Saved best model (val_loss={val_loss:.6f})\")\n",
    "        elif no_improvements >= patience:\n",
    "            return model\n",
    "        else:\n",
    "            no_improvements += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "def load_data(data_path: Path, config: dict) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "\n",
    "    data = np.load(data_path)\n",
    "    caption_embeddings = data['captions/embeddings']\n",
    "    image_embeddings = data['images/embeddings']\n",
    "    caption_labels = data['captions/label']\n",
    "\n",
    "    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n",
    "    \n",
    "    print('Texts shape', X_abs.shape)\n",
    "    print('Images shape', X_abs.shape)\n",
    "    \n",
    "    dataset = TensorDataset(X_abs, y_abs)\n",
    "    train_dataset, val_dataset = random_split(dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n",
    "    \n",
    "    return train_dataset, val_dataset\n",
    "\n",
    "def test(val_dataset: TensorDataset, model: Translator, device):\n",
    "    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n",
    "    for x_val, y_val in val_loader:\n",
    "        results = eval_on_val(x_val, y_val, model=model, device=device)\n",
    "    return results\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-30T15:42:36.030528Z",
     "iopub.status.busy": "2025-10-30T15:42:36.030049Z",
     "iopub.status.idle": "2025-10-30T15:45:44.205363Z",
     "shell.execute_reply": "2025-10-30T15:45:44.204641Z",
     "shell.execute_reply.started": "2025-10-30T15:42:36.030480Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset, val_dataset = load_data(data_path, dict())\n",
    "model_args = {\n",
    "   'input_dim': input_dim,\n",
    "   'output_dim': output_dim,\n",
    "   'hidden_layers': hidden_layers,\n",
    "}\n",
    "model = Translator(**model_args).to(device)\n",
    "\n",
    "train_model(model, model_save_path, train_dataset, val_dataset, batch_size, epochs, lr, temp, patience)\n",
    "\n",
    "print('Finished training. Now testing using best model...')\n",
    "\n",
    "state = torch.load(model_save_path)\n",
    "model.load_state_dict(state)\n",
    "results = test(val_dataset, model, device)\n",
    "print(\"Test Results:\", results)\n",
    "\n",
    "\n",
    "generate_submission(model, Path(test_path), device=device)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.pruners import MedianPruner\n",
    "\n",
    "def objective(trial, train_dataset, val_dataset,\n",
    "              epochs: int = 10, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 4)\n",
    "    layer_choices = [512, 1024, 2048, 4096]\n",
    "    hidden_layers = [trial.suggest_categorical(f\"n_units_l{i}\", layer_choices) for i in range(n_layers)]\n",
    "\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [256, 512, 1024, 2048, 4096])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-6, 1e-3, log=True)\n",
    "    temp = trial.suggest_float(\"temp\", 0.05, 0.3, log=True)\n",
    "    dropout_rate = trial.suggest_categorical('dropout_rate', [0.1,0.2,0.3,0.4,0.5])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = Translator(input_dim=input_dim, output_dim=output_dim, hidden_layers=hidden_layers, dropout_rate=dropout_rate)\n",
    "    model = model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-4)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            y_batch = F.normalize(y_batch, dim=-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = info_nce_loss(outputs, y_batch, temp=temp)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss /= len(train_loader)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_batch = F.normalize(y_batch, dim=-1)\n",
    "                \n",
    "                outputs = model(X_batch)\n",
    "                loss = info_nce_loss(outputs, y_batch, temp=temp)\n",
    "                val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        results = test(val_dataset, model, device)\n",
    "\n",
    "        trial.report(results['mrr'], epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "\n",
    "\n",
    "    return results['mrr']\n",
    "\n",
    "\n",
    "def run_optuna_search(data_path: Path,\n",
    "                      n_trials: int = 30, epochs: int = 10,\n",
    "                      n_jobs: int = 1, sampler=None, pruner=None):\n",
    "    if pruner is None:\n",
    "        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n",
    "    train_dataset, val_dataset = load_data(data_path, config={})\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n",
    "    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset,\n",
    "                                   epochs=epochs)\n",
    "    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n",
    "\n",
    "    print(\"Study statistics:\")\n",
    "    print(\"  Number of finished trials: \", len(study.trials))\n",
    "    print(\"  Best trial:\")\n",
    "    trial = study.best_trial\n",
    "    print(\"    Value: \", trial.value)\n",
    "    print(\"    Params: \")\n",
    "    for k, v in trial.params.items():\n",
    "        print(f\"      {k}: {v}\")\n",
    "\n",
    "    return study"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "study = run_optuna_search(data_path=data_path,\n",
    "                          n_trials=100, epochs=10, n_jobs=1)\n",
    "study.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n",
    "\n",
    "best_trial_number = study.best_trial.number\n",
    "print(\"Best params:\", study.best_params)\n",
    "print(\"Best trial number:\", study.best_trial.number)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 14220991,
     "sourceId": 117959,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31154,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
