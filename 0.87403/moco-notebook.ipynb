{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":13718660,"sourceType":"datasetVersion","datasetId":8727971}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nimport torch\nimport numpy as np\n\nclass SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        hidden_layers,\n        activation,\n        dropout_rate,\n        init_method: str = 'xavier'\n    ):\n        super().__init__()\n\n        self.init_method = init_method.lower()\n        if self.init_method not in ['xavier', 'kaiming']:\n            raise ValueError(\"Unsupported init_method\")\n\n        layers = []\n        last = input_dim\n\n        for hidden in hidden_layers:\n            layers += [\n                nn.Linear(last, hidden),\n                nn.LayerNorm(hidden),\n                activation(),\n                nn.Dropout(dropout_rate)\n            ]\n            last = hidden\n\n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        # Applica l'inizializzazione scelta\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.init_method == 'kaiming':\n                nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n            else:\n                nn.init.xavier_uniform_(module.weight)\n            \n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n                \n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n\n    def forward(self, x):\n        return F.normalize(self.net(x), p=2, dim=1) # Usata la versione normalizzata nell'esempio","metadata":{"execution":{"iopub.status.busy":"2025-11-14T13:53:00.299567Z","iopub.execute_input":"2025-11-14T13:53:00.299842Z","iopub.status.idle":"2025-11-14T13:53:00.306972Z","shell.execute_reply.started":"2025-11-14T13:53:00.299822Z","shell.execute_reply":"2025-11-14T13:53:00.306254Z"},"id":"pErT3mEBFSji","trusted":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n\n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n\n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n\n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n\n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n\n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n\n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n\n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n\n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n\n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n\n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: nn.Module, device) -> dict:\n    gt_indices = torch.arange(len(y_val))\n\n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n\n    return results\n\n\ndef generate_submission(model: nn.Module, test_path: Path, output_file=\"submission-dirmodel.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"‚úì Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"execution":{"iopub.status.busy":"2025-11-14T13:53:02.135393Z","iopub.execute_input":"2025-11-14T13:53:02.135989Z","iopub.status.idle":"2025-11-14T13:53:02.150362Z","shell.execute_reply.started":"2025-11-14T13:53:02.135966Z","shell.execute_reply":"2025-11-14T13:53:02.149722Z"},"id":"umdq3vIdKFfa","trusted":true},"outputs":[],"execution_count":35},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\ndef moco_info_nce_loss(q, k, queue, logit_scale):\n    # Positivo\n    l_pos = torch.sum(q * k, dim=1, keepdim=True)  # [batch,1]\n\n    # Negativi\n    l_neg = q @ queue.T  # [batch, queue_size]\n\n    logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n    logits = torch.cat([l_pos, l_neg], dim=1)\n    logits = logits * logit_scale.exp()\n\n    # Labels: positivo sempre in posizione 0\n    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n\n    return F.cross_entropy(logits, labels)\n\n\n@torch.no_grad()\ndef enqueue(queue, keys, queue_ptr):\n    batch_size = keys.shape[0]\n    queue_size = queue.shape[0]\n    ptr = int(queue_ptr[0])\n\n    if ptr + batch_size <= queue_size:\n        # Slice diretta\n        queue[ptr:ptr+batch_size, :] = keys\n    else:\n        #  (wrap-around)\n        first_part = queue_size - ptr\n        queue[ptr:, :] = keys[:first_part, :]\n        queue[:batch_size - first_part, :] = keys[first_part:, :]\n\n    # Aggiorna il puntatore\n    queue_ptr[0] = (ptr + batch_size) % queue_size\n\ndef train_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    \n    # Lo scheduler segue la Val Loss (che si minimizza), quindi mode='min'\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, threshold=0.003, min_lr=1e-5\n    )\n\n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    best_val_loss = float('inf')\n    loss_no_improvements = 0 \n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n\n        for text_batch, image_emb_batch in progress_bar:\n            text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n            \n            optimizer.zero_grad()\n\n            q = model(text_batch)\n            k = image_emb_batch\n\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        # --- Validation ---\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for text_batch, image_emb_batch in val_loader:\n                text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n\n                q = model(text_batch)\n                k = image_emb_batch\n\n                loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n                running_val_loss += loss.item()\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        results = test(val_dataset, model, device)\n        mrr = results['mrr']\n\n        scheduler.step(avg_val_loss) \n\n        print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | \"\n              f\"MRR: {mrr:.6f} | Recall-1: {results['recall_at_1']:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            loss_no_improvements = 0 \n            \n            # Salvataggio del modello con la Validation Loss migliore\n            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), save_path)\n            print(f\"üíæ Saved new best model (Val Loss={avg_val_loss:.6f})\")\n        else:\n            loss_no_improvements += 1\n            if loss_no_improvements >= patience:\n                print(\"‚èπ Early stopping triggered based on Validation Loss.\")\n                break\n\n    print(f\"‚úÖ Training complete\")\n    return model\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n\ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n\n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n\n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model: nn.Module, device):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device)\n    return results\n\ndef center(X: torch.Tensor):\n    mean = X.mean(dim=0, keepdim=True)\n    return X - mean, mean","metadata":{"execution":{"iopub.status.busy":"2025-11-14T13:53:04.835075Z","iopub.execute_input":"2025-11-14T13:53:04.835770Z","iopub.status.idle":"2025-11-14T13:53:04.853820Z","shell.execute_reply.started":"2025-11-14T13:53:04.835749Z","shell.execute_reply":"2025-11-14T13:53:04.853230Z"},"id":"_Ng-afH6FqPt","trusted":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_path= '/kaggle/input/challenge-dataset/train.npz'\ntest_path= '/kaggle/input/challenge-dataset/test.clean.npz'\n\nsave_path = './models/dir-model.pth'","metadata":{"execution":{"iopub.status.busy":"2025-11-14T13:53:07.861414Z","iopub.execute_input":"2025-11-14T13:53:07.861995Z","iopub.status.idle":"2025-11-14T13:53:07.866172Z","shell.execute_reply.started":"2025-11-14T13:53:07.861969Z","shell.execute_reply":"2025-11-14T13:53:07.865369Z"},"id":"X_QhTeUoFrLm","trusted":true},"outputs":[],"execution_count":37},{"cell_type":"code","source":"x, y = get_data(data_path)\n\n#x_centered, x_center = center(x)\n\ntrain_dataset, val_dataset = get_datasets(x, y)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-13T18:03:49.428087Z","iopub.status.busy":"2025-11-13T18:03:49.427668Z","iopub.status.idle":"2025-11-13T18:04:02.877540Z","shell.execute_reply":"2025-11-13T18:04:02.876747Z","shell.execute_reply.started":"2025-11-13T18:03:49.428065Z"},"id":"zI0blWNyFtDI","outputId":"e2e2f865-ee19-4a00-b275-f0678aa7f501","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Texts shape torch.Size([125000, 1024])\n","Images shape torch.Size([125000, 1536])\n"]}],"execution_count":31},{"cell_type":"code","source":"input_dim = x.shape[1]\noutput_dim = y.shape[1]\nhidden_layers = [1256, 2048]\ndropout_rate = 0.5\nbatch_size = 256\nlr = 0.01\nepochs = 250\npatience = 5\n\nqueue_size = 10000\n\nmodel_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    'hidden_layers': hidden_layers,\n    'dropout_rate': dropout_rate,\n    'activation': nn.GELU\n}\n\nmodel = SpaceTranslator(**model_args)\n\ntrain_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size)\n\nprint('Finished training. Now testing using best model...')\n\nstate = torch.load(save_path)\nmodel.load_state_dict(state)\nresults = test(val_dataset, model, device)\nprint(\"Test Results:\", results)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"execution":{"iopub.execute_input":"2025-11-13T18:52:01.105432Z","iopub.status.busy":"2025-11-13T18:52:01.104845Z","iopub.status.idle":"2025-11-13T18:57:31.393717Z","shell.execute_reply":"2025-11-13T18:57:31.393070Z","shell.execute_reply.started":"2025-11-13T18:52:01.105407Z"},"id":"P5k77RkVFuwg","outputId":"c5d2268c-1c45-4399-e821-b1d5572f5f60","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 001 | Train Loss: 8.727778 | Val Loss: 5.525506 | MRR: 0.725039 | Recall-1: 0.590840 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.725039)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 002 | Train Loss: 5.365557 | Val Loss: 4.570784 | MRR: 0.837310 | Recall-1: 0.743840 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.837310)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 003 | Train Loss: 4.706854 | Val Loss: 4.170775 | MRR: 0.870314 | Recall-1: 0.793680 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.870314)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 004 | Train Loss: 4.321810 | Val Loss: 3.900128 | MRR: 0.887549 | Recall-1: 0.819160 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.887549)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 005 | Train Loss: 4.050368 | Val Loss: 3.720185 | MRR: 0.898636 | Recall-1: 0.837000 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.898636)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 006 | Train Loss: 3.821990 | Val Loss: 3.571072 | MRR: 0.906888 | Recall-1: 0.849640 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.906888)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 007 | Train Loss: 3.631227 | Val Loss: 3.457870 | MRR: 0.912080 | Recall-1: 0.857120 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.912080)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 008 | Train Loss: 3.470839 | Val Loss: 3.378798 | MRR: 0.915122 | Recall-1: 0.862640 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.915122)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                         \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 009 | Train Loss: 3.333408 | Val Loss: 3.284214 | MRR: 0.919675 | Recall-1: 0.869840 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.919675)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 010 | Train Loss: 3.199440 | Val Loss: 3.215446 | MRR: 0.922864 | Recall-1: 0.874840 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.922864)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 011 | Train Loss: 3.080352 | Val Loss: 3.192078 | MRR: 0.923270 | Recall-1: 0.874840 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.923270)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 012 | Train Loss: 2.976312 | Val Loss: 3.119946 | MRR: 0.927357 | Recall-1: 0.881920 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.927357)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 013 | Train Loss: 2.867031 | Val Loss: 3.097158 | MRR: 0.928782 | Recall-1: 0.884200 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.928782)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 014 | Train Loss: 2.774466 | Val Loss: 3.060852 | MRR: 0.930447 | Recall-1: 0.886720 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.930447)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 015 | Train Loss: 2.683679 | Val Loss: 3.018417 | MRR: 0.931809 | Recall-1: 0.889000 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.931809)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 016 | Train Loss: 2.593929 | Val Loss: 2.993433 | MRR: 0.932717 | Recall-1: 0.890920 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.932717)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 017 | Train Loss: 2.516491 | Val Loss: 2.986677 | MRR: 0.933537 | Recall-1: 0.892760 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.933537)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 018 | Train Loss: 2.445432 | Val Loss: 2.973553 | MRR: 0.933315 | Recall-1: 0.891440 | LR: 1.00e-02\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 019 | Train Loss: 2.368953 | Val Loss: 2.946566 | MRR: 0.934461 | Recall-1: 0.893560 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.934461)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 020 | Train Loss: 2.308971 | Val Loss: 2.932775 | MRR: 0.934600 | Recall-1: 0.893960 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.934600)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 021 | Train Loss: 2.239273 | Val Loss: 2.912142 | MRR: 0.935215 | Recall-1: 0.894720 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.935215)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 022 | Train Loss: 2.173672 | Val Loss: 2.896063 | MRR: 0.936930 | Recall-1: 0.897880 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.936930)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 023 | Train Loss: 2.119078 | Val Loss: 2.897631 | MRR: 0.936983 | Recall-1: 0.898040 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.936983)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 024 | Train Loss: 2.072061 | Val Loss: 2.890368 | MRR: 0.938616 | Recall-1: 0.900800 | LR: 1.00e-02\n","üíæ Saved new best model (MRR=0.938616)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 025 | Train Loss: 2.010360 | Val Loss: 2.869364 | MRR: 0.937530 | Recall-1: 0.899320 | LR: 1.00e-02\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 026 | Train Loss: 1.961379 | Val Loss: 2.903009 | MRR: 0.937849 | Recall-1: 0.899280 | LR: 1.00e-02\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 027 | Train Loss: 1.916301 | Val Loss: 2.864191 | MRR: 0.937485 | Recall-1: 0.899280 | LR: 1.00e-02\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 028 | Train Loss: 1.871107 | Val Loss: 2.867248 | MRR: 0.938820 | Recall-1: 0.901320 | LR: 5.00e-03\n","üíæ Saved new best model (MRR=0.938820)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 029 | Train Loss: 1.596607 | Val Loss: 2.851360 | MRR: 0.941144 | Recall-1: 0.905040 | LR: 5.00e-03\n","üíæ Saved new best model (MRR=0.941144)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 030 | Train Loss: 1.497734 | Val Loss: 2.861686 | MRR: 0.941666 | Recall-1: 0.906040 | LR: 5.00e-03\n","üíæ Saved new best model (MRR=0.941666)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 031 | Train Loss: 1.452512 | Val Loss: 2.872866 | MRR: 0.941747 | Recall-1: 0.906160 | LR: 5.00e-03\n","üíæ Saved new best model (MRR=0.941747)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 032 | Train Loss: 1.409349 | Val Loss: 2.909070 | MRR: 0.940283 | Recall-1: 0.904240 | LR: 2.50e-03\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 033 | Train Loss: 1.279878 | Val Loss: 2.898021 | MRR: 0.942009 | Recall-1: 0.906920 | LR: 2.50e-03\n","üíæ Saved new best model (MRR=0.942009)\n"]},{"name":"stderr","output_type":"stream","text":["                                                                          \r"]},{"name":"stdout","output_type":"stream","text":["Epoch 034 | Train Loss: 1.216193 | Val Loss: 2.897232 | MRR: 0.941551 | Recall-1: 0.905920 | LR: 2.50e-03\n","‚èπ Early stopping triggered based on Validation Loss.\n","‚úÖ Training complete. Best MRR: 0.942009\n","Finished training. Now testing using best model...\n","Test Results: {'mrr': 0.942009235100382, 'ndcg': 0.9561381249868742, 'recall_at_1': 0.90692, 'recall_at_3': 0.97312, 'recall_at_5': 0.98604, 'recall_at_10': 0.9942, 'recall_at_50': 0.9994, 'l2_dist': 25.848154067993164}\n"]}],"execution_count":49},{"cell_type":"code","source":"generate_submission(model, Path(test_path), output_file=\"mojo-pin.csv\", device=device)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461},"execution":{"iopub.execute_input":"2025-11-13T18:57:41.521247Z","iopub.status.busy":"2025-11-13T18:57:41.520998Z","iopub.status.idle":"2025-11-13T18:57:44.603192Z","shell.execute_reply":"2025-11-13T18:57:44.602585Z","shell.execute_reply.started":"2025-11-13T18:57:41.521229Z"},"id":"_Wk_esyYFw8X","outputId":"4c5c742d-e576-4c86-ecd7-48e853ef0ea9","trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Generating submission file...\n","‚úì Saved submission to mojo-pin.csv\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>embedding</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>[0.02428484708070755, 0.027138926088809967, -0...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>[-0.006308969110250473, -0.005466327536851168,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","      <td>[0.011657739989459515, -0.03169963136315346, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>4</td>\n","      <td>[0.060520488768815994, -0.02953926846385002, -...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>5</td>\n","      <td>[0.06194249540567398, 0.04723084717988968, 0.0...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1495</th>\n","      <td>1496</td>\n","      <td>[0.010205189697444439, -0.0313861183822155, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>1496</th>\n","      <td>1497</td>\n","      <td>[0.006104839034378529, 0.04210679605603218, 0....</td>\n","    </tr>\n","    <tr>\n","      <th>1497</th>\n","      <td>1498</td>\n","      <td>[0.03780222684144974, -0.0013050471898168325, ...</td>\n","    </tr>\n","    <tr>\n","      <th>1498</th>\n","      <td>1499</td>\n","      <td>[-0.01464053988456726, 0.003734411671757698, 0...</td>\n","    </tr>\n","    <tr>\n","      <th>1499</th>\n","      <td>1500</td>\n","      <td>[0.01115664467215538, -0.028162026777863503, -...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1500 rows √ó 2 columns</p>\n","</div>"],"text/plain":["        id                                          embedding\n","0        1  [0.02428484708070755, 0.027138926088809967, -0...\n","1        2  [-0.006308969110250473, -0.005466327536851168,...\n","2        3  [0.011657739989459515, -0.03169963136315346, 0...\n","3        4  [0.060520488768815994, -0.02953926846385002, -...\n","4        5  [0.06194249540567398, 0.04723084717988968, 0.0...\n","...    ...                                                ...\n","1495  1496  [0.010205189697444439, -0.0313861183822155, 0....\n","1496  1497  [0.006104839034378529, 0.04210679605603218, 0....\n","1497  1498  [0.03780222684144974, -0.0013050471898168325, ...\n","1498  1499  [-0.01464053988456726, 0.003734411671757698, 0...\n","1499  1500  [0.01115664467215538, -0.028162026777863503, -...\n","\n","[1500 rows x 2 columns]"]},"execution_count":50,"metadata":{},"output_type":"execute_result"}],"execution_count":50},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_weights.pth\")","metadata":{"execution":{"iopub.execute_input":"2025-11-13T19:03:49.036715Z","iopub.status.busy":"2025-11-13T19:03:49.036143Z","iopub.status.idle":"2025-11-13T19:03:49.078196Z","shell.execute_reply":"2025-11-13T19:03:49.077587Z","shell.execute_reply.started":"2025-11-13T19:03:49.036690Z"},"id":"y46fQFKd3t4M","trusted":true},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\ndef objective(trial, train_dataset, val_dataset, epochs: int = 15, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    \n    # Optimizer params\n    lr = 0.001\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n    \n    # Training params\n    batch_size = trial.suggest_categorical(\"batch_size\", [128, 256, 512])\n    \n    # MoCo params\n    queue_size = trial.suggest_categorical(\"queue_size\", [8192, 16384, 32768, 65536])\n\n    # Model architecture params\n    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.1, 0.5)\n    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n\n    # Weights init\n    init_method = trial.suggest_categorical(\"init_method\", ['xavier', 'kaiming'])\n    \n    hidden_dims = []\n    for i in range(n_layers):\n        dim = trial.suggest_categorical(f\"hidden_dim_layer_{i}\", [1280, 1472, 1856, 2048])\n        hidden_dims.append(dim)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n\n    model_args = {\n        'input_dim': 1024,\n        'output_dim': 1536,\n        'hidden_layers': hidden_dims,\n        'dropout_rate': dropout_rate,\n        'activation': nn.GELU,\n        'init_method': init_method\n    }\n    model = SpaceTranslator(**model_args).to(device)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n    \n    scheduler = ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, threshold=0.003, min_lr=1e-6\n    )\n\n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    best_val_loss = float('inf')\n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running_loss = 0.0\n\n        for text_batch, image_emb_batch in train_loader:\n            text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n            \n            optimizer.zero_grad()\n            q = model(text_batch)\n            k = image_emb_batch\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for text_batch, image_emb_batch in val_loader:\n                text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n                q = model(text_batch)\n                k = image_emb_batch\n                loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n                running_val_loss += loss.item()\n        \n        avg_val_loss = running_val_loss / len(val_loader)\n        results = test(val_dataset, model, device) \n        mrr = results['mrr']\n\n        scheduler.step(avg_val_loss) \n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n        \n        trial.report(avg_val_loss, epoch)\n        if trial.should_prune():\n            print(f\"‚èπ Optuna pruning triggered for Trial {trial.number}.\")\n            raise optuna.exceptions.TrialPruned()\n\n    print(f\"‚úÖ Trial {trial.number} complete. Best Val Loss: {best_val_loss}, MRR: {mrr:.6f}\")\n    return best_val_loss\n\n\n\ndef run_optuna_search(data_path: Path, n_trials: int = 150, epochs: int = 30, n_jobs: int = 1, sampler=None, pruner=None):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=3) \n\n    X, y = get_data(data_path)\n    train_dataset, val_dataset = get_datasets(X, y)\n\n    study = optuna.create_study(direction=\"minimize\", pruner=pruner, sampler=sampler)\n    \n    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset, epochs=epochs)\n    \n    try:\n        study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n    except KeyboardInterrupt:\n        print(\"Search stopped manually.\")\n\n    print(\"Study statistics:\")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Best trial:\")\n    trial = study.best_trial\n    print(f\"    Value (Min Val Loss): {trial.value:.6f}\")\n    print(\"    Params: \")\n    for k, v in trial.params.items():\n        print(f\"      {k}: {v}\")\n\n    return study","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T13:55:01.898857Z","iopub.execute_input":"2025-11-14T13:55:01.899569Z","iopub.status.idle":"2025-11-14T13:55:01.913077Z","shell.execute_reply.started":"2025-11-14T13:55:01.899547Z","shell.execute_reply":"2025-11-14T13:55:01.912477Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"study = run_optuna_search(data_path=data_path, n_trials=150, epochs=35, n_jobs=1)\nstudy.trials_dataframe().to_csv(\"optuna_trials.csv\", index=False)\n\nprint(\"Best params:\", study.best_params)\nprint(\"Best trial number:\", study.best_trial.number)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T13:55:12.178571Z","iopub.execute_input":"2025-11-14T13:55:12.178882Z","iopub.status.idle":"2025-11-14T13:55:54.356485Z","shell.execute_reply.started":"2025-11-14T13:55:12.178855Z","shell.execute_reply":"2025-11-14T13:55:54.355760Z"}},"outputs":[{"name":"stderr","text":"[I 2025-11-14 13:55:25,364] A new study created in memory with name: no-name-51f0de8b-e002-4f9f-a846-919c48d52574\n","output_type":"stream"},{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-11-14 13:55:54,215] Trial 0 finished with value: 4.038024323327201 and parameters: {'weight_decay': 0.0007643151580502786, 'batch_size': 512, 'queue_size': 16384, 'dropout_rate': 0.1378677604228606, 'n_layers': 1, 'init_method': 'kaiming', 'hidden_dim_layer_0': 1472}. Best is trial 0 with value: 4.038024323327201.\n","output_type":"stream"},{"name":"stdout","text":"‚úÖ Trial 0 complete. Best Val Loss: 4.038024323327201, MRR: 0.902683\nStudy statistics:\n  Number of finished trials:  1\n  Best trial:\n    Value (Min Val Loss): 4.038024\n    Params: \n      weight_decay: 0.0007643151580502786\n      batch_size: 512\n      queue_size: 16384\n      dropout_rate: 0.1378677604228606\n      n_layers: 1\n      init_method: kaiming\n      hidden_dim_layer_0: 1472\nBest params: {'weight_decay': 0.0007643151580502786, 'batch_size': 512, 'queue_size': 16384, 'dropout_rate': 0.1378677604228606, 'n_layers': 1, 'init_method': 'kaiming', 'hidden_dim_layer_0': 1472}\nBest trial number: 0\n","output_type":"stream"}],"execution_count":42}]}