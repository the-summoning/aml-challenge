{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from torch import nn\nfrom torch.nn import functional as F\nimport torch\nimport numpy as np # Assumendo che np sia importato\n\nclass SpaceTranslator(nn.Module):\n    def __init__(\n        self,\n        input_dim,\n        output_dim,\n        hidden_layers,\n        activation,\n        dropout_rate,\n        init_method: str = 'xavier'\n    ):\n        super().__init__()\n\n        self.init_method = init_method.lower()\n        if self.init_method not in ['xavier', 'kaiming']:\n            raise ValueError(\"Unsupported init_method\")\n\n        layers = []\n        last = input_dim\n\n        for hidden in hidden_layers:\n            layers += [\n                nn.Linear(last, hidden),\n                nn.LayerNorm(hidden),\n                activation(),\n                nn.Dropout(dropout_rate)\n            ]\n            last = hidden\n\n        layers.append(nn.Linear(last, output_dim))\n        self.net = nn.Sequential(*layers)\n\n        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1/0.07))\n\n        # Applica l'inizializzazione scelta\n        self.apply(self.init_weights)\n\n    def init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            if self.init_method == 'kaiming':\n                nn.init.kaiming_uniform_(module.weight, nonlinearity='relu')\n            else:\n                nn.init.xavier_uniform_(module.weight)\n            \n            if module.bias is not None:\n                nn.init.constant_(module.bias, 0.0)\n                \n        elif isinstance(module, nn.LayerNorm):\n            nn.init.ones_(module.weight)\n            nn.init.zeros_(module.bias)\n\n\n    def forward(self, x):\n        return F.normalize(self.net(x), p=2, dim=1) # Usata la versione normalizzata nell'esempio","metadata":{"id":"pErT3mEBFSji","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:11:00.552113Z","iopub.execute_input":"2025-11-13T18:11:00.552822Z","iopub.status.idle":"2025-11-13T18:11:00.560209Z","shell.execute_reply.started":"2025-11-13T18:11:00.552798Z","shell.execute_reply":"2025-11-13T18:11:00.559593Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"from pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n'''Code from https://github.com/Mamiglia/challenge'''\n\ndef mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\nimport numpy as np\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n\n\n\n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n\n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n\n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n\n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n\n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n\n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n\n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n\n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n\n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n\n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n\n    return results\n\ndef eval_on_val(x_val: np.ndarray, y_val: np.ndarray, model: nn.Module, device) -> dict:\n    gt_indices = torch.arange(len(y_val))\n\n    model.eval()\n\n    with torch.inference_mode():\n        translated = model(x_val.to(device)).to('cpu')\n\n    results = evaluate_retrieval(translated, y_val, gt_indices)\n\n    return results\n\n\ndef generate_submission(model: nn.Module, test_path: Path, output_file=\"submission-dirmodel.csv\", device=None):\n    test_data = np.load(test_path)\n    sample_ids = test_data['captions/ids']\n    test_embds = test_data['captions/embeddings']\n    test_embds = torch.from_numpy(test_embds).float()\n\n    with torch.no_grad():\n        pred_embds = model(test_embds.to(device)).cpu()\n\n    print(\"Generating submission file...\")\n\n    if isinstance(pred_embds, torch.Tensor):\n        pred_embds = pred_embds.cpu().numpy()\n\n    df_submission = pd.DataFrame({'id': sample_ids, 'embedding': pred_embds.tolist()})\n\n    df_submission.to_csv(output_file, index=False, float_format='%.17g')\n    print(f\"‚úì Saved submission to {output_file}\")\n\n    return df_submission","metadata":{"id":"umdq3vIdKFfa","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:57:26.380490Z","iopub.execute_input":"2025-11-13T17:57:26.380699Z","iopub.status.idle":"2025-11-13T17:57:26.407392Z","shell.execute_reply.started":"2025-11-13T17:57:26.380678Z","shell.execute_reply":"2025-11-13T17:57:26.406708Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"from torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm\n\ndef moco_info_nce_loss(q, k, queue, logit_scale):\n    # Positivo\n    l_pos = torch.sum(q * k, dim=1, keepdim=True)  # [batch,1]\n\n    # Negativi\n    l_neg = q @ queue.T  # [batch, queue_size]\n\n    logit_scale = torch.clamp(logit_scale, min=np.log(0.01), max=np.log(100))\n\n    logits = torch.cat([l_pos, l_neg], dim=1)\n    logits = logits * logit_scale.exp()\n\n    # Labels: positivo sempre in posizione 0\n    labels = torch.zeros(logits.size(0), dtype=torch.long, device=logits.device)\n\n    return F.cross_entropy(logits, labels)\n\n\n@torch.no_grad()\ndef enqueue(queue, keys, queue_ptr):\n    batch_size = keys.shape[0]\n    queue_size = queue.shape[0]\n    ptr = int(queue_ptr[0])\n\n    if ptr + batch_size <= queue_size:\n        # Slice diretta\n        queue[ptr:ptr+batch_size, :] = keys\n    else:\n        #  (wrap-around)\n        first_part = queue_size - ptr\n        queue[ptr:, :] = keys[:first_part, :]\n        queue[:batch_size - first_part, :] = keys[first_part:, :]\n\n    # Aggiorna il puntatore\n    queue_ptr[0] = (ptr + batch_size) % queue_size\n\ndef train_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n    \n    # Lo scheduler segue la Val Loss (che si minimizza), quindi mode='min'\n    scheduler = ReduceLROnPlateau(\n        optimizer, mode='min', factor=0.5, patience=2, threshold=0.003, min_lr=1e-5\n    )\n\n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    best_val_loss = float('inf')\n    loss_no_improvements = 0 \n\n    for epoch in range(1, epochs+1):\n        model.train()\n        running_loss = 0.0\n        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch}/{epochs}\", leave=False)\n\n        for text_batch, image_emb_batch in progress_bar:\n            text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n            \n            optimizer.zero_grad()\n\n            q = model(text_batch)\n            k = image_emb_batch\n\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        # --- Validation ---\n        model.eval()\n        running_val_loss = 0.0\n        with torch.no_grad():\n            for text_batch, image_emb_batch in val_loader:\n                text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n\n                q = model(text_batch)\n                k = image_emb_batch\n\n                loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n                running_val_loss += loss.item()\n        avg_val_loss = running_val_loss / len(val_loader)\n\n        results = test(val_dataset, model, device)\n        mrr = results['mrr']\n\n        scheduler.step(avg_val_loss) \n\n        print(f\"Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f} | Val Loss: {avg_val_loss:.6f} | \"\n              f\"MRR: {mrr:.6f} | Recall-1: {results['recall_at_1']:.6f} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n        if avg_val_loss < best_val_loss:\n            best_val_loss = avg_val_loss\n            loss_no_improvements = 0 \n            \n            # Salvataggio del modello con la Validation Loss migliore\n            Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n            torch.save(model.state_dict(), save_path)\n            print(f\"üíæ Saved new best model (Val Loss={avg_val_loss:.6f})\")\n        else:\n            loss_no_improvements += 1\n            if loss_no_improvements >= patience:\n                print(\"‚èπ Early stopping triggered based on Validation Loss.\")\n                break\n\n    print(f\"‚úÖ Training complete\")\n    return model\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n\ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n\n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.8, 0.2], generator=torch.Generator().manual_seed(42))\n\n    return train_dataset, val_dataset\n\ndef test(val_dataset: TensorDataset, model: nn.Module, device):\n    val_loader = DataLoader(val_dataset, batch_size=len(val_dataset))\n    for x_val, y_val in val_loader:\n        results = eval_on_val(x_val, y_val, model=model, device=device)\n    return results\n\ndef center(X: torch.Tensor):\n    mean = X.mean(dim=0, keepdim=True)\n    return X - mean, mean","metadata":{"id":"_Ng-afH6FqPt","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:44:42.482720Z","iopub.execute_input":"2025-11-13T18:44:42.483103Z","iopub.status.idle":"2025-11-13T18:44:42.503435Z","shell.execute_reply.started":"2025-11-13T18:44:42.483076Z","shell.execute_reply":"2025-11-13T18:44:42.502651Z"},"jupyter":{"source_hidden":true}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndata_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\n\nsave_path = './models/dir-model.pth'","metadata":{"id":"X_QhTeUoFrLm","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T17:57:26.491443Z","iopub.execute_input":"2025-11-13T17:57:26.491648Z","iopub.status.idle":"2025-11-13T17:57:26.512266Z","shell.execute_reply.started":"2025-11-13T17:57:26.491634Z","shell.execute_reply":"2025-11-13T17:57:26.511364Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"x, y = get_data(data_path)\n\n#x_centered, x_center = center(x)\n\ntrain_dataset, val_dataset = get_datasets(x, y)","metadata":{"id":"zI0blWNyFtDI","colab":{"base_uri":"https://localhost:8080/"},"outputId":"e2e2f865-ee19-4a00-b275-f0678aa7f501","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:03:49.427668Z","iopub.execute_input":"2025-11-13T18:03:49.428087Z","iopub.status.idle":"2025-11-13T18:04:02.877540Z","shell.execute_reply.started":"2025-11-13T18:03:49.428065Z","shell.execute_reply":"2025-11-13T18:04:02.876747Z"}},"outputs":[{"name":"stdout","text":"Texts shape torch.Size([125000, 1024])\nImages shape torch.Size([125000, 1536])\n","output_type":"stream"}],"execution_count":31},{"cell_type":"code","source":"input_dim = x.shape[1]\noutput_dim = y.shape[1]\nhidden_layers = [1256, 2048]\ndropout_rate = 0.5\nbatch_size = 256\nlr = 0.01\nepochs = 250\npatience = 5\n\nqueue_size = 10000\n\nmodel_args = {\n    'input_dim': input_dim,\n    'output_dim': output_dim,\n    'hidden_layers': hidden_layers,\n    'dropout_rate': dropout_rate,\n    'activation': nn.GELU\n}\n\nmodel = SpaceTranslator(**model_args)\n\ntrain_model_moco(model, save_path, train_dataset, val_dataset, batch_size, epochs, lr, patience, queue_size)\n\nprint('Finished training. Now testing using best model...')\n\nstate = torch.load(save_path)\nmodel.load_state_dict(state)\nresults = test(val_dataset, model, device)\nprint(\"Test Results:\", results)","metadata":{"id":"P5k77RkVFuwg","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c5d2268c-1c45-4399-e821-b1d5572f5f60","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:52:01.104845Z","iopub.execute_input":"2025-11-13T18:52:01.105432Z","iopub.status.idle":"2025-11-13T18:57:31.393717Z","shell.execute_reply.started":"2025-11-13T18:52:01.105407Z","shell.execute_reply":"2025-11-13T18:57:31.393070Z"}},"outputs":[{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 001 | Train Loss: 8.727778 | Val Loss: 5.525506 | MRR: 0.725039 | Recall-1: 0.590840 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.725039)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 002 | Train Loss: 5.365557 | Val Loss: 4.570784 | MRR: 0.837310 | Recall-1: 0.743840 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.837310)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 003 | Train Loss: 4.706854 | Val Loss: 4.170775 | MRR: 0.870314 | Recall-1: 0.793680 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.870314)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 004 | Train Loss: 4.321810 | Val Loss: 3.900128 | MRR: 0.887549 | Recall-1: 0.819160 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.887549)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 005 | Train Loss: 4.050368 | Val Loss: 3.720185 | MRR: 0.898636 | Recall-1: 0.837000 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.898636)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 006 | Train Loss: 3.821990 | Val Loss: 3.571072 | MRR: 0.906888 | Recall-1: 0.849640 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.906888)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 007 | Train Loss: 3.631227 | Val Loss: 3.457870 | MRR: 0.912080 | Recall-1: 0.857120 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.912080)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 008 | Train Loss: 3.470839 | Val Loss: 3.378798 | MRR: 0.915122 | Recall-1: 0.862640 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.915122)\n","output_type":"stream"},{"name":"stderr","text":"                                                                         \r","output_type":"stream"},{"name":"stdout","text":"Epoch 009 | Train Loss: 3.333408 | Val Loss: 3.284214 | MRR: 0.919675 | Recall-1: 0.869840 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.919675)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 010 | Train Loss: 3.199440 | Val Loss: 3.215446 | MRR: 0.922864 | Recall-1: 0.874840 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.922864)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 011 | Train Loss: 3.080352 | Val Loss: 3.192078 | MRR: 0.923270 | Recall-1: 0.874840 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.923270)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 012 | Train Loss: 2.976312 | Val Loss: 3.119946 | MRR: 0.927357 | Recall-1: 0.881920 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.927357)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 013 | Train Loss: 2.867031 | Val Loss: 3.097158 | MRR: 0.928782 | Recall-1: 0.884200 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.928782)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 014 | Train Loss: 2.774466 | Val Loss: 3.060852 | MRR: 0.930447 | Recall-1: 0.886720 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.930447)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 015 | Train Loss: 2.683679 | Val Loss: 3.018417 | MRR: 0.931809 | Recall-1: 0.889000 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.931809)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 016 | Train Loss: 2.593929 | Val Loss: 2.993433 | MRR: 0.932717 | Recall-1: 0.890920 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.932717)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 017 | Train Loss: 2.516491 | Val Loss: 2.986677 | MRR: 0.933537 | Recall-1: 0.892760 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.933537)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 018 | Train Loss: 2.445432 | Val Loss: 2.973553 | MRR: 0.933315 | Recall-1: 0.891440 | LR: 1.00e-02\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 019 | Train Loss: 2.368953 | Val Loss: 2.946566 | MRR: 0.934461 | Recall-1: 0.893560 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.934461)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 020 | Train Loss: 2.308971 | Val Loss: 2.932775 | MRR: 0.934600 | Recall-1: 0.893960 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.934600)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 021 | Train Loss: 2.239273 | Val Loss: 2.912142 | MRR: 0.935215 | Recall-1: 0.894720 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.935215)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 022 | Train Loss: 2.173672 | Val Loss: 2.896063 | MRR: 0.936930 | Recall-1: 0.897880 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.936930)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 023 | Train Loss: 2.119078 | Val Loss: 2.897631 | MRR: 0.936983 | Recall-1: 0.898040 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.936983)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 024 | Train Loss: 2.072061 | Val Loss: 2.890368 | MRR: 0.938616 | Recall-1: 0.900800 | LR: 1.00e-02\nüíæ Saved new best model (MRR=0.938616)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 025 | Train Loss: 2.010360 | Val Loss: 2.869364 | MRR: 0.937530 | Recall-1: 0.899320 | LR: 1.00e-02\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 026 | Train Loss: 1.961379 | Val Loss: 2.903009 | MRR: 0.937849 | Recall-1: 0.899280 | LR: 1.00e-02\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 027 | Train Loss: 1.916301 | Val Loss: 2.864191 | MRR: 0.937485 | Recall-1: 0.899280 | LR: 1.00e-02\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 028 | Train Loss: 1.871107 | Val Loss: 2.867248 | MRR: 0.938820 | Recall-1: 0.901320 | LR: 5.00e-03\nüíæ Saved new best model (MRR=0.938820)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 029 | Train Loss: 1.596607 | Val Loss: 2.851360 | MRR: 0.941144 | Recall-1: 0.905040 | LR: 5.00e-03\nüíæ Saved new best model (MRR=0.941144)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 030 | Train Loss: 1.497734 | Val Loss: 2.861686 | MRR: 0.941666 | Recall-1: 0.906040 | LR: 5.00e-03\nüíæ Saved new best model (MRR=0.941666)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 031 | Train Loss: 1.452512 | Val Loss: 2.872866 | MRR: 0.941747 | Recall-1: 0.906160 | LR: 5.00e-03\nüíæ Saved new best model (MRR=0.941747)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 032 | Train Loss: 1.409349 | Val Loss: 2.909070 | MRR: 0.940283 | Recall-1: 0.904240 | LR: 2.50e-03\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 033 | Train Loss: 1.279878 | Val Loss: 2.898021 | MRR: 0.942009 | Recall-1: 0.906920 | LR: 2.50e-03\nüíæ Saved new best model (MRR=0.942009)\n","output_type":"stream"},{"name":"stderr","text":"                                                                          \r","output_type":"stream"},{"name":"stdout","text":"Epoch 034 | Train Loss: 1.216193 | Val Loss: 2.897232 | MRR: 0.941551 | Recall-1: 0.905920 | LR: 2.50e-03\n‚èπ Early stopping triggered based on Validation Loss.\n‚úÖ Training complete. Best MRR: 0.942009\nFinished training. Now testing using best model...\nTest Results: {'mrr': 0.942009235100382, 'ndcg': 0.9561381249868742, 'recall_at_1': 0.90692, 'recall_at_3': 0.97312, 'recall_at_5': 0.98604, 'recall_at_10': 0.9942, 'recall_at_50': 0.9994, 'l2_dist': 25.848154067993164}\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"generate_submission(model, Path(test_path), output_file=\"mojo-pin.csv\", device=device)","metadata":{"id":"_Wk_esyYFw8X","colab":{"base_uri":"https://localhost:8080/","height":461},"outputId":"4c5c742d-e576-4c86-ecd7-48e853ef0ea9","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T18:57:41.520998Z","iopub.execute_input":"2025-11-13T18:57:41.521247Z","iopub.status.idle":"2025-11-13T18:57:44.603192Z","shell.execute_reply.started":"2025-11-13T18:57:41.521229Z","shell.execute_reply":"2025-11-13T18:57:44.602585Z"}},"outputs":[{"name":"stdout","text":"Generating submission file...\n‚úì Saved submission to mojo-pin.csv\n","output_type":"stream"},{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"        id                                          embedding\n0        1  [0.02428484708070755, 0.027138926088809967, -0...\n1        2  [-0.006308969110250473, -0.005466327536851168,...\n2        3  [0.011657739989459515, -0.03169963136315346, 0...\n3        4  [0.060520488768815994, -0.02953926846385002, -...\n4        5  [0.06194249540567398, 0.04723084717988968, 0.0...\n...    ...                                                ...\n1495  1496  [0.010205189697444439, -0.0313861183822155, 0....\n1496  1497  [0.006104839034378529, 0.04210679605603218, 0....\n1497  1498  [0.03780222684144974, -0.0013050471898168325, ...\n1498  1499  [-0.01464053988456726, 0.003734411671757698, 0...\n1499  1500  [0.01115664467215538, -0.028162026777863503, -...\n\n[1500 rows x 2 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>embedding</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>[0.02428484708070755, 0.027138926088809967, -0...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>[-0.006308969110250473, -0.005466327536851168,...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>[0.011657739989459515, -0.03169963136315346, 0...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>[0.060520488768815994, -0.02953926846385002, -...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>[0.06194249540567398, 0.04723084717988968, 0.0...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1495</th>\n      <td>1496</td>\n      <td>[0.010205189697444439, -0.0313861183822155, 0....</td>\n    </tr>\n    <tr>\n      <th>1496</th>\n      <td>1497</td>\n      <td>[0.006104839034378529, 0.04210679605603218, 0....</td>\n    </tr>\n    <tr>\n      <th>1497</th>\n      <td>1498</td>\n      <td>[0.03780222684144974, -0.0013050471898168325, ...</td>\n    </tr>\n    <tr>\n      <th>1498</th>\n      <td>1499</td>\n      <td>[-0.01464053988456726, 0.003734411671757698, 0...</td>\n    </tr>\n    <tr>\n      <th>1499</th>\n      <td>1500</td>\n      <td>[0.01115664467215538, -0.028162026777863503, -...</td>\n    </tr>\n  </tbody>\n</table>\n<p>1500 rows √ó 2 columns</p>\n</div>"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"torch.save(model.state_dict(), \"model_weights.pth\")","metadata":{"id":"y46fQFKd3t4M","trusted":true,"execution":{"iopub.status.busy":"2025-11-13T19:03:49.036143Z","iopub.execute_input":"2025-11-13T19:03:49.036715Z","iopub.status.idle":"2025-11-13T19:03:49.078196Z","shell.execute_reply.started":"2025-11-13T19:03:49.036690Z","shell.execute_reply":"2025-11-13T19:03:49.077587Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"import optuna\nfrom optuna.pruners import MedianPruner\n\ndef objective(trial, train_dataset, val_dataset, epochs: int = 15, device=None):\n    if device is None:\n        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    # --- Parametri MLP ---\n    n_dir_layers = trial.suggest_int(\"n_dir_layers\", 2, 4)\n    n_scale_layers = trial.suggest_int(\"n_scale_layers\", 2, 4)\n\n    layer_choices = [1024, 1152, 1546, 2048, 4096]\n    dir_hidden_dims = [trial.suggest_categorical(f\"dir_units_l{i}\", layer_choices) for i in range(n_dir_layers)]\n    scale_hidden_dims = [trial.suggest_categorical(f\"scale_units_l{i}\", layer_choices) for i in range(n_scale_layers)]\n\n    # --- Parametri di training ---\n    batch_size = trial.suggest_categorical(\"batch_size\", [64, 512, 1024, 2048, 4096])\n    lr = trial.suggest_float(\"lr\", 1e-3, 1e-2, log=True)\n    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-3, log=True)\n    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.3, 0.4, 0.5])\n    activation_name = 'GELU'\n    activation = nn.GELU\n\n    # --- DataLoader ---\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n    # --- Modello ---\n    model_args = {\n        'input_dim': 1024,\n        'output_dim': 1536,\n        'dir_hidden_dims': dir_hidden_dims,\n        'scale_hidden_dims': scale_hidden_dims,\n        'dropout_rate': dropout_rate,\n        'activation': activation\n    }\n    model = SpaceTranslator(**model_args).to(device)\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n        optimizer, mode='max', factor=0.5, patience=1, threshold=0.01, min_lr=1e-7\n    )\n\n    for epoch in range(epochs):\n        model.train()\n        train_loss = 0.0\n\n        for X_batch, y_batch in train_loader:\n            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n            optimizer.zero_grad()\n            output = model(X_batch)\n            \n            loss = info_nce_loss(output, y_batch, model.logit_scale)\n            \n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n\n        train_loss /= len(train_loader)\n\n        # --- Validation ---\n        model.eval()\n        val_loss = 0.0\n        with torch.inference_mode():\n            for X_batch, y_batch in val_loader:\n                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n\n                output = model(X_batch)\n                \n                loss = info_nce_loss(output, y_batch, model.logit_scale)\n                \n                val_loss += loss.item()\n        val_loss /= len(val_loader)\n\n        # --- MRR ---\n        results = test(val_dataset, model, device)\n        mrr = results['mrr']\n\n        scheduler.step(mrr)\n\n        trial.report(mrr, epoch)\n        if trial.should_prune():\n            raise optuna.exceptions.TrialPruned()\n\n        print(f\"[Trial {trial.number} | Epoch {epoch+1}/{epochs}] \"\n              f\"Train Loss: {train_loss:.4f} Val Loss: {val_loss:.4f} \"\n              f\"MRR: {mrr:.4f} Activation: {activation_name} | LR: {optimizer.param_groups[0]['lr']:.2e}\")\n\n\n    return best_mrr\n\n\ndef run_optuna_search(data_path: Path, n_trials: int = 150, epochs: int = 30, n_jobs: int = 1, sampler=None, pruner=None):\n    if pruner is None:\n        pruner = MedianPruner(n_startup_trials=5, n_warmup_steps=1)\n\n    X, y = get_data(data_path)\n    train_dataset, val_dataset = get_datasets(X, y)\n\n    study = optuna.create_study(direction=\"maximize\", pruner=pruner)\n    func = lambda trial: objective(trial, train_dataset=train_dataset, val_dataset=val_dataset, epochs=epochs)\n    study.optimize(func, n_trials=n_trials, n_jobs=n_jobs)\n\n    print(\"Study statistics:\")\n    print(\"  Number of finished trials: \", len(study.trials))\n    print(\"  Best trial:\")\n    trial = study.best_trial\n    print(\"    Value: \", trial.value)\n    print(\"    Params: \")\n    for k, v in trial.params.items():\n        print(f\"      {k}: {v}\")\n\n    return study","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import DataLoader\nfrom tqdm import tqdm\n\ndef train_model_moco_full(model, save_path, full_dataset, batch_size, epochs, lr, queue_size):\n    \"\"\"\n    Versione full-dataset di train_model_moco:\n    - niente validation\n    - niente early stopping\n    - allena su full_dataset (train + val)\n    \"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    train_loader = DataLoader(full_dataset, batch_size=batch_size, shuffle=True)\n\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-5)\n\n    \n    queue = torch.zeros(queue_size, 1536, device=device)\n    queue_ptr = torch.zeros(1, dtype=torch.long, device=device)\n\n    print(\"=== FULL-DATASET MoCo TRAINING (NO VALIDATION) ===\")\n    for epoch in range(1, epochs + 1):\n        model.train()\n        running_loss = 0.0\n\n        progress_bar = tqdm(train_loader, desc=f\"[FULL] Epoch {epoch}/{epochs}\", leave=False)\n        for text_batch, image_emb_batch in progress_bar:\n            text_batch, image_emb_batch = text_batch.to(device), image_emb_batch.to(device)\n\n            optimizer.zero_grad()\n\n            # text proiection\n            q = model(text_batch)\n            # corresponding image\n            k = image_emb_batch\n\n            loss = moco_info_nce_loss(q, k, queue, model.logit_scale)\n            loss.backward()\n            optimizer.step()\n\n            with torch.no_grad():\n                enqueue(queue, k, queue_ptr)\n\n            running_loss += loss.item()\n            progress_bar.set_postfix(loss=loss.item())\n\n        avg_train_loss = running_loss / len(train_loader)\n        print(f\"[FULL] Epoch {epoch:03d} | Train Loss: {avg_train_loss:.6f}\")\n\n    # \n    Path(save_path).parent.mkdir(parents=True, exist_ok=True)\n    torch.save(model.state_dict(), save_path)\n    print(f\"üíæ Saved full-dataset MoCo model to {save_path}\")\n\n    return model\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.utils.data import TensorDataset\n\nfull_dataset = TensorDataset(x, y)\n\n\nstate = torch.load(save_path, map_location=device)\nmodel.load_state_dict(state)\nmodel.to(device)\n\n# no validation, full dataset \nsave_path_full = \"./models/dir-model-moco-full.pth\"\n\nepochs_full = 50  \n\nmodel = train_model_moco_full(\n    model,\n    save_path_full,\n    full_dataset,\n    batch_size=batch_size,  \n    epochs=epochs_full,\n    lr=lr,                  \n    queue_size=queue_size   \n)\n\nprint(\"Finished FULL-DATASET training. Now testing on validation set for comparison...\")\n\n\nstate_full = torch.load(save_path_full, map_location=device)\nmodel.load_state_dict(state_full)\nmodel.to(device)\n\nresults_full = test(val_dataset, model, device)\nprint(\"Test Results after full-dataset training:\", results_full)\nprint(f\"MRR: {results_full['mrr']:.6f} | Recall@1: {results_full['recall_at_1']:.6f}\")\n\n\ngenerate_submission(\n    model,\n    Path(test_path),\n    output_file=\"mojo-pin-full.csv\",\n    device=device\n)\nprint(\"new submission: mojo-pin-full.csv\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}