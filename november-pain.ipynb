{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":117959,"databundleVersionId":14220991,"sourceType":"competition"}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from typing import Optional\nfrom torch import nn\nfrom torch.nn import functional as F\nimport torch\nfrom pathlib import Path\nimport numpy as np\nimport pandas as pd\n\n\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.utils.data import random_split\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:23:43.041049Z","iopub.execute_input":"2025-11-09T16:23:43.041437Z","iopub.status.idle":"2025-11-09T16:23:44.967845Z","shell.execute_reply.started":"2025-11-09T16:23:43.041408Z","shell.execute_reply":"2025-11-09T16:23:44.967243Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def mrr(pred_indices: np.ndarray, gt_indices: np.ndarray) -> float:\n    \"\"\"\n    Compute Mean Reciprocal Rank (MRR)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries (top-K)\n        gt_indices: (N,) array of ground truth indices\n    Returns:\n        mrr: Mean Reciprocal Rank\n    \"\"\"\n    reciprocal_ranks = []\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i] == gt_indices[i])[0]\n        if matches.size > 0:\n            reciprocal_ranks.append(1.0 / (matches[0] + 1))\n        else:\n            reciprocal_ranks.append(0.0)\n    return np.mean(reciprocal_ranks)\n\n\ndef recall_at_k(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int) -> float:\n    \"\"\"Compute Recall@k\n    Args:\n        pred_indices: (N, N) array of top indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        recall: Recall@k\n    \"\"\"\n    recall = 0\n    for i in range(len(gt_indices)):\n        if gt_indices[i] in pred_indices[i, :k]:\n            recall += 1\n    recall /= len(gt_indices)\n    return recall\n\n\n\ndef ndcg(pred_indices: np.ndarray, gt_indices: np.ndarray, k: int = 100) -> float:\n    \"\"\"\n    Compute Normalized Discounted Cumulative Gain (NDCG@k)\n    Args:\n        pred_indices: (N, K) array of predicted indices for N queries\n        gt_indices: (N,) array of ground truth indices\n        k: number of top predictions to consider\n    Returns:\n        ndcg: NDCG@k\n    \"\"\"\n    ndcg_total = 0.0\n    for i in range(len(gt_indices)):\n        matches = np.where(pred_indices[i, :k] == gt_indices[i])[0]\n        if matches.size > 0:\n            rank = matches[0] + 1\n            ndcg_total += 1.0 / np.log2(rank + 1)  # DCG (IDCG = 1)\n    return ndcg_total / len(gt_indices)\n    \n@torch.inference_mode()\ndef evaluate_retrieval(translated_embd, image_embd, gt_indices, max_indices = 99, batch_size=100):\n    \"\"\"Evaluate retrieval performance using cosine similarity\n    Args:\n        translated_embd: (N_captions, D) translated caption embeddings\n        image_embd: (N_images, D) image embeddings\n        gt_indices: (N_captions,) ground truth image indices for each caption\n        max_indices: number of top predictions to consider\n    Returns:\n        results: dict of evaluation metrics\n    \n    \"\"\"\n    # Compute similarity matrix\n    if isinstance(translated_embd, np.ndarray):\n        translated_embd = torch.from_numpy(translated_embd).float()\n    if isinstance(image_embd, np.ndarray):\n        image_embd = torch.from_numpy(image_embd).float()\n    \n    n_queries = translated_embd.shape[0]\n    device = translated_embd.device\n    \n    # Prepare containers for the fragments to be reassembled\n    all_sorted_indices = []\n    l2_distances = []\n    \n    # Process in batches - the narrow gate approach\n    for start_idx in range(0, n_queries, batch_size):\n        batch_slice = slice(start_idx, min(start_idx + batch_size, n_queries))\n        batch_translated = translated_embd[batch_slice]\n        batch_img_embd = image_embd[batch_slice]\n        \n        # Compute similarity only for this batch\n        batch_similarity = batch_translated @ batch_img_embd.T\n\n        # Get top-k predictions for this batch\n        batch_indices = batch_similarity.topk(k=max_indices, dim=1, sorted=True).indices.numpy()\n        all_sorted_indices.append(gt_indices[batch_slice][batch_indices])\n\n        # Compute L2 distance for this batch\n        batch_gt = gt_indices[batch_slice]\n        batch_gt_embeddings = image_embd[batch_gt]\n        batch_l2 = (batch_translated - batch_gt_embeddings).norm(dim=1)\n        l2_distances.append(batch_l2)\n    \n    # Reassemble the fragments\n    sorted_indices = np.concatenate(all_sorted_indices, axis=0)\n    \n    # Apply the sacred metrics to the whole\n    metrics = {\n        'mrr': mrr,\n        'ndcg': ndcg,\n        'recall_at_1': lambda preds, gt: recall_at_k(preds, gt, 1),\n        'recall_at_3': lambda preds, gt: recall_at_k(preds, gt, 3),\n        'recall_at_5': lambda preds, gt: recall_at_k(preds, gt, 5),\n        'recall_at_10': lambda preds, gt: recall_at_k(preds, gt, 10),\n        'recall_at_50': lambda preds, gt: recall_at_k(preds, gt, 50),\n    }\n    \n    results = {\n        name: func(sorted_indices, gt_indices)\n        for name, func in metrics.items()\n    }\n    \n    l2_dist = torch.cat(l2_distances, dim=0).mean().item()\n    results['l2_dist'] = l2_dist\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:23:47.790822Z","iopub.execute_input":"2025-11-09T16:23:47.791720Z","iopub.status.idle":"2025-11-09T16:23:47.804721Z","shell.execute_reply.started":"2025-11-09T16:23:47.791689Z","shell.execute_reply":"2025-11-09T16:23:47.803819Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"data_path= '/kaggle/input/aml-competition/train/train/train.npz'\ntest_path= '/kaggle/input/aml-competition/test/test/test.clean.npz'\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\ndef analyze_tensor_distances(X: torch.Tensor, Y: torch.Tensor):\n    print(\"-\" * 40)\n    \n    # Distanza euclidea per riga\n    row_distances = torch.norm(X - Y, dim=1)  # Nx1\n    mean_euclidean = row_distances.mean().item()\n    print(f\"üìè Mean Euclidean distance (per row): {mean_euclidean:.4f}\")\n    \n    print(\"-\" * 40)\n    \n    # Similarit√† coseno per riga\n    row_cosine_sim = F.cosine_similarity(X, Y, dim=1)  # Nx1\n    mean_cosine_sim = row_cosine_sim.mean().item()\n    print(f\"üìê Mean Cosine similarity (per row): {mean_cosine_sim:.4f}\")\n    \n    print(\"-\" * 40, end='\\n\\n')\n\ndef get_data(data_path: Path):\n    data = np.load(data_path)\n    caption_embeddings = data['captions/embeddings']\n    image_embeddings = data['images/embeddings']\n    caption_labels = data['captions/label']\n    data.close()\n\n    X_abs, y_abs = torch.tensor(caption_embeddings), torch.tensor(image_embeddings[np.argmax(caption_labels, axis=1)])\n\n    return X_abs, y_abs\n    \ndef get_datasets(X_abs, y_abs) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:    \n    print('Texts shape', X_abs.shape)\n    print('Images shape', y_abs.shape)\n    \n    dataset = TensorDataset(X_abs, y_abs)\n    train_dataset, val_dataset = random_split(dataset, [0.9, 0.1], generator=torch.Generator().manual_seed(42))\n    \n    return train_dataset, val_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:23:51.418682Z","iopub.execute_input":"2025-11-09T16:23:51.418958Z","iopub.status.idle":"2025-11-09T16:23:51.484753Z","shell.execute_reply.started":"2025-11-09T16:23:51.418937Z","shell.execute_reply":"2025-11-09T16:23:51.483793Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def anchor_pruning(X: torch.Tensor, n_samples: int, threshold: float = 0.65, seed=324) -> torch.Tensor:\n    N, D = X.shape\n    device = X.device\n    selected = []\n\n    torch.manual_seed(seed)\n    if device.type == 'cuda':\n        torch.cuda.manual_seed(seed)\n\n    first_idx = torch.randint(0, N, (1,), device=device).item()\n    selected.append(first_idx)\n    \n    first_point = X[first_idx].unsqueeze(0)\n    min_dists = 1.0 - torch.abs(X @ first_point.T).squeeze(1)\n    \n    while len(selected) < n_samples:\n        farthest_idx = torch.argmax(min_dists).item()\n        max_dist = min_dists[farthest_idx].item()\n        \n        if max_dist < threshold:\n            break\n        \n        selected.append(farthest_idx)\n        \n        new_point = X[farthest_idx].unsqueeze(0)\n        dist_to_new = 1.0 - torch.abs(X @ new_point.T).squeeze(1)\n        min_dists = torch.minimum(min_dists, dist_to_new)\n    \n    return torch.tensor(selected, dtype=torch.long, device=device)\n\n\ndef anchor_completion(Ax: torch.Tensor, Ay: torch.Tensor, desired_dim: int):\n    assert Ax.size(0) == Ay.size(0)\n    assert isinstance(desired_dim, int) and desired_dim > 0\n    \n    if desired_dim < Ax.size(0):\n        return Ax, Ay\n    \n    Sx = torch.eye(Ax.size(1)).to(Ax.device)\n    Sy = (Sx @ Ax.T) @ torch.linalg.pinv(Ay.T)\n    \n    remaining_dim = desired_dim - Ax.size(0)\n    \n    Ax = torch.cat([Ax, Sx[:remaining_dim]], dim=0)\n    Ay = torch.cat([Ay, Sy[:remaining_dim]], dim=0)\n    \n    return Ax, Ay\n\n\ndef anchor_subspaces(X_normalized: torch.Tensor, Y_normalized: torch.Tensor, anchors_no: int, omega: int, delta: float):\n    assert isinstance(omega, int) and omega > 0\n    \n    anchors = []\n    \n    for _ in range(omega):\n        seed = torch.randint(0, 2**32, (1,)).item()\n        indices = anchor_pruning(Y_normalized.to('cuda'), anchors_no, delta, seed).cpu()\n    \n        Ax = X_normalized[indices]\n        Ay = Y_normalized[indices]\n    \n        Ax, Ay = anchor_completion(Ax, Ay, anchors_no)\n\n        anchors.append((Ax, Ay))\n    \n    return anchors\n\n\ndef reconstruct_source(X_normalized: torch.Tensor, subspaces: list[tuple[torch.Tensor, torch.Tensor]]):   \n    anchors = []\n\n    X_reconstruct = torch.zeros_like(X_normalized).to('cuda')\n    \n    for Ax, _ in subspaces:\n        X_rel = X_normalized @ Ax.T\n        X_reconstruct += X_rel @ torch.linalg.pinv(Ax.T)\n    \n    return X_reconstruct / len(subspaces)\n\n\ndef reconstruct_target(Y_normalized: torch.Tensor, subspaces: list[tuple[torch.Tensor, torch.Tensor]]):   \n    anchors = []\n\n    Y_reconstruct = torch.zeros_like(Y_normalized).to('cuda')\n    \n    for _, Ay in subspaces:\n        Y_rel = Y_normalized @ Ay.T\n        Y_reconstruct += Y_rel @ torch.linalg.pinv(Ay.T)\n    \n    return Y_reconstruct / len(subspaces)\n\n\ndef reconstruct_target_from_data(data: torch.Tensor, subspaces: list[tuple[torch.Tensor, torch.Tensor]]):   \n    anchors = []\n\n    Y_reconstruct = None #torch.zeros_like(data).to('cuda')\n    \n    for _, Ay in subspaces:\n        #print(data.shape, torch.linalg.pinv(Ay.T).shape)\n        if Y_reconstruct is None:\n            Y_reconstruct = data @ torch.linalg.pinv(Ay.T)\n        else:\n            Y_reconstruct += data @ torch.linalg.pinv(Ay.T)\n    \n    return Y_reconstruct / len(subspaces)\n\n\ndef center(X: torch.Tensor):\n    mean = X.mean(dim=0, keepdim=True)\n    return X - mean, mean\n\ndef normalize(X: torch.Tensor):\n    norms = torch.norm(X, p=2, dim=1, keepdim=True)\n    norms = torch.clamp(norms, min=1e-8)  # evita divisione per zero\n    return X / norms, norms.expand_as(X)\n\ndef pad(x: torch.Tensor, M: int) -> torch.Tensor:\n    N, D = x.shape\n    if M < D:\n        raise ValueError(f\"M={M} must be >= D={D}\")\n    \n    pad = (0, M - D)  \n    return F.pad(x, pad, mode='constant', value=0.0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:23:53.555001Z","iopub.execute_input":"2025-11-09T16:23:53.555311Z","iopub.status.idle":"2025-11-09T16:23:53.569850Z","shell.execute_reply.started":"2025-11-09T16:23:53.555286Z","shell.execute_reply":"2025-11-09T16:23:53.568998Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"X, Y = get_data(data_path)\n\nX = X.to(device)\nY = Y.to(device)\n\nX = pad(X, Y.size(1))\n\nX_centered, X_center = center(X)\nY_centered, Y_center = center(Y)\n\nX_normalized, X_norm = normalize(X_centered)\nY_normalized, Y_norm = normalize(Y_centered)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:23:56.945864Z","iopub.execute_input":"2025-11-09T16:23:56.946659Z","iopub.status.idle":"2025-11-09T16:24:11.002570Z","shell.execute_reply.started":"2025-11-09T16:23:56.946624Z","shell.execute_reply":"2025-11-09T16:24:11.001985Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"subspaces = anchor_subspaces(X_normalized, Y_normalized, Y.size(1), 1, 0.65)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:25:06.321955Z","iopub.execute_input":"2025-11-09T16:25:06.322638Z","iopub.status.idle":"2025-11-09T16:25:14.588105Z","shell.execute_reply.started":"2025-11-09T16:25:06.322608Z","shell.execute_reply":"2025-11-09T16:25:14.587265Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"X_reconstruct = reconstruct_source(X_normalized, subspaces)\nanalyze_tensor_distances(X_reconstruct * X_norm + X_center, X)\n\nY_reconstruct = reconstruct_target(Y_normalized, subspaces)\nanalyze_tensor_distances(Y_reconstruct * Y_norm + Y_center, Y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:25:21.750313Z","iopub.execute_input":"2025-11-09T16:25:21.750627Z","iopub.status.idle":"2025-11-09T16:25:23.698294Z","shell.execute_reply.started":"2025-11-09T16:25:21.750589Z","shell.execute_reply":"2025-11-09T16:25:23.697710Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------\nüìè Mean Euclidean distance (per row): 0.0024\n----------------------------------------\nüìê Mean Cosine similarity (per row): 1.0000\n----------------------------------------\n\n----------------------------------------\nüìè Mean Euclidean distance (per row): 0.2150\n----------------------------------------\nüìê Mean Cosine similarity (per row): 1.0000\n----------------------------------------\n\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"import pickle as pk\n\nwith open('data.pkl', 'wb') as f:\n    data = {\n        'X_norm': X_norm,\n        'Y_norm': Y_norm,\n        'X_center': X_center,\n        'Y_center': Y_center,\n        'subspaces': subspaces\n    }\n    pk.dump(data, f)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"Ax_batch = torch.stack([Ax for Ax, Ay in subspaces], dim=0)  # [num_subspaces x d x f]\nAy_batch = torch.stack([Ay for Ax, Ay in subspaces], dim=0)  # [num_subspaces x d x f]\n\nX_proj = torch.einsum('nf,sdf->snd', X_normalized, Ax_batch)\nY_proj = torch.einsum('nf,sdf->snd', Y_normalized, Ay_batch)\n\nX_rel = X_proj.mean(dim=0)  # [n x d]\nY_rel = Y_proj.mean(dim=0)  # [n x d]\n\nanalyze_tensor_distances(X_rel, Y_rel)\n\nlambda_reg = 0.05\nd = X_rel.shape[1]\nI = torch.eye(d, device=X_rel.device)\nW = torch.linalg.solve(X_rel.T @ X_rel + lambda_reg*I, X_rel.T @ Y_rel)\n\nanalyze_tensor_distances(X_rel @ W, Y_rel)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:25:31.199257Z","iopub.execute_input":"2025-11-09T16:25:31.199555Z","iopub.status.idle":"2025-11-09T16:25:32.196821Z","shell.execute_reply.started":"2025-11-09T16:25:31.199533Z","shell.execute_reply":"2025-11-09T16:25:32.196211Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------\nüìè Mean Euclidean distance (per row): 5.7006\n----------------------------------------\nüìê Mean Cosine similarity (per row): 0.2416\n----------------------------------------\n\n----------------------------------------\nüìè Mean Euclidean distance (per row): 2.6045\n----------------------------------------\nüìê Mean Cosine similarity (per row): 0.6623\n----------------------------------------\n\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"data_changed_space = (X_rel @ W)\ntranslation = reconstruct_target_from_data(data_changed_space, subspaces) * Y_norm + Y_center\nanalyze_tensor_distances(translation, Y)\nanalyze_tensor_distances(F.normalize(translation, dim=1), F.normalize(Y, dim=1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:28:23.573180Z","iopub.execute_input":"2025-11-09T16:28:23.573692Z","iopub.status.idle":"2025-11-09T16:28:24.771533Z","shell.execute_reply.started":"2025-11-09T16:28:23.573668Z","shell.execute_reply":"2025-11-09T16:28:24.770762Z"}},"outputs":[{"name":"stdout","text":"----------------------------------------\nüìè Mean Euclidean distance (per row): 14.5563\n----------------------------------------\nüìê Mean Cosine similarity (per row): 0.8255\n----------------------------------------\n\n----------------------------------------\nüìè Mean Euclidean distance (per row): 0.5858\n----------------------------------------\nüìê Mean Cosine similarity (per row): 0.8255\n----------------------------------------\n\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"gt_indices = torch.arange(len(Y_normalized))\nevaluate_retrieval(translation.cpu().numpy(), Y.cpu().numpy(), gt_indices)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-09T16:28:51.178333Z","iopub.execute_input":"2025-11-09T16:28:51.179152Z","iopub.status.idle":"2025-11-09T16:29:08.900308Z","shell.execute_reply.started":"2025-11-09T16:28:51.179116Z","shell.execute_reply":"2025-11-09T16:29:08.899104Z"}},"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"{'mrr': 0.3435692022718723,\n 'ndcg': 0.4919091671844142,\n 'recall_at_1': 0.137784,\n 'recall_at_3': 0.415616,\n 'recall_at_5': 0.693944,\n 'recall_at_10': 0.840224,\n 'recall_at_50': 0.98788,\n 'l2_dist': 14.556254386901855}"},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"# test_data = np.load(test_path)\n# sample_ids = test_data['captions/ids']\n# test_embds = test_data['captions/embeddings']\n# test_embds = torch.from_numpy(test_embds).float()\n\n# test_data.close()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test_embds = test_embds.to(device)\n# test_embds = pad(test_embds, Y.size(1))\n# test_embds, test_center = center(test_embds)\n# test_normalized, test_norm = normalize(test_embds)\n\n# print(test_normalized.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}